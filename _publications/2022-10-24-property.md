---
title: "Visualizing the Obvious: A Concreteness-based Ensemble Model for Noun Property Prediction"
collection: publications
permalink: /publication/2022-10-24-property
excerpt: 'Abstract. Neural language models encode rich knowledge about entities and their relationships which can be extracted from their representations using probing. Common properties of nouns (e.g., red strawberries, small ant) are, however, more challenging to extract compared to other types of knowledge because they are rarely explicitly stated in texts. We hypothesize this to mainly be the case for perceptual properties which are obvious to the participants in the communication. We propose to extract these properties from images and use them in an ensemble model, in order to complement the information that is extracted from language models. We consider perceptual properties to be more concrete than abstract properties (e.g., interesting, flawless). We propose to use the adjectives' concreteness score as a lever to calibrate the contribution of each source (text vs. images). We evaluate our ensemble model in a ranking task where the actual properties of a noun need to be ranked higher than other non-relevant properties. Our results show that the proposed combination of text and images greatly improves noun property prediction compared to powerful text-based language models. '
date: 2022-10-24
venue: 'Findings of EMNLP 2022'
# paperurl: 'http://academicpages.github.io/files/paper2.pdf'
citation: 'Yue Yang, Artemis Panagopoulou, Marianna Apidianaki, Mark Yatskar, Chris Callison-Burch. &quot;Visualizing the Obvious: A Concreteness-based Ensemble Model for Noun Property Prediction.&quot; <i>Findings of EMNLP 2022</i>.'
---
Abstract. Understanding what sequence of steps are needed to complete a goal can help artificial intelligence systems reason about human ac- tivities. Past work in NLP has examined the task of goal-step inference for text. We intro- duce the visual analogue. We propose the Vi- sual Goal-Step Inference (VGSI) task, where a model is given a textual goal and must choose which of four images represents a plausible step towards that goal. With a new dataset har- vested from wikiHow consisting of 772,277 images representing human actions, we show that our task is challenging for state-of-the- art multimodal models. Moreover, the mul- timodal representation learned from our data can be effectively transferred to other datasets like HowTo100m, increasing the VGSI accu- racy by 15 - 20%. Our task will facilitate mul- timodal reasoning about procedural events.

[Download paper here](https://arxiv.org/pdf/2210.12905.pdf)

Recommended citation: Yue Yang*, Artemis Panagopoulou*, Marianna Apidianaki, Mark Yatskar, Chris Callison-Burch. "Visualizing the Obvious: A Concreteness-based Ensemble Model for Noun Property Prediction." <i>Findings of EMNLP 2022</i>.