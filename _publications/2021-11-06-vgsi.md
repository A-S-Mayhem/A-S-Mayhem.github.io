---
title: "Visual Goal-Step Inference using wikiHow"
collection: publications
permalink: /publication/2021-11-06-vgsi
excerpt: 'Abstract. Understanding what sequence of steps are needed to complete a goal can help artificial intelligence systems reason about human activities. Past work in NLP has examined the task of goal-step inference for text. We introduce the visual analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model is given a textual goal and must choose which of four images represents a plausible step towards that goal. With a new dataset harvested from wikiHow consisting of 772,277 images representing human actions, we show that our task is challenging for state-of-the-art multimodal models. Moreover, the multimodal representation learned from our data can be effectively transferred to other datasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task will facilitate multimodal reasoning about procedural events.'
date: 2021-11-06
venue: 'EMNLP (Oral)'
# paperurl: 'http://academicpages.github.io/files/paper2.pdf'
citation: 'Yang, Yue, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar, and Chris Callison-Burch. &quot;Visual Goal-Step Inference using wikiHow.&quot; <i>In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.</i>.'
---
Abstract. Understanding what sequence of steps are needed to complete a goal can help artificial intelligence systems reason about human ac- tivities. Past work in NLP has examined the task of goal-step inference for text. We intro- duce the visual analogue. We propose the Vi- sual Goal-Step Inference (VGSI) task, where a model is given a textual goal and must choose which of four images represents a plausible step towards that goal. With a new dataset har- vested from wikiHow consisting of 772,277 images representing human actions, we show that our task is challenging for state-of-the- art multimodal models. Moreover, the mul- timodal representation learned from our data can be effectively transferred to other datasets like HowTo100m, increasing the VGSI accu- racy by 15 - 20%. Our task will facilitate mul- timodal reasoning about procedural events.

[Download paper here](https://arxiv.org/pdf/2104.05845.pdf)

Yang, Yue, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar, and Chris Callison-Burch. "Visual Goal-Step Inference using wikiHow." <i>In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.</i>.