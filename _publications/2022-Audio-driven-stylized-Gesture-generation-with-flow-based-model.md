---
title: "Audio-driven stylized Gesture generation with flow-based model"
collection: publications
permalink: /publication/2022-Audio-driven-stylized-Gesture-generation-with-flow-based-model
excerpt: ''
date: 2022-10-28
venue: 'ECCV'
tags:
  - Conference Publications
citation: 'Sheng Ye, Yu-Hui Wen, Yanan Sun, Ying He, Ziyang Zhang, Yaoyuan Wang, Weihua He, and Yong-Jin Liu*. Audio-Driven Stylized Gesture Generation with Flow-Based Model. European conference on computer vision (ECCV) 2022.'
---



Abstract: Generating stylized audio-driven gestures for robots and vir-tual avatars has attracted increasing considerations recently. Existing methods require style labels (e.g. speaker identities), or complex prepro-cessing of data to obtain the style control parameters. In this paper, we propose a new end-to-end flow-based model, which can generate audio-driven gestures of arbitrary styles with neither preprocessing nor style labels. To achieve this goal, we introduce a global encoder and a gesture perceptual loss into the classic generative flow model to capture both global and local information. We conduct extensive experiments on two benchmark datasets: the TED Dataset and the Trinity Dataset. Both quantitative and qualitative evaluations show that the proposed model outperforms state-of-the-art models.



[Download paper here](http://yongjinliu.github.io/files/2022-Audio-driven-stylized-Gesture-generation-with-flow-based-model.pdf)

[Download code here](https://github.com/yesheng-THU/GFGE)

Recommended citation: Sheng Ye, Yu-Hui Wen, Yanan Sun, Ying He, Ziyang Zhang, Yaoyuan Wang, Weihua He, and **Yong-Jin Liu***. Audio-Driven Stylized Gesture Generation with Flow-Based Model. European conference on computer vision (ECCV) 2022.

