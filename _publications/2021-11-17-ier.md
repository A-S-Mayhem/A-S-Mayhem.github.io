---
title: "Induce, Edit, Retrieve: Language Grounded Multimodal Schema for Instructional Video Retrieval"
collection: publications
permalink: /publication/2021-11-17-ier
excerpt: 'Abstract. Schemata are structured representations of complex tasks that can aid artificial intelligence by allowing models to break down complex tasks into intermediate steps. We propose a novel system that induces schemata from web videos and generalizes them to capture unseen tasks with the goal of improving video retrieval performance. Our system proceeds in three major phases: (1) Given a task with related videos, we construct an initial schema for a task using a joint video-text model to match video segments with text representing steps from wikiHow; (2) We generalize schemata to unseen tasks by leveraging language models to edit the text within existing schemata. Through generalization, we can allow our schemata to cover a more extensive range of tasks with a small amount of learning data; (3) We conduct zero-shot instructional video retrieval with the unseen task names as the queries. Our schema-guided approach outperforms existing methods for video retrieval, and we demonstrate that the schemata induced by our system are better than those generated by other models.'
date: 2021-11-17
venue: 'Arxiv'
# paperurl: 'https://arxiv.org/pdf/2111.09276'
citation: 'Yang, Yue, et al. "Induce, Edit, Retrieve: Language Grounded Multimodal Schema for Instructional Video Retrieval." <i>arXiv preprint arXiv:2111.09276 (2021)</i>'
---
Abstract. Understanding what sequence of steps are needed to complete a goal can help artificial intelligence systems reason about human activities. Past work in NLP has examined the task of goal-step inference for text. We introduce the visual analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model is given a textual goal and must choose which of four images represents a plausible step towards that goal. With a new dataset harvested from wikiHow consisting of 772,277 images representing human actions, we show that our task is challenging for state-of-the- art multimodal models. Moreover, the multimodal representation learned from our data can be effectively transferred to other datasets like HowTo100m, increasing the VGSI accu- racy by 15 - 20%. Our task will facilitate multimodal reasoning about procedural events.

[Download paper here](https://arxiv.org/pdf/2111.09276)

Recommended citation: Yang, Yue, et al. "Induce, Edit, Retrieve: Language Grounded Multimodal Schema for Instructional Video Retrieval." <i>arXiv preprint arXiv:2111.09276 (2021)</i>.