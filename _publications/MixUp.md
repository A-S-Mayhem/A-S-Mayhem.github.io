---
title: "Leveraging BERT with Mixup For Sentence Classification"
collection: publications
permalink: /publication/MixUp
excerpt: 'BERT + Mixup = SOTA in sentence classification and more robust models (spotlight presentation)'
date: 2020-10-01
venue: 'AAAI'
paperurl: 
citation: 'Jindal, A., Gnaneshwar, D., Sawhney, R., & Shah, R. R. (2020, April). Leveraging BERT with mixup for sentence classification (student abstract). In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 10, pp. 13829-13830).'
---
Good generalization capability is an important quality of well-trained and robust neural networks. However, networks usually struggle when faced with samples outside the training distribution. Mixup is a technique that improves generalization, reduces memorization, and increases adversarial robustness. We apply a variant of Mixup called Manifold Mixup to the sentence classification problem, and present the results along with an ablation study. Our methodology outperforms CNN, LSTM, and vanilla BERT models in generalization.

Our paper was amongst the few selected for an oral spotlight presentation in NYC for AAAI 2020 Student Abstract Session.

[Download paper here](https://ojs.aaai.org/index.php/AAAI/article/download/7186/7040)
