---
layout: archive
title: ""
permalink: /readings/
author_profile: true
---

# 🤖 Embodied Artificial Intelligence Seminar - Readings and Resources

Welcome to the CS6604 Embodied AI Seminar! 

Below is a list of topics we'll cover during the semester, along with recommended readings, and links to project pages or source code repositories where applicable.

For each paper, click on 📚 for the PDF version and on 🌍 for additional resources.
<details>
  <summary><b>Topic 1: Benchmarks: Simulators, Environments, Datasets</b></summary>
  <ul>
    <li>ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes <a href="https://arxiv.org/abs/2304.04321">📚</a> <a href="https://arnold-benchmark.github.io/">🌍</a></li>
    <li>iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes <a href="https://arxiv.org/abs/2012.02924">📚</a> <a href="https://svl.stanford.edu/igibson/">🌍</a></li>
    <li>Matterport3D: Interpreting Visually-Grounded Navigation Instructions in Real Environments <a href="https://arxiv.org/abs/1711.07280">📚</a> <a href="https://bringmeaspoon.org/">🌍</a></li>
    <li>CVDN: Vision-and-Dialog Navigation <a href="https://arxiv.org/abs/1907.04957">📚</a></li>
    <li>Soundspaces: Audio-Visual Navigation in 3D Environments <a href="https://link.springer.com/chapter/10.1007/978-3-030-58539-6_2">📚</a> <a href="https://vision.cs.utexas.edu/projects/audio_visual_navigation/">🌍</a></li>
    <li>AI2-THOR: An Interactive 3D Environment for Visual AI <a href="https://arxiv.org/abs/1712.05474">📚</a> <a href="https://ai2thor.allenai.org/">🌍</a></li>
    <li>Rearrangement: A Challenge for Embodied AI <a href="https://arxiv.org/abs/2011.01975">📚</a></li>
    <li>Visual Room Rearrangement <a href="https://arxiv.org/abs/2103.16544">📚</a> <a href="https://ai2thor.allenai.org/rearrangement/">🌍</a></li>
    <li>ProcTHOR: Large-Scale Embodied AI Using Procedural AI Generation <a href="https://arxiv.org/abs/2206.06994">📚</a> <a href="https://procthor.allenai.org/">🌍</a></li>
    <li>ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills <a href="https://arxiv.org/abs/2302.04659">📚</a> <a href="https://maniskill2.github.io/">🌍</a></li>
    <li>Object Goal Navigation using Goal-Oriented Semantic Exploration <a href="https://arxiv.org/abs/2007.00643">📚</a> <a href="https://devendrachaplot.github.io/projects/semantic-exploration.html">🌍</a></li>
    <li>Embodied Question Answering in Photorealistic Environments with Point Cloud Perception <a href="https://arxiv.org/abs/1904.03461">📚</a> <a href="https://embodiedqa.org/">🌍</a></li>
    <li>Alfred: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Shridhar_ALFRED_A_Benchmark_for_Interpreting_Grounded_Instructions_for_Everyday_Tasks_CVPR_2020_paper.html">📚</a> <a href="https://askforalfred.com/">🌍</a></li>
    <li>DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following <a href="https://arxiv.org/abs/2202.13330">📚</a> <a href="https://github.com/xfgao/DialFRED">🌍</a></li>
    <li>Alexa Arena: A User-Centric Interactive Platform for Embodied AI <a href="https://arxiv.org/abs/2303.01586">📚</a> <a href="https://github.com/amazon-science/alexa-arena">🌍</a></li>
    <li>VirtualHome: Simulating Household Activities via Programs <a href="https://arxiv.org/abs/1806.07011">📚</a> <a href="http://virtual-home.org/">🌍</a></li>
    <li>BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and Realistic Simulation <a href="https://proceedings.mlr.press/v205/li23a.html">📚</a> <a href="https://behavior.stanford.edu/">🌍</a></li>
    <li>MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge <a href="https://arxiv.org/abs/2206.08853">📚</a> <a href="https://minedojo.org/">🌍</a></li>
  </ul>
</details>

<details>
  <summary><b>Topic 2: Conceptual Framing, World Models, Behavioral and Performance Metrics</b></summary>
  <ul>
    <li>World Models <a href="https://arxiv.org/abs/1803.10122">📚</a> <a href="https://worldmodels.github.io/">🌍</a></li>
    <li>Machine Theory of Mind <a href="https://arxiv.org/abs/1802.07740">📚</a> </li>
    <li>Collaborative World Models: An Online-Offline Transfer RL Approach <a href="https://arxiv.org/abs/2305.15260">📚</a> </li>
    <li>Transformers are Sample-Efficient World Models <a href="https://arxiv.org/abs/2209.00588">📚</a> </li>
    <li>Learning Temporally Abstract World Models without Online Experimentation <a href="https://proceedings.mlr.press/v202/freed23a/freed23a.pdf">📚</a> </li>
    <li>Reward-Free Curricula for Training Robust World Models <a href="https://arxiv.org/abs/2306.09205">📚</a> </li>
    <li>Recurrent World Models Facilitate Policy Evolution <a href="https://arxiv.org/abs/1809.01999">📚</a> <a href="https://worldmodels.github.io/">🌍</a></li>
      <li>Discovering and Achieving Goals via World Models <a href="https://arxiv.org/abs/2110.09514">📚</a> <a href="https://orybkin.github.io/lexa/">🌍</a></li>
    <li>Planning to Explore via Self-Supervised World Models <a href="https://arxiv.org/abs/2005.05960">📚</a> <a href="https://ramanans1.github.io/plan2explore/">🌍</a></li>
    <li>Learning to Model the World with Language<a href="https://arxiv.org/abs/2308.01399">📚</a> <a href="https://dynalang.github.io/">🌍</a></li>
    <li>Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling <a href="https://arxiv.org/abs/2301.12050">📚</a></li>
    <li>Dream to Control: Learning Behaviors by Latent Imagination <a href="https://arxiv.org/abs/1912.01603">📚</a></li>
    <li>DayDreamer: World Models for Physical Robot Learning <a href="https://arxiv.org/abs/2206.14176">📚</a> <a href="https://danijar.com/project/daydreamer/">🌍</a></li>
    <li>Mastering Diverse Domains through World Models <a href="https://arxiv.org/abs/2301.04104">📚</a> <a href="https://danijar.com/project/dreamerv3/">🌍</a></li>
    <li>Mastering Atari with Discrete World Models <a href="https://arxiv.org/abs/2010.02193">📚</a> <a href="https://danijar.com/project/dreamerv2/">🌍</a></li>
    <li>Masked World Models for Visual Control <a href="https://proceedings.mlr.press/v205/seo23a/seo23a.pdf">📚</a> <a href="https://sites.google.com/view/mwm-rl">🌍</a></li>
    <li>Structured World Models from Human Videos <a href="https://www.roboticsproceedings.org/rss19/p012.pdf">📚</a> <a href="https://human-world-model.github.io/">🌍</a></li>
    <li>Building Machines That Learn and Think Like People <a href="https://arxiv.org/abs/1604.00289">📚</a></li>
    <li>Action and Perception as Divergence Minimization <a href="https://arxiv.org/abs/2009.01791">📚</a></li>
    <li>Intrinsically Motivated Reinforcement Learning <a href="https://proceedings.neurips.cc/paper_files/paper/2004/file/4be5a36cbaca8ab9d2066debfe4e65c1-Paper.pdf">📚</a> </li>
       <li>Decision Transformer: Reinforcement Learning via Sequence Modeling <a href="https://arxiv.org/abs/2106.01345">📚</a> </li>
    <li>Curiosity-Driven Exploration of Learned Disentangled Goal Spaces <a href="http://proceedings.mlr.press/v87/laversanne-finot18a/laversanne-finot18a.pdf">📚</a></li>
    <li>Encouraging and Evaluating Embodied Exploration <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_EXCALIBUR_Encouraging_and_Evaluating_Embodied_Exploration_CVPR_2023_paper.pdf">📚</a></li>
    <li>Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven Exploration <a href="https://proceedings.neurips.cc/paper/2020/file/274e6fcf4a583de4a81c6376f17673e7-Paper.pdf">📚</a></li>
    <li>Learning to play with intrinsically-motivated, self-aware agents <a href="https://arxiv.org/abs/1802.07442">📚</a></li>
    <li>On Evaluation of Embodied Navigation Agents <a href="https://arxiv.org/abs/1807.06757">📚</a></li>
     <li>ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to Objects <a href="https://arxiv.org/abs/2006.13171">📚</a></li>
     <li>On the Evaluation of Vision-and-Language Navigation Instructions <a href="https://arxiv.org/abs/2101.10504">📚</a></li>
     <li>A New Path: Scaling Vision-and-Language Navigation With Synthetic Instructions and Imitation Learning <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kamath_A_New_Path_Scaling_Vision-and-Language_Navigation_With_Synthetic_Instructions_and_CVPR_2023_paper.html">📚</a></li>
     <li>Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation <a href="https://arxiv.org/abs/1905.12255">📚</a></li>
     <li>Iterative Vision-and-Language Navigation  <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Krantz_Iterative_Vision-and-Language_Navigation_CVPR_2023_paper.html">📚</a></li>
    <li>GRIDTOPIX : Training Embodied Agents with Minimal Supervision <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Jain_GridToPix_Training_Embodied_Agents_With_Minimal_Supervision_ICCV_2021_paper.pdf">📚</a> <a href="https://unnat.github.io/gridtopix">🌍</a></li>
  <li>On the Limits of Evaluating Embodied Agent Model Generalization Using Validation Sets <a href="https://aclanthology.org/2022.insights-1.15.pdf">📚</a></li>
  </ul>
</details>


<details>
  <summary><b>Topic 3: Learning about visual sensory information through interaction </b></summary>
  <ul>
    <li> Scene Graph Contrastive Learning for Embodied Navigation <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Singh_Scene_Graph_Contrastive_Learning_for_Embodied_Navigation_ICCV_2023_paper.pdf">📚</a></li>
    <li> Learning Navigational Visual Representations with Semantic Map Supervision <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Learning_Navigational_Visual_Representations_with_Semantic_Map_Supervision_ICCV_2023_paper.pdf">📚</a></li>
    <li> Topological Semantic Graph Memory for Image-Goal Navigation <a href="https://proceedings.mlr.press/v205/kim23a/kim23a.pdf">📚</a></li>
    <li> Object-Goal Visual Navigation via Effective Exploration of Relations among Historical Navigation States  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Object-Goal_Visual_Navigation_via_Effective_Exploration_of_Relations_Among_Historical_CVPR_2023_paper.pdf">📚</a></li>
    <li> One-4-All: Neural Potential Fields for Embodied Navigation <a href="https://arxiv.org/pdf/2303.04011.pdf">📚</a></li>
    <li> 🏅 Emergence of Maps in the Memories of Blind Navigation Agents (ICLR'23 Outstanding Paper) <a href="https://iclr.cc/virtual/2023/oral/12560">📚</a></li>
    <li> Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks <a href="https://arxiv.org/abs/1903.03878">📚</a></li>
    <li>Graph Attention Memory for Visual Navigation <a href="https://arxiv.org/abs/1905.13315">📚</a></li>
    <li>Instance-Specific Image Goal Navigation: Training Embodied Agents to Find Object Instances <a href="https://arxiv.org/pdf/2211.15876.pdf">📚</a></li>
    <li>Navigating to Objects Specified by Images <a href="https://arxiv.org/pdf/2304.01192.pdf">📚</a> <a href="https://jacobkrantz.github.io/modular_iin">🌍</a></li>
    <li> TIDEE: Tidying Up Novel Rooms using Visuo-Semantic Commonsense Priors <a href="https://arxiv.org/abs/2207.10761">📚</a> <a href="https://tidee-agent.github.io/">🌍</a></li>
    <li>Egocentric Planning for Scalable Embodied Task Achievement <a href="https://arxiv.org/pdf/2306.01295.pdf">📚</a></li>
    <li> ALP: Action-Aware Embodied Learning for Perception <a href="https://arxiv.org/pdf/2306.10190.pdf">📚</a></li>
    <li> Simple but Effective: CLIP Embeddings for Embodied AI <a href="https://arxiv.org/abs/2111.09888">📚</a></li>
    <li> Continuous Scene Representations for Embodied AI <a href="https://arxiv.org/abs/2203.17251">📚</a></li>
    <li> Graph-based Environment Representation for Vision-and-Language Navigation in Continuous Environments<a href="https://arxiv.org/pdf/2301.04352.pdf">📚</a></li>
    <li> Learning Affordance Landscapes for Interaction Exploration in 3D Environments <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/15825aee15eb335cc13f9b559f166ee8-Paper.pdf">📚</a></li>
   <li> PASTA: Pretrained Action-State Transformer Agents <a href="https://arxiv.org/abs/2307.10936">📚</a></li>
  </ul>
</details>

<details>
  <summary><b>Topic 4: Learning about language and language-guided interaction </b></summary>
  <ul>
    <li>Plan4MC: Skill reinforcement learning and planning for open-world Minecraft tasks <a href="https://arxiv.org/abs/2303.16563">📚</a></li> 
    <li>VOYAGER: An Open-Ended Embodied Agent with Large Language Models <a href="https://arxiv.org/abs/2305.16291">📚</a></li>
    <li>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents <a href="https://arxiv.org/pdf/2302.01560.pdf">📚</a></li> 
    <li>Embodied Task Planning with Large Language Models <a href="https://arxiv.org/abs/2307.01848">📚</a></li>
    <li> Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning <a href="https://arxiv.org/abs/2305.18499">📚</a></li>
    <li>Chasing Ghosts: Instruction Following as Bayesian State Tracking  <a href="https://arxiv.org/pdf/1907.02022.pdf">📚</a></li> 
    <li>Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Context-Aware_Planning_and_Environment-Aware_Memory_for_Instruction_Following_Embodied_Agents_ICCV_2023_paper.pdf">📚</a></li>
    <li>SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation <a href="https://openreview.net/pdf?id=E5EoQqCVYX">📚</a></li>
    <li>Building Cooperative Embodied Agents Modularly with Large Language Models <a href="https://arxiv.org/abs/2305.15695">📚</a></li>
    <li>Asking Before Action: Gather Information in Embodied Decision Making with Language Models <a href="https://arxiv.org/pdf/2307.02485.pdf">📚</a></li>
    <li>Language Models Meet World Models: Embodied Experiences Enhance Language Models <a href="https://arxiv.org/pdf/2305.10626.pdf">📚</a></li>
    <li>DANLI: Deliberative Agent for Following Natural Language Instructions <a href="https://arxiv.org/pdf/2210.12485.pdf">📚</a></li>
    <li>3D-LLM: Injecting the 3D World into Large Language Models <a href="https://arxiv.org/abs/2307.12981">📚</a></li>
    <li>EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought <a href="https://arxiv.org/abs/2305.15021">📚</a></li>
    <li>PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World <a href="https://arxiv.org/abs/2106.00188">📚</a></li>
    <li>Embodied Executable Policy Learning with Language-based Scene Summarization<a href="https://arxiv.org/pdf/2306.05696.pdf">📚</a></li>
  </ul>
</details>

<details>
  <summary><b>Topic 5: Dive into robotics, Sim2Sim/Sim2Real Transfer, Emergent Embodied Communication, Multi-agent Embodied Collaboration</b></summary>
  <ul>
    <li> Eureka: Human-Level Reward Design via Coding Large Language Models <a href="https://arxiv.org/abs/2310.12931">📚</a></li> 
    <li> PaLM-E: An Embodied Multimodal Language Model <a href="https://palm-e.github.io/assets/palm-e.pdf">📚</a></li>
    <li> Learning Interactive Real-World Simulators <a href="https://arxiv.org/abs/2310.06114">📚</a> <a href="https://universal-simulator.github.io/unisim/">🌍</a></li>
    <li> Open X-Embodiment: Robotic Learning Datasets and RT-X Models <a href="https://arxiv.org/abs/2310.08864">📚</a> <a href="https://robotics-transformer-x.github.io/">🌍</a></li>
    <li> RT-2: Vision-Language-Action Models <a href="https://robotics-transformer2.github.io/assets/rt2.pdf">📚</a> <a href="https://robotics-transformer2.github.io/">🌍</a></li>
    <li> Scaling Robot Learning with Semantically Imagined Experience <a href="https://arxiv.org/abs/2302.11550">📚</a> <a href="https://diffusion-rosie.github.io/">🌍</a></li>
    <li> AR2-D2:Training a Robot Without a Robot <a href="https://arxiv.org/abs/2306.13818">📚</a> <a href="www.ar2d2.site">🌍</a></li>
    <li> IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience<a href="https://arxiv.org/abs/2305.01098">📚</a> <a href="https://www.joannetruong.com/projects/i2o.html">🌍</a></li>
    <li> VIMA: General Robot Manipulation with Multimodal Prompts <a href="https://vimalabs.github.io/assets/vima_paper.pdf">📚</a> <a href="https://vimalabs.github.io/">🌍</a></li>
    <li> AdaptSim: Task-Driven Simulation Adaptation for Sim-to-Real Transfer <a href="https://arxiv.org/abs/2302.04903">📚</a> <a href="https://irom-lab.github.io/AdaptSim/">🌍</a></li>
    <li> RoboCat: A self-improving robotic agent <a href="https://arxiv.org/abs/2306.11706">📚</a> <a href="https://www.deepmind.com/blog/robocat-a-self-improving-robotic-agent">🌍</a></li>
    <li> Policy Stitching: Learning Transferable Robot Policies <a href="https://arxiv.org/abs/2309.13753">📚</a> <a href="http://generalroboticslab.com/PolicyStitching/">🌍</a></li> 
    <li> EC2 : Emergent Communication for Embodied Control <a href="https://arxiv.org/abs/2304.09448">📚</a></li>
    <li> Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Patel_Interpretation_of_Emergent_Communication_in_Heterogeneous_Collaborative_Embodied_Agents_ICCV_2021_paper.pdf">📚</a></li>
    <li> Heterogeneous Embodied Multi-Agent Collaboration <a href="https://arxiv.org/abs/2307.13957">📚</a> <a href="https://hetercol.github.io/">🌍
    </a></li>
    <li> Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments <a href="https://arxiv.org/pdf/2204.09667.pdf">📚</a> <a href="https://jacobkrantz.github.io/sim-2-sim">🌍
    </a></li>
  </ul>
</details>


<details>
  <summary><b>Topic 6: Diffusion Policies</b></summary>
  <ul>
    <li>Diffusion Policy: Visuomotor Policy Learning via Action Diffusion <a href="https://arxiv.org/pdf/2303.04137.pdf">📚</a></li>
    <li>NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration <a href="https://arxiv.org/pdf/2310.07896.pdf">📚</a></li>
    <li>PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play <a href="https://openreview.net/forum?id=afF8RGcBBP">📚</a></li>
    <li>Learning Universal Policies via Text-Guided Video Generation <a href="https://arxiv.org/pdf/2302.00111.pdf">📚</a></li>
    <li>Compositional Foundation Models for Hierarchical Planning <a href="https://arxiv.org/pdf/2309.08587.pdf">📚</a></li>
    <li>XSkill: Cross Embodiment Skill Discovery <a href="https://arxiv.org/pdf/2307.09955.pdf">📚</a></li>
  </ul>
</details>


## Additional Resources
- 🏠 [Course Syllabus](https://isminoula.github.io/cs6604FA23/)
- 🗓️ [Seminar Schedule](https://isminoula.github.io/cs6604FA23/#schedule)
- 🧠 [Reinforcement Learning Supplemental Reading](https://lilianweng.github.io/posts/2018-02-19-rl-overview/)
- 📊 [Transformers Supplemental Reading](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)
- 🌐 Diffusion for [robotics](https://github.com/mbreuss/diffusion-literature-for-robotics) and [RL](https://github.com/opendilab/awesome-diffusion-model-in-rl)
