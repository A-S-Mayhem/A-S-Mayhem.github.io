---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Welcome! I am currently a fourth-year Ph.D. student at the [Department of Computer Science, Emory University](https://www.cs.emory.edu/home/), where I am fortunate to be advised by Dr. [Liang Zhao](https://cs.emory.edu/~lzhao41/). Previously, I received my master's degree in Statistics from George Washington University in 2020. I received my bachelor's degree in Mathematics from the [School of Mathematical Science, Fudan University](https://math.fudan.edu.cn/) in Shanghai, China in 2018. I previously worked as a research intern at [NEC Lab America](https://www.nec-labs.com/).

Research Interests
======
I am interested in designing **efficient, generalizable, and explainable learning algorithms with theoretical guarantees**. Specifically, my current research topics include but are not limited to **1.** Learning strategies for domain transfer problems, such as multi-task learning (MTL), domain adaptation (DA), and domain generalization (DG). **2.** Large-scale machine learning algorithms with better scalability and performance, such as distributed training for Graph Neural Networks (GNNs) and model compression & acceleration of LLMs, etc. **3.** Online learning such as continual/lifelong learning with memory replay and neuro-inspiration. 

# Selected Projects

This section highlights my research projects, categorized by major themes and subtopics.

## 1. Domain and Knowledge Transfer

This project focuses on enhancing machine learning models' adaptability and effectiveness across various domains.

### a) Multi-task Learning
- **Sign-Regularized Multi-Task Learning**  
  _SDM 2023_  
  [Paper](https://epubs.siam.org/doi/pdf/10.1137/1.9781611977653.ch89) | [Code](https://github.com/BaiTheBest/SRML)
- **Saliency-Regularized Deep Multi-Task Learning**  
  _KDD 2022_  
  [Paper](https://dl.acm.org/doi/pdf/10.1145/3534678.3539442) | [Code](https://github.com/BaiTheBest/SRDML)

### b) Domain Adaptation
- **Deep Spatial Domain Generalization**  
  _ICDM 2022_  
  [Paper](https://arxiv.org/pdf/2210.00729) | [Code](https://github.com/dyu62/Deep-domain-generalization)

### c) Domain Generalization
- **Temporal Domain Generalization with Drift-Aware Dynamic Neural Networks**  
  _ICLR 2023 (Oral)_  
  [Paper](https://openreview.net/pdf?id=sWOsRj4nT1n) | [Code](https://github.com/BaiTheBest/DRAIN)

## 2. Large-Scale Machine Learning

Exploring scalable solutions in machine learning, particularly in graph neural networks and large language models.

- **Distributed Training for Graph Neural Networks (GNNs)**  
  Various publications like "Staleness-Alleviated Distributed GNN Training" and more.
- **Model Compression & Acceleration of LLMs**  
  Ongoing research and development in this area.

## 3. Continual and Lifelong Learning

Focusing on memory-replay and neuro-inspiration approaches for continual learning without sub-categories.

- **Saliency-Guided Hidden Associative Replay for Continual Learning**  
  _AMHN Workshop @NeurIPS 2023_  
  [Paper](https://openreview.net/pdf?id=Fhx7nVoCQW) | [Code](https://github.com/BaiTheBest/SHARC)
- **Saliency-Augmented Memory Completion for Continual Learning**  
  _SDM 2023_  
  [Paper](https://epubs.siam.org/doi/pdf/10.1137/1.9781611977653.ch28) | [Code](https://github.com/BaiTheBest/SAMC)


Services and Awards
======
* PC member for AISTATS (23'24'), NeurIPS (22'23'), ICLR (24'), AAAI (24')
* Reviewer for KDD, ICML, ICLR, ICDM
* 2023 SDM student travel award
* 2022 CIKM student travel award
* 2022 KDD student travel award



