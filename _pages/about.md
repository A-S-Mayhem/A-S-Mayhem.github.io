---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Welcome! I am currently a fourth-year Ph.D. student at the [Department of Computer Science, Emory University](https://www.cs.emory.edu/home/), where I am fortunate to be advised by Dr. [Liang Zhao](https://cs.emory.edu/~lzhao41/). Previously, I received my master's degree in Statistics from George Washington University in 2020. I received my bachelor's degree in Mathematics from the [School of Mathematical Science, Fudan University](https://math.fudan.edu.cn/) in Shanghai, China in 2018. I previously worked as a research intern at [NEC Lab America](https://www.nec-labs.com/).

Research Interests
======
I am interested in designing **efficient, generalizable, and explainable learning algorithms with theoretical guarantees**. Specifically, my current research topics include but are not limited to **1.** Learning strategies for domain transfer problems, such as multi-task learning (MTL), domain adaptation (DA), and domain generalization (DG). **2.** Large-scale machine learning algorithms with better scalability and performance, such as distributed training for Graph Neural Networks (GNNs) and model compression & acceleration of LLMs, etc. **3.** Online learning such as continual/lifelong learning with memory replay and neuro-inspiration. 

Selected Projects
=====
## 1. Domain and Knowledge Transfer
Focusing on enhancing machine learning models' adaptability and effectiveness across various domains/tasks.
### a) Multi-task Learning
- [**Sign-Regularized Multi-Task Learning**](https://epubs.siam.org/doi/pdf/10.1137/1.9781611977653.ch89)   
  _SDM 2023_  
- [**Saliency-Regularized Deep Multi-Task Learning**](https://dl.acm.org/doi/pdf/10.1145/3534678.3539442)  
  _KDD 2022_  

### b) Domain Adaptation
- [**Prompt-based Domain Discrimination for Multi-source Time Series Domain Adaptation**](https://arxiv.org/abs/2312.12276)   
  _Preprint_  

### c) Domain Generalization
- [**Temporal Domain Generalization with Drift-Aware Dynamic Neural Networks**](https://openreview.net/pdf?id=sWOsRj4nT1n)  
  _ICLR 2023_   
- [**Deep Spatial Domain Generalization**](https://arxiv.org/pdf/2210.00729)  
  _ICDM 2022_ 

## 2. Efficient Large-Scale Machine Learning
Exploring scalable solutions in machine learning, particularly in GNNs and LLMs.
### a) Distributed Training for Graph Neural Networks (GNNs)
- [**Distributed Graph Neural Network Training with Periodic Stale Representation Synchronization**](https://arxiv.org/pdf/2206.00057)   
  _Preprint_  
- [**Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction**](https://arxiv.org/pdf/2308.13466)  
  _Preprint_  
### b) Model Compression & Acceleration of LLMs 
  Ongoing research and development in this area.

## 3. Continual and Lifelong Learning
Focusing on memory-replay and neuro-inspiration approaches for continual learning.
- [**Saliency-Guided Hidden Associative Replay for Continual Learning**](https://openreview.net/pdf?id=Fhx7nVoCQW)   
  _AMHN Workshop @NeurIPS 2023_  
- [**Saliency-Augmented Memory Completion for Continual Learning**](https://epubs.siam.org/doi/pdf/10.1137/1.9781611977653.ch28)  
  _SDM 2023_  


Services and Awards
======
* PC member for AISTATS (23'24'), NeurIPS (22'23'), ICLR (24'), AAAI (24')
* Reviewer for KDD, ICML, ICLR, ICDM
* 2023 SDM student travel award
* 2022 CIKM student travel award
* 2022 KDD student travel award



