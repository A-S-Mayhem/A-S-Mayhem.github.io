---
permalink: /
title: ""
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

:wave: Hello! I'm Nandan Thakur (नंदन ठाकुर / নন্দন ঠাকুর). 

I'm second-year PhD student working on "Data-Efficient and Multilingual Information Retrieval" at the [University of Waterloo](https://uwaterloo.ca/) in Canada. 
I am supervised by [Prof. Jimmy Lin](https://cs.uwaterloo.ca/~jimmylin/). 
I recently finished a 6 month internship at Google Research in Mountain View, California in Winter 2023. 

Prior to joining my PhD, I was as a research assistant (RA) at the [UKP Lab, Technical University of Darmstadt](https://www.informatik.tu-darmstadt.de/ukp/ukp_home/index.en.jsp) in Germany working with [Prof. Iryna Gurevych](https://www.informatik.tu-darmstadt.de/ukp/ukp_home/head_ukp/index.en.jsp) and [Dr. Nils Reimers](https://www.nils-reimers.de/). 
I completed my bachelor's in Electronics and Instrumentation with a minor in Finance from [Birla Institute of Technology and Science, Pilani (BITS Pilani)](https://www.bits-pilani.ac.in/) in 2018.
Besides these experiences, I have over a year of industrial experience in Machine Learning, working as a Data Scientist in [KNOLSKAPE](https://knolskape.com/) and interning at [Gibson Lab](https://www.embl.org/groups/gibson/) in EMBL Heidelberg and [Belong.co](https://belong.co/). 


## :mag: Research Interests
My research interests lie within NLP and Information Retrieval. I like to study and work on building easy, scalable and efficient systems in the neural search paradigm. My long-term research ambition lies in building robust and generalizable retrieval models to help serve information better for everyone. Few of my notable works include developing the BEIR Benchmark [(Thakur et al., 2021)](https://openreview.net/forum?id=wCu6T5xFjeJ) and Augmented SBERT [(Thakur et al., 2021)](https://aclanthology.org/2021.naacl-main.28/). 

### :bar_chart: Data Efficiency: Transfer Learning, Data Augmentation and Zero-shot Learning 
In order to train neural retriever systems, large amounts of human-labeled training data is required which is often cumbersome and expensive to generate for real-world tasks. Data efficiency plays a crucial role to address this challenge. Transfer learning is motivated by distilling knowledge from pretrained models or LLMs to train data-efficient models. Data Augmentation techniques involve generating high-quality synthetic data for training purposes. Zero-shot learning enables models to generalize to unseen classes or queries without any training examples.

### :speaking_head: Languages: Multilingual Retrieval
Multilingual Retrieval aims to provide relevant search results for user searching across multiple languages. Multilingual retrieval involves various challenges, including language mismatch, translation ambiguity, and language-specific resource limitations. To overcome these challenges, machine translation, cross-lingual IR and mulitilingual embeddings have been employed. However, training data for such tasks is even more scarce than English making it a important challenge.

## :fire: Recent News
* <sub>**[Jul 2023]** I will be attending the SIGIR 2023 virtual conference being held in Taipei, Taiwan! Say hi to me (virtally)!</sub>
* <sub>**[Jul 2023]** I will be attending the ACL 2023 in-person conference being held in Toronto, Canada! Say hi to me!</sub>
* <sub>**[Jun 2023]** The Domain Adaptation Paper has been accepted in [ReNeuIR 2023 Workshop](https://reneuir.org/) to be held jointly with SIGIR 2023!</sub>
* <sub>**[Jun 2023]** The SPRINT Toolkit Paper has been accepted in [SIGIR 2023 Resource Track](https://sigir.org/sigir2023/)!</sub>
* <sub>**[May 2023]** The MIRACL Paper has been accepted in [TACL 2023](https://transacl.org/index.php/tacl)!</sub>
* <sub>**[May 2023]** The Evaluating Embedding API Paper has been accepted in [ACL 2023 Industry Track](https://2023.aclweb.org/calls/industry_track/)!</sub>
* <sub>**[Sep 2022]** The MIRACL Challenge was accepted in [WSDM Cup 2023](). The Challenge is now live and looking for participants.</sub> 
* <sub>**[Aug 2022]** I started my Fall Internship at the Language Team in [Google Research](https://research.google/teams/language/) with Daniel Cer and Jianmo Ni.</sub>

<details markdown=1><summary markdown="span"><b>Click here for older news</b></summary>
* <sub>**[Mar 2021]** Augmented SBERT got accepted as a long paper at NAACL 2021! [PDF](https://aclanthology.org/2021.naacl-main.28/) []
* <sub>**[Feb 2021]** Designed and Attended The First ELLIS NLP 2021 Workshop Website. [<a href="https://sites.google.com/view/ellisnlp2021/organization?authuser=0#h.jhoas58vwjmn">link</a>]</sub>
* <sub>**[Jan 2021]** Designed the Second 2021 SustaiNLP Workshop Website. [<a href="https://sites.google.com/view/sustainlp2021">link</a>]</li>
* <sub>**[Nov 2020]** [Cancelled (COVID-19)] Selected to speak at PyCon Italia 2020: "Extract or Replace Keywords in sentences 28x times faster than Regex - FlashText". [<a href="https://pycon.it/en/talk/extract-or-replace-keywords-in-sentences-28x-times-faster-than-regex-flashtexttm">Abstract</a>] [<a href="https://www.youtube.com/watch?v=s8WP79QU1zw">YouTube</a>] [<a href="https://github.com/vi3k6i5/flashtext">Github</a>]</sub>
* <sub>**[Jul 2020]** ArgumenText won 4th place amongst 3000+ startups in Nordbayerischen Businessplan. [<a href="https://www.baystartup.de/startupdate/aus-den-wettbewerben/sieger-der-phase-2-im-businessplan-wettbewerb-nordbayern-2020">link</a>] </sub>
* <sub>**[Jul 2020]** I attended the Association for Computational Linguistics (ACL) 2020 virtual conference.</sub>