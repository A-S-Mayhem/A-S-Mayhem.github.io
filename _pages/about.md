---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Welcome! I am a final-year Ph.D. student at the [Department of Computer Science, Emory University](https://www.cs.emory.edu/home/), where I am fortunate to be advised by Dr. [Liang Zhao](https://cs.emory.edu/~lzhao41/). Previously, I received my master's degree in Statistics from George Washington University in 2020. I received my bachelor's degree in Mathematics from the [School of Mathematical Science, Fudan University](https://math.fudan.edu.cn/) in Shanghai, China in 2018. I worked as a research intern at [Argonne National Laboratory](https://www.anl.gov/) and  [NEC Lab America](https://www.nec-labs.com/).

<code style="color : Red">News:</code> **I am actively looking for applied scientist and machine learning engineer positions starting in 2025. Please feel free to DM me.**

Research Interests
======
I am interested in designing **efficient, generalizable, and explainable learning algorithms with theoretical guarantees**. Specifically, my current research topics include but are not limited to **1.** Developing various learning algorithms for knowledge/domain transfer, such as multi-task learning (MTL), domain adaptation (DA), and domain generalization (DG). **2.** Designing large-scale machine learning algorithms with enhanced efficiency, such as model compression & acceleration of LLMs and distributed training for deep neural networks. **3.** Online learning such as continual/lifelong learning with memory replay and neuro-inspiration. 

Selected Projects
=====
## 1. Domain and Knowledge Transfer
This project focuses on enhancing machine learning models' adaptability and effectiveness across various domains/tasks.
### a) Multi-task Learning
- [**Sign-Regularized Multi-Task Learning**](https://epubs.siam.org/doi/pdf/10.1137/1.9781611977653.ch89)   
  _SDM 2023_  
- [**Saliency-Regularized Deep Multi-Task Learning**](https://dl.acm.org/doi/pdf/10.1145/3534678.3539442)  
  _KDD 2022_  

### b) Domain Adaptation
- [**Prompt-Based Domain Discrimination for Multi-source Time Series Domain Adaptation**](https://arxiv.org/abs/2312.12276)   
  _KDD 2024_  

### c) Domain Generalization
- [**Temporal Domain Generalization with Drift-Aware Dynamic Neural Networks**](https://openreview.net/pdf?id=sWOsRj4nT1n)  
  _ICLR 2023_   

## 2. Efficient Large-Scale Machine Learning
Exploring scalable solutions in machine learning.
### a) Model Compression & Acceleration of LLMs 
- [**Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models**](https://arxiv.org/abs/2401.00625)   
  _Under review of CSUR_
- [**SparseLLM: Towards Global Pruning for Pre-trained Language Models**](https://arxiv.org/pdf/2402.17946v3)   
  _Preprint_ 

### b) Distributed Training for Deep Neural Networks
- [**Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction**](https://arxiv.org/pdf/2308.13466)  
  _Preprint_

## 3. Neuro-Inspired Continual Learning
Focusing on memory-replay and neuro-inspiration approaches for continual learning.
- [**Saliency-Guided Hidden Associative Replay for Continual Learning**](https://openreview.net/pdf?id=Fhx7nVoCQW)   
  _AMHN Workshop @NeurIPS 2023_  
- [**Saliency-Augmented Memory Completion for Continual Learning**](https://epubs.siam.org/doi/pdf/10.1137/1.9781611977653.ch28)  
  _SDM 2023_  


Services and Awards
======
* PC member for AISTATS (23'24'), NeurIPS (22'23'), ICLR (24'), AAAI (24')
* Reviewer for KDD, ICML, ICLR, ICDM
* 2023 SDM student travel award
* 2022 CIKM student travel award
* 2022 KDD student travel award



