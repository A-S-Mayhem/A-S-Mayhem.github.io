---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am Yusheng Su, a postdoctoral researcher at [CMU](https://www.cmu.edu/)/[MBZUAI](https://mbzuai.ac.ae/) hosted by Prof. [Eric Xing](http://www.cs.cmu.edu/~epxing/). I completed my Ph.D. degree at [Department of Computer Science and Technology](http://www.cs.tsinghua.edu.cn/), [Tsinghua University](https://www.tsinghua.edu.cn/publish/thu2018en/index.html). During my Ph.D (2019-2023), I was advised by Prof. [Zhiyuan Liu](https://scholar.google.com/citations?user=dT0v5u0AAAAJ&hl=zh-TW) and the member of THUNLP Lab ([Link1](https://github.com/thunlp), [Link2](https://github.com/OpenBMB)) led by Prof. [Maosong Sun](https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm). I'm on the job market (4-year experiences in LLM). [[Google Schlar]](https://scholar.google.com/citations?user=xwy6Va4AAAAJ&hl=en)
<!--[[CV]](https://www.dropbox.com/s/4j059nncu2k6lrf/Yusheng_Su_Resume_2023_05_15.pdf?dl=0)-->

## Research
<!--My research interests are in the theory and practice of building machine learning systems that remain reliable when deployed in real application contexts. For example:-->


<!--My research spans the areas of natural language processing and machine learning. My long-term goal of research is to build a general-purpose machine learning system that has human-like cognitive capacities (e.g., understanding, reasoning, etc) and can generalize to various real-world applications efficiently. Toward this goal, my work spans across:-->

I have 4-year experiences in LLMs. My research spans the areas of natural language processing and machine learning. My long-term goal of research is to build a general-purpose machine learning system that can <b>sufficiently learn</b> human-like cognitive capacities (e.g., understanding, reasoning, reflecting, etc.), <b>efficiently adapt</b> to various tasks, and remain <b>reliable when deployed</b> in real applications. Toward this goal, my work spans across:

* <b>General-purpose model. (Model Learning)</b> Building pre-trained foundation models that can actively access to various data sources (e.g., knowledge bases, web pages, textual documents, etc) and acquire knowledge to improve the abilities of understanding, reasoning, etc. ([CokeBERT](https://arxiv.org/abs/2009.13964), [CSS-LM](https://arxiv.org/abs/2102.03752), [CPM](https://www.sciencedirect.com/science/article/pii/S266665102100019X), [Knowledge Inheritance](https://aclanthology.org/2022.naacl-main.288/))

* <b>Computational efficiency method. (Model Manipulating)</b> Developing theory, tools, and algorithms to computation-friendly and efficiently manipulate large-scale pre-trained foundation models toward downstream tasks (e.g., prompt tuning methods, in-context learning, etc.). ([Prompt Transferability](https://aclanthology.org/2022.naacl-main.290/), [IPT](https://arxiv.org/abs/2110.07867), [Parameter-efficient Fine-tuning Survey](https://arxiv.org/abs/2203.06904))

* <b>AI Agent. (Model Controlling)</b> Designing methods to understand the emerging human-like capacities of contemporary foundation models and ensure they are reliable (perform tasks in accordance with human's real intentions and follow safety/ethical rules) and accomplish complex real-world tasks. ([Model Emotion](https://arxiv.org/abs/2302.09582), [Tool Leaning](https://arxiv.org/abs/2304.08354), [MultiAgent](https://arxiv.org/abs/2308.10848), [ChatDev](https://arxiv.org/abs/2307.07924), [Chateval](https://arxiv.org/abs/2308.07201))

<!--
the models perform tasks in accordance with human's real intentions. ([Model Emotion](https://arxiv.org/abs/2302.09582), [Removing Backdoors](), [Tool Leaning](https://arxiv.org/abs/2304.08354))
-->

<!--
(https://www.dropbox.com/s/0yuudhokyyomkup/Removing_Backdoors_in_Pre-trained_Models_by_Regularized_Continual_Pretraining.pdf?dl=0)
-->


<!--
My research spans the areas of natural language processing and machine learning. At the current stage, I am particularly interested in <b>large-scale pre-trained models (LLMs)</b>. My research aims to build more general LLMs and develop efficient paradigms for them to deploy in real-world applications.

* <b>General LLMs.</b> Building LLMs that can actively access to various data sources (e.g., knowledge bases, web pages, textual documents, etc ) and acquire knowledge to accomplish new tasks.

* <b>Efficient Paradigms.</b> Developing theory, tools, and algorithms to efficiently adapt LLMs downstream tasks (e.g., parameter-efficient tuning methods, in-context learning, etc.).
-->


<!--My research interests are in the theory and practice of building machine learning systems that remain reliable when deployed in real application contexts. For example:-->

<!--exploring the mechanism (theory) and mainpulation (practice) of <b>large-scale pre-trained models (LLMs)</b> to efficiently and effectively adapt to real-world applications. To achieve these goals, I focus on the following directions-->

<!--My research spans the areas of natural language processing and machine learning. At the current stage, I am particularly interested in exploring the mechanism (theory) and mainpulation (practice) of <b>large-scale pre-trained models (LLMs)</b> to efficiently and effectively adapt to real-world applications. To achieve these goals, I focus on the following directions:-->

<!--
* <b>Parameter-Efficient Tuning.</b> Adapting LLMs to downstream tasks incurs a huge computational burden. How can we adapt LLMs to downstream tasks efficiently (with less computational burden)? <!--Driving LLMs with minimal parameters (parameter-efficient tuning) is worth exploring.


* <b>Knowledge Acquisition From Multi-Modal Data.</b> Adapting LLMs to downstream tasks requires training on task-specific manually annotated data. How can we equip LLMs with the ability to actively acquire task-specific knowledge from multi-modal data in the open world?
-->


<!--Human-Like Cognitive Abilities and Human-computer interaction are the furthure direction that I want to explore-->


  
<!--<b>* <b>Human-Like Cognitive Abilities.</b> Adapting LLMs to downstream tasks requires training on task-specific manually annotated data, which needs manual effort involving labeling. How can we acquire task-specific knowledge effectively (without manual effort)? Equipping LLMs with human-like cognitive abilities to leverage open-world information is a promising direction.-->

<!--<b>AI4Science.</b> How many human-like capabilities LLMs learn in the pre-training stage remains unknown. To further understand, we develop tools to explore and conduct a series of analyses.-->



<!--My research interests lie within the intersection of Natural Language Processing and Machine Learning, particularly in pre-trained language models (PLMs). At the current stage, I am particularly studying how to effectively and efficiently adapt PLMs to complex downstream tasks. To achieve these goals, I specifically focus on knowledge-guided methods, transfer learning, in-context learning, and parameter-efficient tuning.-->

<!--You can find my CV [here](/cv/).-->

## On The Job Market
I'm on the job market, looking for Post-doc and industrial research positions, starting in 2023 Autumn. [[Google Schlar](https://scholar.google.com/citations?user=xwy6Va4AAAAJ)] 
<!-- [[CV]](https://www.dropbox.com/s/4j059nncu2k6lrf/Yusheng_Su_Resume_2023_05_15.pdf?dl=0) -->
<!-- (in North America) -->

<!-- ## Coming Soon
Will share our work: [On Transferability of Prompt Tuning for Natural Language Processing](https://arxiv.org/abs/2111.06719)<br>
<b>Conference</b>: NAACL 2022, Seattle, Washington <br>
<b>Session</b>: Efficient Methods in NLP <br>
<b>Time</b>: July 10, 2022, 10:45 â€“ 12:15 (UTC - 7:00) <br>
</font> -->


## News
<!-- 
* [Mar. 2022] Got one paper accepted at ACL 2022. 
* [Aug. 2021] Got one paper accepted at IEEE/TASLP 2021. 
* [Feb. 2021] Got one paper accepted at WWW 2021. 
* [Aug. 2020] Got one paper accepted at EMNLP 2020. 
</font> -->

* [Jul. 2023] Invited Research Talk at [Eric P. Xing](http://www.cs.cmu.edu/~epxing/)'s [CMU](https://www.cmu.edu/)/[MBZUAI](https://mbzuai.ac.ae/) team. Topic: Efficient Adaptation of Large-scale Pre-trained Language Models [[slide](https://drive.google.com/file/d/1ow2Q-YUOk-Hyvou3VAH88yvGlQzS7SFN/view?usp=sharing)].
* [May. 2023] [AgentVerse](https://github.com/OpenBMB/AgentVerse) was published. It provides a flexible framework that simplifies the process of building LLM-based agent to accomplish various tasks in real world.
* [Apr. 2023] [Tool Learning](https://arxiv.org/pdf/2304.08354.pdf) survey was published. It demonstrates how recently proposed LLMs leverage the emerging ability to comprehend, create, and manipulate tools, thereby assisting humans in accomplishing their intended objectives.
* [Jan. 2023] [Parameter-efficient Fine-tuning of Large-scale Pre-trained Language Models](https://www.nature.com/articles/s42256-023-00626-4) was accepted by [Natural Machine Intelligence](https://www.nature.com/natmachintell/) (Cover Article).
* [Sep. 2022] Advancement of Foundation Models. Invited Talk (on-line) @ [SAP - AI Research](https://www.sap.com/products/artificial-intelligence.html), Headquarter, Germany.
* [Jul. 2022] I will attend [NAACL 2022](https://2022.naacl.org/) in person to orally present our works and look forward to meeting new/old friends in Seattle.
* [Apr. 2022] [Transferability of Prompt Tuning](https://aclanthology.org/2022.naacl-main.290/) and [Knowledge Inheritance](https://aclanthology.org/2022.naacl-main.288/) are accepted to [NAACL 2022](https://2022.naacl.org/).

<!-- <font color="gray"> </font> -->




## Publications

* <b>Parameter-efficient Fine-tuning of Large-scale Pre-trained Language Models</b>\
*Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, <b>Yusheng Su</b>, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, Maosong Sun.*\
Nature Machine Intelligence 2023 (<b>Cover Article</b>). [[pdf](https://www.nature.com/articles/s42256-023-00626-4)] [[code](https://github.com/thunlp/OpenDelta)]

* <b>On Transferability of Prompt Tuning for Natural Language Processing</b>\
*<b>Yusheng Su</b>, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Zhiyuan Liu, Peng Li, Juanzi Li, Lei Hou, Maosong Sun, Jie Zhou*\
NAACL 2022 (<b>Oral</b>). [[pdf]](https://aclanthology.org/2022.naacl-main.290/) [[code]](https://github.com/thunlp/Prompt-Transferability) [[BibTex](https://aclanthology.org/2022.naacl-main.290.bib)] [[slide](https://drive.google.com/file/d/1OSmU3s7DOv-Gux5JcjuY0w79MzRRJuiA/view?usp=sharing)] [[video](https://www.youtube.com/watch?v=KVgmtgMQ3ig)]

* <b>Knowledge Inheritance for Pre-trained Language Models</b>\
*Yujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han, Zhengyan Zhang, <b>Yusheng Su</b>, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou*\
NAACL 2022 (<b>Oral</b>). [[pdf](https://aclanthology.org/2022.naacl-main.288/)] [[code](https://github.com/thunlp/Knowledge-Inheritance)]

* <b>Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning</b>\
*Yujia Qin, Xiaozhi Wang, <b>Yusheng Su</b>, Yankai Lin, Ning Ding, Zhiyuan Liu, Juanzi Li, Lei Hou, Peng Li, Maosong Sun, Jie Zhou*\
ACL 2022 Findings. [[pdf](https://arxiv.org/abs/2110.07867)] [[code](https://github.com/thunlp/Intrinsic-Prompt-Tuning)]

* <b>CPM: A large-scale Generative Chinese Pre-trained Language Model</b>\
*Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, <b>Yusheng Su</b>, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun*\
AI OPEN 2021. [[pdf](https://www.sciencedirect.com/science/article/pii/S266665102100019X)] [[code](https://github.com/TsinghuaAI/CPM)]

* <b>CSS-LM: A Contrastive Framework for Semi-supervised Fine-tuning of Pre-trained Language Models</b>\
*<b>Yusheng Su</b>, Xu Han, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Peng Li, Maosong Sun*\
[WWW 2021 Workshop](https://www.aminer.cn/ssl_www2021), IEEE/TASLP 2021. [[pdf](https://arxiv.org/abs/2102.03752)] [[code](https://github.com/thunlp/CSS-LM)] [[slide](https://drive.google.com/file/d/1TqAnFYL5CBwWhrDamB83akqQniVd2B9o/view?usp=sharing)]

* <b>CokeBERT: Contextual Knowledge Selection and Embedding Towards Enhanced Pre-Trained Language Models</b>\
*<b>Yusheng Su</b>, Xu Han, Zhengyan Zhang, Peng Li, Zhiyuan Liu, Yankai Lin, Jie Zhou, Maosong Sun*\
EMNLP 2020 Findings, AI OPEN 2021. [[pdf](https://arxiv.org/abs/2009.13964)] [[pdf](https://www.sciencedirect.com/science/article/pii/S2666651021000188)] [[code](https://github.com/thunlp/CokeBERT)]



## Under Review or Preprint Version <!-- Submitted for Publications-->

* <b>Arbitrary Few Parameters are Good Enough for Adapting Large-scale Pre-trained Language Models</b>\
*<b>Yusheng Su</b>, Chi-Min Chan, Jiali Cheng, Yujia Qin, Yankai Lin, Shengding Hu, Zonghan Yang, Ning Ding, Zhiyuan Liu, Maosong Sun*\
ArXiv 2023. [[pdf](https://arxiv.org/abs/2306.02320)]

* <b>Human Emotion Knowledge Representation Emerges in Large Language Models and Supports Discrete Emotion Inference</b>\
*Ming Li<sup><big>*</big></sup>, <b>Yusheng Su</b><sup><big>*</big></sup>, Hsiu-Yuan Huang, Jiali Cheng, Xin Hu, Xinmiao Zhang, Huadong Wang, Yujia Qin, Xiaozhi Wang, Zhiyuan Liu, Dan Zhang* (&nbsp;<sup><big>*</big></sup> indicates equal contribution)\
(Submitted to Nature Human Behaviour 2023). [[pdf](https://arxiv.org/abs/2302.09582)] [[code](https://github.com/thunlp/OpenNeuron)] <small>(Refactoring - User friendly toolkit coming soon)</small>

* <b>Tool Learning with Foundation Models</b>\
*Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, <b>Yusheng Su</b>, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, Maosong Sun*\
ArXiv 2023. [[pdf](https://arxiv.org/abs/2304.08354)] [[code](https://github.com/OpenBMB/BMTools)] 

* <b>AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents</b>\
*Weize Chen<sup><big>*</big></sup>, <b>Yusheng Su</b><sup><big>*</big></sup>, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou* (&nbsp;<sup><big>*</big></sup> indicates equal contribution)\
ArXiv 2023. [[pdf](https://arxiv.org/abs/2308.10848)] [[code](https://github.com/openbmb/agentverse)]

* <b>Communicative agents for software development</b>\
*Chen Qian, Xin Cong, Cheng Yang, Weize Chen, <b>Yusheng Su</b>, Juyuan Xu, Zhiyuan Liu, Maosong Sun*\
ArXiv 2023. [[pdf](https://arxiv.org/abs/2307.07924)] [[code](https://github.com/openbmb/chatdev)]

* <b>ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate</b>\
*Chi-Min Chan, Weize Chen, <b>Yusheng Su</b>, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu*\
ArXiv 2023. [[pdf](https://arxiv.org/abs/2308.07201)] [[code](https://github.com/thunlp/ChatEval)]


<!--
* <b>Removing Backdoors in Pre-trained Models by Regularized Continual Pre-training</b>\
*Biru Zhu, Ganqu Cui, Yangyi Chen, Yujia Qin, <b>Yusheng Su</b>, Lifan Yuan, Chong Fu, Yangdong Deng, Zhiyuan Liu, Maosong Sun, Ming Gu.*\
(Submitted to TACL). [[pdf]](https://www.dropbox.com/s/0yuudhokyyomkup/Removing_Backdoors_in_Pre-trained_Models_by_Regularized_Continual_Pretraining.pdf?dl=0) 
-->


## Lead/Co-lead Projects

* <b>Prompt Transferability</b>. This system assists users in building a prompt bank, allowing them to save well-trained prompts. It also enables swift access and reuse of these prompts whenever the user requires them on unseen tasks and heterogeneous models.

[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=thunlp&repo=Prompt-Transferability)](https://github.com/thunlp/Prompt-Transferability)

* <b>AgentVerse</b>. AgentVerse provides a framework that streamlines the process of developing custom multi-agent systems using LLMs in user-defined environments. This facilitates the design of more efficient multi-agent systems that can be applied to real-world applications.

[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=OpenBMB&repo=AgentVerse)](https://github.com/OpenBMB/AgentVerse)



## Joined Projects
* <b>ChatDev</b>. Create customized software using natural language idea through LLM-powered multi-agent collaboration

[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=OpenBMB&repo=ChatDev)](https://github.com/OpenBMB/ChatDev)


* <b>Tool Learning</b>. Tool learning for LLMs, open-source solutions of ChatGPT-Plugins.

[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=OpenBMB&repo=BMTools)](https://github.com/OpenBMB/BMTools)


<!--
## Teaching
* [Jul. 2022] Tsinghu University - NLP Big Model Course (Summer) [link](https://www.bilibili.com/video/BV1UG411p7zv?p=1&unique_k=OwC3PgP)
-->

## Talks
* [Jul. 2023] Invited Research Talk at [Eric P. Xing](http://www.cs.cmu.edu/~epxing/)'s [CMU](https://www.cmu.edu/)/[MBZUAI](https://mbzuai.ac.ae/) team. Topic: Efficient Adaptation of Large-scale Pre-trained Language Models [[slide](https://drive.google.com/file/d/1ow2Q-YUOk-Hyvou3VAH88yvGlQzS7SFN/view?usp=sharing)].
* [Sep. 2022] Invited Talk (on-line) @ [SAP - AI Research](https://www.sap.com/products/artificial-intelligence.html), Headquarter, Germany
* [Jul. 2022] Oral Talk (in persion) @ NAACL 2022, [[slide](https://drive.google.com/file/d/1OSmU3s7DOv-Gux5JcjuY0w79MzRRJuiA/view?usp=sharing)] [[video](https://www.youtube.com/watch?v=KVgmtgMQ3ig)]
* [Apr. 2021] Spotlight Talks (on-line) @ WWW 2021 (Self-Supervised Learning Workshop)



## Professional Services
<!--
* Reviewer: COLING 2022
* Review Committee Member: EMNLP 2022
* Reviewer: ICML 2022
* Reviewer: ACL Rolling 2022
* Reviewer: ACL Rolling 2021
* Reviewer: EMNLP/ACL/IEEE-TASLP 2021
-->
Reviewer (Since 2021): ACL, NAACL, AACL, ACL Roling, EMNLP, COLING, ICLR, ICML, IJCAI



## Internships
<!--
### Tsinghua NLP Lab. (Beijing) 09.2019 - 07.2023
* Ph.D. Student 
* Advised by [Prof. Zhiyuan Liu](http://nlp.csai.tsinghua.edu.cn/~lzy/).
</font> -->

### MediaTek. (Taiwan) 07.2018 - 08.2019
* Deep/Machine Learning Engineer Intern
* Advised by Jing-Han Wang.

### Microsoft. (Taiwan) 07.2015 - 07.2016
* Research and Development Intern
* Advised by [Kuang-Chao Yeh](https://www.linkedin.com/in/kuang-chao-yeh/) and [Gordon Chang](https://www.linkedin.com/in/gordonwinnow).

## Pre-doctoral Student Mentoring
* (Since 2021-2023) [Chi-Min Chan](https://scholar.google.com/citations?user=5U4P54wAAAAJ&oi=ao): Tsinghua University (BS) -> Hong Kong University of Science and Technology (HKUST) (MS) 
* (Since 2022-2023) Jiali Cheng: University of North Carolina (MS->PhD)  
* (Since 2022-2023) Yu Xia: Peking University (MS) -> Tsinghua University (PhD)  
* (Since 2022-2023) Xiuyuan Huang: University of Science and Technology Beijing (BS) -> Peking University (MS) 
