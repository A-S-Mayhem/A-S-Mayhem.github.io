---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a PhD candidate at NYU advised by Professor [Andrew Gordon Wilson](https://cims.nyu.edu/~andrewgw/). I work on the foundations of deep learning and focus on understanding the generalization properties of deep learning models using notions that relate to generalization such as the marginal likelihood, PAC-Bayes bounds and model compression. Using insights about generalization, my goal is to build more robust and reliable machine learning models. 

My PhD research has been recognized with an [ICML Outstanding Paper Award](https://icml.cc/virtual/2022/poster/17991) and is generously supported by the [Microsoft Research PhD Fellowship](https://nyudatascience.medium.com/cds-students-sanae-lotfi-and-lucius-bynum-receive-the-microsoft-research-phd-fellowship-63ce04660227), the [Google DeepMind Fellowship](https://nyudatascience.medium.com/deepmind-fellow-profile-sanae-lotfi-9197c0c5fb94), and the Meta AI Mentorship Program. I was recently distinguished as a [Rising Star in Machine Learning](https://ml.umd.edu/rising-stars-workshop) by the University of Maryland Center for Machine Learning.

Prior to NYU, I worked with [Andrea Lodi](https://tech.cornell.edu/people/andrea-lodi/) and [Dominique Orban](https://dpo.github.io/) at Polytechnique Montreal to design stochastic first and second-order algorithms with compelling theoretical and empirical properties for machine learning and large-scale optimization. I was awarded the [Best Master's Thesis Award](https://www.gerad.ca/en/posts/903) for this work. 

In 2022-2023, I was fortunate to work with [Brandon Amos](http://bamos.github.io/) as a Visiting Researcher in the Fundamental AI Research (FAIR) group at Meta AI. I was also fortunate to work with [Bernie Wang](http://web.mit.edu/~ywang02/www/) and [Richard Kurle](https://scholar.google.fr/citations?user=q2YBN34AAAAJ&hl=en) at Amazon as an Applied Scientist Intern in summer 2022.

**You can contact me at sl8160[at]nyu[dot]edu**

Recent News 
======

- July 2024: I will be a keynote speaker at the [Machine Learning and Compression Workshop](https://neuralcompression.github.io/workshop24) @ NeurIPS 2024. 

- July 2024: I'm co-organizing a workshop on _Scientific Methods for Understanding Neural Networks_ @ NeurIPS 2024. More details coming soon! 

- July 2024: I passed my thesis proposal. **I will graduate in Spring 2025 and will start looking for postdoc and research scientist positions in Fall 2024.** Feel free to reach out if you see a good fit! 

- June 2024: _[Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models](https://openreview.net/forum?id=cQWsTeTSkZ)_ got selected for an oral presentation at the ICML Workshop on Theoretical Foundations of Foundation Models. 

- June 2024: I gave a talk on _Non-Vacuous Generalization Bounds for Large Language Models_ at ML Collective. 

- June 2024: I started my summer internship at Microsoft Research NYC, where I will be working on large language model merging for multi-task learning with [Miro Dudik](https://www.microsoft.com/en-us/research/people/mdudik/) and [Jordan Ash](https://www.jordantash.com/). 

- May 2024: _[Non-Vacuous Generalization Bounds for Large Language Models](https://arxiv.org/abs/2312.17173)_ got accepted to ICML! 

- May 2024: I gave a talk on _Non-Vacuous Generalization Bounds for Large Language Models_ at Cohere for AI and UIUC ML Reading group.

  
