---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am Yusheng Su, a Ph.D. student in [THUNLP Lab](https://twitter.com/tsinghuanlp), the [Department of Computer Science and Technology](http://www.cs.tsinghua.edu.cn/) at [Tsinghua University](https://www.tsinghua.edu.cn/publish/thu2018en/index.html). I am advised by [Prof. Zhiyuan Liu](https://scholar.google.com/citations?user=dT0v5u0AAAAJ&hl=zh-TW). 

<!--My research interests are in the theory and practice of building machine learning systems that remain reliable when deployed in real application contexts. For example:-->
My research interests are exploring the mechanism (theory) and operation (practice) of large-scale pre-trained models (LLMs) to make them effective and efficient when deployed in real applications. For example:

<b>Parameter-Efficient Tuning.</b> Adapting LLMs to downstream tasks incurs a huge computational burden. How can we adapt LLMs to downstream tasks efficiently (with less computational burden)? Driving LLMs with minimal parameters (parameter-efficient tuning) is worth exploring.

<b>Human-Like Cognitive Abilities.</b> Adapting LLMs to downstream tasks requires task-specific manually annotated data, which needs manual effort involving labeling. How can we acquire task-specific knowledge effectively (without manual effort)? Equipping PLMs with human-like cognitive abilities to leverage open-world information is a promising direction.

<b>AI4Science.</b> How many human-like capabilities LLMs learn in the pre-training stage remains unknown. To further understand, we develop tools to explore and conduct a series of analyses.



<!--My research interests lie within the intersection of Natural Language Processing and Machine Learning, particularly in pre-trained language models (PLMs). At the current stage, I am particularly studying how to effectively and efficiently adapt PLMs to complex downstream tasks. To achieve these goals, I specifically focus on knowledge-guided methods, transfer learning, in-context learning, and parameter-efficient tuning.-->

<!--You can find my CV [here](/cv/).-->

## On The Job Market
I'm on the job market, looking for Post-doc and industrial research positions, starting in 2023 Autumn.
<!-- (in North America) -->

<!-- ## Coming Soon
Will share our work: [On Transferability of Prompt Tuning for Natural Language Processing](https://arxiv.org/abs/2111.06719)<br>
<b>Conference</b>: NAACL 2022, Seattle, Washington <br>
<b>Session</b>: Efficient Methods in NLP <br>
<b>Time</b>: July 10, 2022, 10:45 â€“ 12:15 (UTC - 7:00) <br>
</font> -->


## News
<!-- 
* [Mar. 2022] Got one paper accepted at ACL 2022. 
* [Aug. 2021] Got one paper accepted at IEEE/TASLP 2021. 
* [Feb. 2021] Got one paper accepted at WWW 2021. 
* [Aug. 2020] Got one paper accepted at EMNLP 2020. 
</font> -->
* [Sep. 2022] Invited Talk (on-line) @ [SAP - AI Research](https://www.sap.com/products/artificial-intelligence.html) Headquarter, Germany
* [Jul. 2022] I will attend [NAACL 2022](https://2022.naacl.org/) in person to orally present our works and look forward to meeting new/old friends in Seattle.
* [Apr. 2022] [Transferability of Prompt Tuning](https://aclanthology.org/2022.naacl-main.290/) and [Knowledge Inheritance](https://aclanthology.org/2022.naacl-main.288/) are accepted to [NAACL 2022](https://2022.naacl.org/).

<!-- <font color="gray"> </font> -->




## Publications

* <b>On Transferability of Prompt Tuning for Natural Language Processing</b>\
*<b>Yusheng Su</b>, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Zhiyuan Liu, Peng Li, Juanzi Li, Lei Hou, Maosong Sun, Jie Zhou*\
NAACL 2022 <b>Oral</b>. [[pdf]](https://aclanthology.org/2022.naacl-main.290/) [[code]](https://github.com/thunlp/Prompt-Transferability) [[BibTex]](https://aclanthology.org/2022.naacl-main.290.bib) [[slide]](https://drive.google.com/file/d/1OSmU3s7DOv-Gux5JcjuY0w79MzRRJuiA/view?usp=sharing) [[video]](https://www.youtube.com/watch?v=KVgmtgMQ3ig)


* <b>Knowledge Inheritance for Pre-trained Language Models</b>\
*Yujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han, Zhengyan Zhang, <b>Yusheng Su</b>, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou*\
NAACL 2022 <b>Oral</b>. [[pdf]](https://aclanthology.org/2022.naacl-main.288/) [[code]](https://github.com/thunlp/Knowledge-Inheritance)

* <b>Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning</b>\
*Yujia Qin, Xiaozhi Wang, <b>Yusheng Su</b>, Yankai Lin, Ning Ding, Zhiyuan Liu, Juanzi Li, Lei Hou, Peng Li, Maosong Sun, Jie Zhou*\
ACL 2022 Findings. [[pdf]](https://arxiv.org/abs/2110.07867) [[code]](https://github.com/thunlp/Intrinsic-Prompt-Tuning)

* <b>CPM: A large-scale Generative Chinese Pre-trained Language Model</b>\
*Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, <b>Yusheng Su</b>, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun*\
AI OPEN 2021. [[pdf]](https://www.sciencedirect.com/science/article/pii/S266665102100019X) [[code]](https://github.com/TsinghuaAI/CPM)

* <b>CSS-LM: A Contrastive Framework for Semi-supervised Fine-tuning of Pre-trained Language Models</b>\
*<b>Yusheng Su</b>, Xu Han, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Peng Li, Maosong Sun*\
[WWW 2021 Workshop](https://www.aminer.cn/ssl_www2021), IEEE/TASLP 2021. [[pdf]](https://arxiv.org/abs/2102.03752) [[code]](https://github.com/thunlp/CSS-LM) [[slide]](https://drive.google.com/file/d/1TqAnFYL5CBwWhrDamB83akqQniVd2B9o/view?usp=sharing)

* <b>CokeBERT: Contextual Knowledge Selection and Embedding Towards Enhanced Pre-Trained Language Models</b>\
*<b>Yusheng Su</b>, Xu Han, Zhengyan Zhang, Peng Li, Zhiyuan Liu, Yankai Lin, Jie Zhou, Maosong Sun*\
EMNLP 2020 Findings, AI OPEN 2021. [[pdf]](https://arxiv.org/abs/2009.13964) [[pdf]](https://www.sciencedirect.com/science/article/pii/S2666651021000188) [[code]](https://github.com/thunlp/CokeBERT)




## Under Review or Preprint Version <!-- Submitted for Publications-->

* <b>Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models</b>\
*Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, <b>Yusheng Su</b>, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, Maosong Sun.*\
ArXiv. [[pdf]](https://arxiv.org/abs/2203.06904)

<!--
## Teaching
* [Jul. 2022] Tsinghu University - NLP Big Model Course (Summer) [link](https://www.bilibili.com/video/BV1UG411p7zv?p=1&unique_k=OwC3PgP)
-->

## Talks
* [Sep. 2022] Invited Talk (on-line) @ [SAP - AI Research](https://www.sap.com/products/artificial-intelligence.html) Headquarter, Germany
* [Jul. 2022] Oral Talk (in persion) @ NAACL 2022, [[slide]](https://drive.google.com/file/d/1OSmU3s7DOv-Gux5JcjuY0w79MzRRJuiA/view?usp=sharing) [[video]](https://www.youtube.com/watch?v=KVgmtgMQ3ig)
* [Apr. 2021] Spotlight Talks (on-line) @ WWW 2021 (Self-Supervised Learning Workshop)



## Professional Services
* Reviewer: COLING 2022
* Review Committee Member: EMNLP 2022
* Reviewer: ICML 2022
* Reviewer: ACL Rolling 2022
* Reviewer: ACL Rolling 2021
* Reviewer: EMNLP/ACL/IEEE-TASLP 2021




## Internships
<!--
### Tsinghua NLP Lab. (Beijing) 09.2019 - 07.2023
* Ph.D. Student 
* Advised by [Prof. Zhiyuan Liu](http://nlp.csai.tsinghua.edu.cn/~lzy/).
</font> -->

### MediaTek. (Taiwan) 07.2018 - 08.2019
* Deep/Machine Learning Engineer Intern
* Advised by Jing-Han Wang.

### Microsoft. (Taiwan) 07.2015 - 07.2016
* Research and Development Intern
* Advised by [Kuang-Chao Yeh](https://www.linkedin.com/in/kuang-chao-yeh/) and [Gordon Chang](https://www.linkedin.com/in/gordonwinnow).
