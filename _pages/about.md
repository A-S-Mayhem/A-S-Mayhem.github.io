---
permalink: /
title: "Bio"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<p style="text-align:justify; text-justify:inter-ideograph;">
Shaolei Zhang is currently working toward the Ph.D. degree in Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS). He received the B.S. degree from Beijing University of Posts and Telecommunications in 2020. His research interests include nature language processing, machine translation and simultaneous translation. He has published 11 papers on the international conferences, and won the first place in the streaming transcription track of AutoSimTrans 2021.
</p>


News
======

Publications
======
- **Shaolei Zhang**, Yang Feng. Information-Transport-based Policy for Simultaneous Translation. *EMNLP 2022*. [[PDF](https://arxiv.org/pdf/2210.12357.pdf)] [[Code](https://github.com/ictnlp/ITST)]
- **Shaolei Zhang**, Shoutao Guo, Yang Feng. Wait-info Policy: Balancing Source and Target at Information Level for Simultaneous Machine Translation. *EMNLP 2022 findings*. [[PDF](https://arxiv.org/pdf/2210.11220.pdf)] [[Code](https://github.com/ictnlp/Wait-info)]
- Shoutao Guo, **Shaolei Zhang**, Yang Feng. Turning Fixed to Adaptive: Integrating Post-Evaluation into Simultaneous Machine Translation. *EMNLP 2022 findings*. [[PDF](https://arxiv.org/pdf/2210.11900.pdf)] [[Code](https://github.com/ictnlp/PED-SiMT)]
- **Shaolei Zhang**, Yang Feng. Modeling Dual Read/Write Paths for Simultaneous Machine Translation. *ACL 2022*. [[PDF](https://aclanthology.org/2022.acl-long.176.pdf)] [[Code](https://github.com/ictnlp/Dual-Path)]
- **Shaolei Zhang**, Yang Feng. Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework. *ACL 2022*. [[PDF](https://aclanthology.org/2022.acl-long.467.pdf)]
- **Shaolei Zhang**, Yang Feng. Gaussian Multi-head Attention for Simultaneous Machine Translation. *ACL 2022 findings*. [[PDF](https://aclanthology.org/2022.findings-acl.238.pdf)] [[Code](https://github.com/ictnlp/GMA)]
- **Shaolei Zhang**, Yang Feng. Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy. *EMNLP 2021*. [[PDF](https://aclanthology.org/2021.emnlp-main.581.pdf)] [[Code](https://github.com/ictnlp/MoE-Waitk)]
- **Shaolei Zhang**, Yang Feng. Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model. *EMNLP 2021 findings*. [[PDF](https://aclanthology.org/2021.findings-emnlp.121.pdf)] 
- **Shaolei Zhang**, Yang Feng. ICTâ€™s System for AutoSimTrans 2021: Robust Char-Level Simultaneous Translation. *AutoSimTrans@NAACL 2021*. [[PDF](https://aclanthology.org/2021.autosimtrans-1.1.pdf)]
- **Shaolei Zhang**, Yang Feng, Liangyou Li. Future-Guided Incremental Transformer for Simultaneous Translation. *AAAI 2021*. [[PDF](https://arxiv.org/pdf/2012.12465.pdf)]




