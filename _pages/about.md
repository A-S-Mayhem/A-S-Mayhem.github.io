---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a PhD candidate at NYU advised by [Andrew Gordon Wilson](https://cims.nyu.edu/~andrewgw/). I work on the foundations of deep learning and focus on understanding the generalization properties of deep neural networks using notions that relate to generalization such as model compression, the marginal likelihood, and PAC-Bayes bounds. Using insights about generalization, my goal is to build more robust and reliable machine learning models. 

My PhD research has been recognized with an [ICML Outstanding Paper Award](https://icml.cc/virtual/2022/poster/17991) and is generously supported by the [Microsoft Research PhD Fellowship](https://nyudatascience.medium.com/cds-students-sanae-lotfi-and-lucius-bynum-receive-the-microsoft-research-phd-fellowship-63ce04660227), the [Google DeepMind Fellowship](https://nyudatascience.medium.com/deepmind-fellow-profile-sanae-lotfi-9197c0c5fb94), and the Meta AI Mentorship Program. I was recently distinguished as a [Rising Star in Machine Learning](https://ml.umd.edu/rising-stars-workshop) by the University of Maryland Center for Machine Learning.

Prior to NYU, I worked with [Andrea Lodi](https://tech.cornell.edu/people/andrea-lodi/) and [Dominique Orban](https://dpo.github.io/) at Polytechnique Montreal to design stochastic first and second-order algorithms with compelling theoretical and empirical properties for machine learning and large-scale optimization. I was awarded the [Best Master's Thesis Award](https://www.gerad.ca/en/posts/903) for this work. 

**You can contact me at sl8160[at]nyu[dot]edu**

## Work Experience

- I am currently interning at **Microsoft Research**, where I work with [Miro Dudik](https://www.microsoft.com/en-us/research/people/mdudik/) and [Jordan Ash](https://www.jordantash.com/) to build novel methods for efficient large language model merging for mutli-task learning. 
  
- In 2022-2023, I was a Visiting Researcher in the **Fundamental AI Research (FAIR)** group at **Meta**, where I worked with [Brandon Amos](http://bamos.github.io/) to derive generalization bounds for large language models and understand the benefits of input-dependent augmentations in image classification.
  
- In summer 2022, I worked with [Bernie Wang](http://web.mit.edu/~ywang02/www/) and [Richard Kurle](https://scholar.google.fr/citations?user=q2YBN34AAAAJ&hl=en) at **Amazon** to understand and quantify distribution shift in time series.  


## Recent News 

- July 2024: I will be a keynote speaker at the [Machine Learning and Compression Workshop](https://neuralcompression.github.io/workshop24) @ NeurIPS 2024. 

- July 2024: I'm co-organizing a workshop on _Scientific Methods for Understanding Neural Networks_ @ NeurIPS 2024. More details coming soon! 

- July 2024: I passed my thesis proposal. **I will graduate in Spring 2025 and will start looking for postdoc and research scientist positions in Fall 2024.** Feel free to reach out if you see a good fit! 

- June 2024: _[Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models](https://openreview.net/forum?id=cQWsTeTSkZ)_ got selected for an oral presentation at the ICML Workshop on Theoretical Foundations of Foundation Models. 

- June 2024: I gave a talk on _Non-Vacuous Generalization Bounds for Large Language Models_ at ML Collective. 

- June 2024: I started my summer internship at Microsoft Research NYC, where I will be working on large language model merging for multi-task learning with [Miro Dudik](https://www.microsoft.com/en-us/research/people/mdudik/) and [Jordan Ash](https://www.jordantash.com/). 

- May 2024: _[Non-Vacuous Generalization Bounds for Large Language Models](https://arxiv.org/abs/2312.17173)_ got accepted to ICML! 

- May 2024: I gave a talk on _Non-Vacuous Generalization Bounds for Large Language Models_ at Cohere for AI and UIUC ML Reading group.

  
