---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---

{% if author.googlescholar %}
  You can also find my articles on <u><a href="{{author.googlescholar}}">my Google Scholar profile</a>.</u>
{% endif %}

{% include base_path %}

<!--THESIS-->
<div>
  <h2>Learning Temporally-Consistent Representations for Data-Efficient Reinforcement Learning</h2>
</div>
<!--<div>-->
  <!--<img src="{{ base_path }}/images/cheetah-run.gif" border="100" style="margin: 1em 1em 1em 0em; width: 10%; height: auto;"/>-->
  <!--<p>Development of $k$-Step Latent, a novel representation learning routine for reinfrocement learning that provides state-of-the-art results in the populat PlaNet benchmark suite.</p>-->
<!--</div>-->
<!--<div style="width:100%; height:auto;">-->
  <!--<div>-->
    <!--<img src="{{ base_path }}/images/cheetah-run.gif" border="100" style="margin: 0em 0em 0em 0em; width: 18%; height: auto; float:left; vertical-align:middle;"/>-->
  <!--</div>-->
  <!--<div style="margin-left:190px;">-->
    <!--<p>-->
      <!--Development of $k$-Step Latent (KSL), a novel representation learning routine for reinforcement learning that provides state-of-the-art results in the populat PlaNet benchmark suite.-->
      <!--KSL's representation learning module consists of several sub-networks with momentum counterparts (similar to BYOL), as well as a recurrent operation that encourages temporally coherent representations <i>in the latent space</i>.-->
      <!--We also analyze the representations learned by various methods and determine a list of characteristics that make representations useful for the ultimate reinforcement learning goal.-->
    <!--</p>-->
  <!--</div>-->
<!--</div>-->
<div style="display:flex; align-items:center;">
  <img src="{{ base_path }}/images/cheetah-run.gif" border="100" style="width:18%;"/>
  <span style="margin-left:15px;">
    Development of $k$-Step Latent (KSL), a novel representation learning routine for reinforcement learning that provides state-of-the-art results in the populat PlaNet benchmark suite.
    KSL's representation learning module consists of several sub-networks with momentum counterparts (similar to BYOL), as well as a recurrent operation that encourages temporally coherent representations <i>in the latent space</i>.
    We also analyze the representations learned by various methods and determine a list of characteristics that make representations useful for the ultimate reinforcement learning goal.
  </span>
</div>

<div style="text-align:center;">
  <p>
    <!--<a href="https://arxiv.org/abs/2103.06398v1">[PAPER]</a>-->
    <a href="https://arxiv.org/abs/2110.04935">[PAPER]</a>
    &nbsp; &nbsp; &nbsp; &nbsp;
    <a href="https://github.com/uoe-agents/ksl">[CODE]</a>
  </p>
</div>

<br>

<!--ANALYZING HIDDEN ACTIVATIONS-->
<div>
  <h2>Analyzing the Hidden Activations of Deep Policy Networks: Why Representation Matters</h2>
</div>
<!--<div style="width:100%; height:auto; padding:1%;">-->
  <!--<div>-->
    <!--<img src="{{ base_path }}/images/analyzing_z.png" border="100" style="margin: 0em 0em 0em 0em; width: 18%; height: auto; float:left;"/>-->
  <!--</div>-->
  <!--<div style="margin-left:190px;">-->
    <!--<p>Analyzed the hidden activations of policy networks and revealed patterns that show how deep reinforcement learning agents learn to organize their internal representations.-->
      <!--Performed systematic study across agents that learn in high-dimensional space and agents that learn on latent representations from auxiliary models.-->
      <!--We revealed that the training process for reinforcement learning agents is greatly sped up when internal representations are organized around reward, which can be achieved <i>a-priori</i>.</p>-->
  <!--</div>-->
<!--</div>-->
<div style="display:flex; align-items:center;">
  <img src="{{ base_path }}/images/analyzing_z.png" border="100" style="width:18%;"/>
  <span style="margin-left:15px;">
    Analyzed the hidden activations of policy networks and revealed patterns that show how deep reinforcement learning agents learn to organize their internal representations.
    Performed systematic study across agents that learn in high-dimensional space and agents that learn on latent representations from auxiliary models.
    We revealed that the training process for reinforcement learning agents is greatly sped up when internal representations are organized around reward, which can be achieved <i>a-priori</i>.
  </span>
</div>

<div style="text-align:center;">
  <p>
    <a href="https://arxiv.org/abs/2103.06398v1">[PAPER]</a>
    &nbsp; &nbsp; &nbsp; &nbsp;
    <a href="https://github.com/trevormcinroe/rl_with_latents">[CODE]</a>
  </p>
</div>

<br>

<!--REMOVING RAIN-->
<div>
  <h2>Removing Rain from Images with Densely-Connected Convolutional Networks</h2>
</div>
<!--<div style="width:100%; height:auto; padding:1%;">-->
  <!--<div>-->
    <!--<img src="{{ base_path }}/images/derain_drn.png" border="100" style="margin: 0em 0em 0em -1.5em; width: 20%; height:auto; float:left;"/>-->
  <!--</div>-->
  <!--<div style="margin-left:190px;">-->
    <!--<p>Designed and implemented a convolutional encoder-decoder with novel "block" structure that is capable of removing rain from images.-->
      <!--Rain, and other weather-based impediments, may harm the performance of on-vehicle computer vision models, which may result in unsafe decisions being made by autonomous systems.-->
      <!--Our process was proven to improve the performance of downstream models across a multitude of tasks. Details on model omitted to protect IP.</p>-->
  <!--</div>-->
<!--</div>-->
<div style="display:flex; align-items:center;">
  <img src="{{ base_path }}/images/derain_drn.png" border="100" style="width:20%;"/>
  <span style="margin-left:15px;">
    Designed and implemented a convolutional encoder-decoder with novel "block" structure that is capable of removing rain from images.
    Rain, and other weather-based impediments, may harm the performance of on-vehicle computer vision models, which may result in unsafe decisions being made by autonomous systems.
    Our process was proven to improve the performance of downstream models across a multitude of tasks. Details on model omitted to protect IP.
  </span>
</div>

<div style="text-align:center;">
  <p>
    <a href="https://toyota-connected-assets.s3.us-east-2.amazonaws.com/pdf/mcinroe_deraining_case_study_final.pdf">[PAPER]</a>
    &nbsp; &nbsp; &nbsp; &nbsp;
    [CODE NOT PUBLIC]
  </p>
</div>

<!--IRL-->
<div>
  <h2>Creating Web Ads with Inverse Reinforcement Learning</h2>
</div>
<!--<div style="width:100%; height:auto; padding:1%;">-->
  <!--<div>-->
    <!--<img src="{{ base_path }}/images/derain_drn.png" border="100" style="margin: 0em 0em 0em -2em; width: 20%; height: auto; float:left;"/>-->
  <!--</div>-->
  <!--<div style="margin-left:190px;">-->
    <!--<p>Designed algorithm based on IRL to learn expert's policy on shot-by-shot video characteristics.-->
      <!--Once deployed, agent constructed various 30 second ads by drawing from a pool of one second clips.-->
      <!--Paper and code belong to funder, Keurig Dr Pepper.</p>-->
  <!--</div>-->
<!--</div>-->

<div style="display:flex; align-items:center;">
  <img src="{{ base_path }}/images/derain_drn.png" border="100" style="width:20%;"/>
  <span style="margin-left:15px;">
    Designed algorithm based on IRL to learn expert's policy on shot-by-shot video characteristics.
    Once deployed, agent constructed various 30 second ads by drawing from a pool of one second clips.
    Paper and code belong to funder, Keurig Dr Pepper.
  </span>
</div>

<div style="text-align:center;">
  <p>
    [PAPER NOT PUBLIC]
    &nbsp; &nbsp; &nbsp; &nbsp;
    [CODE NOT PUBLIC]
  </p>
</div>

