---
permalink: /
title: "Antreas Antoniou"
excerpt: "Home"
author_profile: true
redirect_from: 
  - /home/
  - /home.html
---

<meta name="google-site-verification" content="giQzN4aACkcRD3IY7dwaL7jyKfwAU3XSfmDbkKYn0pA" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-131324268-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-131324268-1');
</script>

I am a Machine Learning PhD student at the University of Edinburgh, supervised by Prof. Amos Storkey. I am a member of the [BayesWatch](https://www.bayeswatch.com/) research group and the Adaptive and Neural Computation (ANC) research institute. My broader research topic is 
meta-learning in deep neural networks. More specifically, I am interested in applying meta-learning to 
learn a variety of
internal network components (e.g. losses, optimizers, learning rates, architectures, memory modules, 
initializations etc.) such that a model can perform very well on a target task. 

Wait.. What is <em>meta-learning</em>?

Meta-learning or *learning to learn* can be broadly defined as a machine learning paradigm, where we learn the learning algorithms themselves. The premise of such meta-learned learning algorithms lies in the fact that they are learned over a number of hours/days,
 and will often generalize better than manually invented learning algorithms.
In essence, building systems that become more proficient at learning with more experience, thus learning *how* to learn.

                                                                                

