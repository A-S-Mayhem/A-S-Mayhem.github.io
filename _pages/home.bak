---
permalink: /
title: "Antreas Antoniou"
excerpt: "Home"
author_profile: true
redirect_from: 
  - /home/
  - /home.html
---

I am a Machine Learning PhD student at the University of Edinburgh. My broader research topic is 
meta-learning in deep neural networks. More specifically, I am interested in applying meta-learning to 
learn a variety of
internal network components (e.g. losses, optimizers, learning rates, architectures, memory modules, 
initializations etc.) such that a model can perform very well on a target task. Meta-learning or *learning to learn*
can be broadly defined as a machine learning paradigm, where models are trained to become more proficient at learning
with more experience, thus learning *how* to learn.


Achieving meta-learning usually involves abstracting learning into multiple levels. 
The inner-most levels usually involve acquiring *task-specific* knowledge (i.e. knowledge learned from a particular 
task in order to solve that task) whereas the outermost level involves acquiring *across-task* knowledge (i.e. knowledge
extracted across tasks that enables the system to become better at learning from individual tasks and generalzing better
on some target metric)


