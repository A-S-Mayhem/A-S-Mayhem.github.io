---
layout: default
title: "Thesis"
permalink: /thesis/
author_profile: true
---

{% include base_path %}

<!--MathJax-->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<div style="text-align:center;">
    <h1>Representation Learning for Data-Efficient Reinforcement Learning</h1>
</div>

<div style="text-align:center;">
    <img src="{{ base_path }}/images/cheetah-run.gif" border="100" style="margin: 1em 1em 1em 1em; width: 15%; height: auto;"/>
<img src="{{ base_path }}/images/walker-walk.gif" border="100" style="margin: 1em 1em 1em 1em; width: 15%; height: auto;"/>
</div>

<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h2>Abstract</h2>
</div>

<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        Deep reinforcement learning (RL) agents that exist in high-dimensional state spaces, such as those composed of images, have two learning burdens: (a) they must learn an action-selection policy that completes their given task and (b) they must learn to discern between useful and useless information in the state space. The reward function is the only supervised feedback that RL agents receive, which causes a representation learning bottleneck on (b) that ultimately manifests in poor sample efficiency. In this thesis, we explore representation learning methods for RL in the context of the popular PlaNet benchmark suite, a set of six continuous control tasks with image-based states. We introduce $k$-Step Latent (KSL), a new representation learning method that provides state of the art (SotA) results in this suite. KSL accomplishes these results by exploiting the environment's underlying Markov decision process via a learned recurrent environment transition model in the latent space. Finally, we spend time analyzing the representations that are learned by KSL and other previous SotA methods. Through this analysis, we uncover a set of desirable characteristics of latent representations as they relate to the RL goal. Using these characteristics, it may be possible to design new representation learning methods in the future that open the door for feasible real-world RL applications.
    </p>
</div>

<div style="text-align:center;">
  <p>
    <!--<a href="https://arxiv.org/abs/2103.06398v1">[PAPER]</a>-->
    [PAPER IN REVIEW]
    &nbsp; &nbsp; &nbsp; &nbsp;
    <a href="https://github.com/trevormcinroe/thesis">[CODE]</a>
  </p>
</div>

<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h2>Method</h2>
</div>

<div style="text-align:center;">
    <img src="{{ base_path }}/images/ksl.png" border="100" style="margin: 1em 1em 1em 1em; width: 60%; height: auto;"/>
</div>

<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        KSL's representation learning modules build off of the recent success of <a href="https://proceedings.neurips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf">"Boostrap Your Own Latent"</a> (BYOL).
        We expand BYOL's architecture with a learned recurrent transition-module that operates in the latent space.
        The addition of this module allows KSL to exploit the RL problem's underlying Markov decision process and encourages KSL's representations to be temporally coherent.
    </p>
</div>

<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h2>Results</h2>
</div>

<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h3>PlaNet Benchmark Suite</h3>
</div>
<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        The results in the table below are from the PlaNet benchmark suite. One key note is that KSL achieves SotA results with small batch sizes. The results reported for KSL are with a batch size of 128 while previous SotA methods (RAD, DrQ, CURL, and SAC+AE) all use a batch size of 512. These methods suffer significantly with smaller batch sizes, as shown in the following section.
    </p>
</div>


<div style="text-align:center;">
    <img src="{{ base_path }}/images/results-table.png" border="100" style="margin: 1em 1em 1em 1em; width: 70%; height: auto;"/>
</div>

<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h3>Effects of Smaller Batch Sizes</h3>
</div>
<div style="text-align:center;">
    <img src="{{ base_path }}/images/cheetah_small_batch.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/walker_small_batch.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
</div>

<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h2>Analyzing Latent Representations</h2>
</div>
<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        Here, we provide a high-level overview of our analysis on the latent representations that are produced by various representation learning methods. We focus on KSL and DrQ and, in some cases, also provide results from an RL agent learning without any representation learning routines, which we call Pixel.
    </p>
</div>

<div style="display:inline-block; max-width:40%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h3>Translation Invariance and Temporal Coherence</h3>
</div>
<div style="text-align:center;">
    <img src="{{ base_path }}/images/trans_invar_in.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/dist_z_zprime_in.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
</div>

<div style="display:inline-block; max-width:50%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h3>Representations as They Relate to Reward</h3>
</div>
<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        We analyzed the organization of learned representations for KSL (top row) and DrQ (bottom row) at 5k (left column), 10k (middle column) and 15k (right column) steps.
        Plots are made via PCA projections of latent representations created by each methods' encoders and points are colored by the given reward in the original state.
        We highlight that KSL learns to organize its representations around reward more quickly than DrQ.
    </p>
</div>
<div style="text-align:center;">
    <img src="{{ base_path }}/images/pca_ksl_5k_single.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/pca_ksl_10k_single.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/pca_ksl_15k_single.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
</div>

<div style="text-align:center;">
    <img src="{{ base_path }}/images/pca_drq_5k_single.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/pca_drq_10k_single.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/pca_drq_15k_single.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
</div>

<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        We quantitatively expanded the above analysis by computing how <i>linearly predictable</i> reward is from given representations.
        Via fitting a linear regression, we computed the mean squared error for a set of given representations that are produced by encoders every 5k learning steps from 0k to 100k.
    </p>
    <p>
        The right plot is simply a zoomed-in version of the left plot that provides are more clear depiction of the differences between KSL and DrQ. We highlight two findings. First, the RL learning loop is not enough to learn representations that can reliably predict reward. Second, KSL's representation learning routine provides representations that are much more closely related to reward than DrQ's representations.
    </p>
</div>
<div style="text-align:center;">
    <img src="{{ base_path }}/images/mse_linear_out.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/mse_linear_in.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
</div>

<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h2>Ablations</h2>
</div>
<div style="text-align:center;">
    <img src="{{ base_path }}/images/cheetah_k1.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/cheetah_k3.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/cheetah_k5.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
</div>