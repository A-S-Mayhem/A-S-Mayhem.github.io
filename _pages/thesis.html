---
layout: default
title: "Thesis"
permalink: /thesis/
author_profile: true
---

{% include base_path %}

<!--MathJax-->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<div style="text-align:center;">
    <h1>Representation Learning for Data-Efficient Reinforcement Learning</h1>
</div>

<div style="text-align:center;">
    <img src="{{ base_path }}/images/cheetah-run.gif" border="100" style="margin: 1em 1em 1em 1em; width: 15%; height: auto;"/>
<img src="{{ base_path }}/images/walker-walk.gif" border="100" style="margin: 1em 1em 1em 1em; width: 15%; height: auto;"/>
</div>

<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h2>Abstract</h2>
</div>

<div style="display:inline-block; max-width:50%; display:block; margin-left:auto; margin-right:auto">
    <h4>
        Deep reinforcement learning (RL) agents that exist in high-dimensional state spaces, such as those composed of images, have two learning burdens: (a) they must learn an action-selection policy that completes their given task and (b) they must learn to discern between useful and useless information in the state space. The reward function is the only supervised feedback that RL agents receive, which causes a representation learning bottleneck on (b) that ultimately manifests in poor sample efficiency. In this thesis, we explore representation learning methods for RL in the context of the popular PlaNet benchmark suite, a set of six continuous control tasks with image-based states. We introduce $k$-Step Latent (KSL), a new representation learning method that provides state of the art (SotA) results in this suite. KSL accomplishes these results by exploiting the environment's underlying Markov decision process via a learned recurrent environment transition model in the latent space. Finally, we spend time analyzing the representations that are learned by KSL and other previous SotA methods. Through this analysis, we uncover a set of desirable characteristics of latent representations as they relate to the RL goal. Using these characteristics, it may be possible to design new representation learning methods in the future that open the door for feasible real-world RL applications
    </h4>
</div>

<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h2>Method</h2>
</div>

<div style="text-align:center;">
    <img src="{{ base_path }}/images/ksl.png" border="100" style="margin: 1em 1em 1em 1em; width: 60%; height: auto;"/>
</div>


<div style="text-align:center;">
    <img src="{{ base_path }}/images/cheetah_small_batch.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/walker_small_batch.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
</div>

<div style="text-align:center;">
    <img src="{{ base_path }}/images/trans_invar_in.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/dist_z_zprime_in.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
</div>