---
layout: default
title: "Thesis"
permalink: /thesis/
author_profile: true
---

{% include base_path %}

<!--MathJax-->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<div style="text-align:center;">
    <h1>Representation Learning for Data-Efficient Reinforcement Learning</h1>
</div>

<div style="text-align:center;">
    <img src="{{ base_path }}/images/cheetah-run.gif" border="100" style="margin: 1em 1em 1em 1em; width: 15%; height: auto;"/>
<img src="{{ base_path }}/images/walker-walk.gif" border="100" style="margin: 1em 1em 1em 1em; width: 15%; height: auto;"/>
</div>

<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h2>Abstract</h2>
</div>

<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        Deep reinforcement learning (RL) agents that exist in high-dimensional state spaces, such as those composed of images, have two learning burdens: (a) they must learn an action-selection policy that completes their given task and (b) they must learn to discern between useful and useless information in the state space. The reward function is the only supervised feedback that RL agents receive, which causes a representation learning bottleneck on (b) that ultimately manifests in poor sample efficiency. In this thesis, we explore representation learning methods for RL in the context of the popular PlaNet benchmark suite, a set of six continuous control tasks with image-based states. We introduce $k$-Step Latent (KSL), a new representation learning method that provides state of the art (SotA) results in this suite. KSL accomplishes these results by exploiting the environment's underlying Markov decision process via a learned recurrent environment transition model in the latent space. Finally, we spend time analyzing the representations that are learned by KSL and other previous SotA methods. Through this analysis, we uncover a set of desirable characteristics of latent representations as they relate to the RL goal. Using these characteristics, it may be possible to design new representation learning methods in the future that open the door for feasible real-world RL applications.
    </p>
</div>

<div style="text-align:center;">
  <p>
    <!--<a href="https://arxiv.org/abs/2103.06398v1">[PAPER]</a>-->
    [THESIS IN REVIEW]
    &nbsp; &nbsp; &nbsp; &nbsp;
    <a href="https://github.com/trevormcinroe/thesis">[CODE]</a>
  </p>
</div>

<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h2>Method</h2>
</div>

<div style="text-align:center;">
    <img src="{{ base_path }}/images/ksl.png" border="100" style="margin: 1em 1em 1em 1em; width: 60%; height: auto;"/>
</div>

<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        KSL's representation learning modules build off of the recent success of <a href="https://proceedings.neurips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf">"Boostrap Your Own Latent"</a> (BYOL).
        We expand BYOL's architecture with a recurrent transition-module that operates in the latent space.
        The addition of this module allows KSL to exploit the RL problem's underlying Markov decision process and encourages KSL's representations to be temporally coherent.
        The learned encoders within KSL are used to project incoming image-based states into latent vectors, which are then used to train the RL agent.
        For specific details, see Chapter 3 of the linked thesis.
    </p>
</div>

<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h2>Results</h2>
</div>

<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h3>PlaNet Benchmark Suite</h3>
</div>
<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        The results in the table below are from the <a href="https://arxiv.org/pdf/1811.04551.pdf">PlaNet</a> benchmark suite, a set of six continuous control tasks.
        We provide figures for the "data efficiency" (100k steps) and "asymptotic performace" (500k steps) checkpoints.
        Results are reported as the mean $\pm$ one standard deviation across several random seeds.
        Best results for each task and checkpoint are bolded.
    </p>
</div>


<div style="text-align:center;">
    <img src="{{ base_path }}/images/results-table.png" border="100" style="margin: 1em 1em 1em 1em; width: 70%; height: auto;"/>
</div>

<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        One key note is that KSL achieves SotA results with small batch sizes.
        The results reported for KSL are with a batch size of 128 while previous SotA methods
        (<a href="https://proceedings.neurips.cc/paper/2020/file/e615c82aba461681ade82da2da38004a-Paper.pdf">RAD</a>,
        <a href="https://openreview.net/pdf?id=GY6-6sTvGaf">DrQ</a>,
        <a href="http://proceedings.mlr.press/v119/laskin20a.html">CURL</a>,
        and
        <a href="https://arxiv.org/pdf/1910.01741.pdf">SAC+AE</a>) all use a batch size of 512.
        These methods suffer significantly with smaller batch sizes, as shown in the below plots.
    </p>
</div>

<!--<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">-->
    <!--<h3>Effects of Smaller Batch Sizes</h3>-->
<!--</div>-->
<div style="text-align:center;">
    <img src="{{ base_path }}/images/cheetah_small_batch.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/walker_small_batch.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
</div>

<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h2>Analyzing Latent Representations</h2>
</div>
<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        Here, we provide a high-level overview of our analysis on the latent representations that are produced by various representation learning methods.
        We focus on KSL and DrQ and, in some cases, also provide results from an RL agent learning without any representation learning routines, which we call Pixel.
        For a detailed description of each evaluation test, see Chapter 5 in the linked thesis.
    </p>
</div>

<div style="display:inline-block; max-width:40%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h3>Translation Invariance and Temporal Coherence</h3>
</div>

<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        Ideally, representations in RL should be robust to small perturbations in the original-data space as well as be temporally coherent.
        We measure robustness via translation invariance by measuring the Euclidean distance between representation vectors from several augmented versions of a given state.
        For each 5k RL-learning steps from 0k to 100k, we compute this measurement over 5,000 states.
        The below plot (left) shows the average (bold line) and one standard deviation (shaded area) for representations learned by KSL, DrQ, and Pixel.
        Here, lower is better.
        Also, we measure the Euclidean distance between a given state and next-state pair over ten episodes.
        The below plot (right) shows the average (bold line) and one standard deviation (shaded area) for representations learned by KSL, DrQ, and Pixel.
        Here, lower is better.
    </p>
</div>

<div style="text-align:center;">
    <img src="{{ base_path }}/images/trans_invar_in.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/dist_z_zprime_in.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
</div>

<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        We highlight that the RL loop, without any representation learning help, is not enough to produce satisfactory results for either of the above two metrics.
        Also, KSL's learning routine produces superior results over DrQ.
    </p>
</div>

<div style="display:inline-block; max-width:50%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h3>Representations as They Relate to Reward</h3>
</div>
<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        We analyzed the organization of learned representations for KSL (top row) and DrQ (bottom row) at 5k (left column), 10k (middle column) and 15k (right column) steps.
        Plots are made via PCA projections of latent representations created by each methods' encoders and points are colored by the given reward in the original state.
        We highlight that KSL learns to organize its representations around reward more quickly than DrQ.
    </p>
</div>
<div style="text-align:center;">
    <img src="{{ base_path }}/images/pca_ksl_5k_single.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/pca_ksl_10k_single.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/pca_ksl_15k_single.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
</div>

<div style="text-align:center;">
    <img src="{{ base_path }}/images/pca_drq_5k_single.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/pca_drq_10k_single.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/pca_drq_15k_single.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
</div>

<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        We quantitatively expanded the above analysis by computing how <i>linearly predictable</i> reward is from given representations.
        Via fitting a linear regression, we computed the mean squared error for a set of given representations that are produced by encoders every 5k learning steps from 0k to 100k.
    </p>
    <p>
        The right plot is simply a zoomed-in version of the left plot that provides are more clear depiction of the differences between KSL and DrQ. We highlight two findings. First, the RL learning loop is not enough to learn representations that can reliably predict reward. Second, KSL's representation learning routine provides representations that are much more closely related to reward than DrQ's representations.
    </p>
</div>
<div style="text-align:center;">
    <img src="{{ base_path }}/images/mse_linear_out.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/mse_linear_in.png" border="100" style="margin: 1em 1em 1em 1em; width: 30%; height: auto;"/>
</div>

<div style="display:inline-block; max-width:30%; text-align:center;  display:block; margin-left:auto; margin-right:auto">
    <h2>Ablations</h2>
</div>
<div style="display:inline-block; max-width:55%; display:block; margin-left:auto; margin-right:auto">
    <p>
        We provide a grid sweep over architecture and implementation choices to aid practical use of KSL.
        Specifically, we sweep over various levels of $k \in \{1,3,5\}$ as well as various depths of knowledge sharing $l \in \{0,1,2\}$.
        For more detail on knowledge sharing and results, see Appendix A in the linked thesis.
    </p>
</div>

<div style="text-align:center;">
    <img src="{{ base_path }}/images/cheetah_k1.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/cheetah_k3.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
    <img src="{{ base_path }}/images/cheetah_k5.png" border="100" style="margin: 0.2em 0.2em 0.2em 0.2em; width: 30%; height: auto;"/>
</div>