---
title: "KART: Privacy Leakage Framework of Language Models Pre-trained with Clinical Records "
collection: publications
permalink: /publication/2020-kart.md
excerpt: 'For the safe sharing pre-trained language models, no guidelines exist at present owing to the difficulty in estimating the upper bound of the risk of privacy leakage. One problem is that previous studies have assessed the risk for different real-world privacy leakage scenarios and attack methods, which reduces the portability of the findings. To tackle this problem, we represent complex real-world privacy leakage scenarios under a universal parameterization, \textit{Knowledge, Anonymization, Resource, and Target} (KART).'
date: 2020-12-31
venue: 'arXiv'
paperurl: 'https://arxiv.org/abs/2101.00036'
citation: 'Yuta Nakamura, Shouhei Hanaoka, Yukihiro Nomura, Naoto Hayashi, Osamu Abe, Shuntaro Yada, Shoko Wakamiya, Eiji Aramaki. arXiv preprint arXiv:2101.00036'
---