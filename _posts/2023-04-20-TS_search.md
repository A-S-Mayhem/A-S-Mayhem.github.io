---
title: 'Transition State Searches on Neural Network Potential-Energy Surfaces'
date: 2023-04-17
permalink: /posts/2020/04/ts_nnp/
tags:
  - research
  - TS
  - GNNP
---

Background
======
Predicting reactive behavior is essential to catalysis, drug design, and the development of new materials. Reactive behavior of a system can be described through thermodynamic and kinetic rate parameters, however, theoretical calculation of these parameters requires knowledge of the transition state. 
Locating transition states is a computationally expensive task that frequently requires significant human intervention. Transition states are generally found through path finding algorithms that explore the potential energy surface between minima. Significant efforts have been made to develop more efficient path finding algorithms as evaluation of ab initio potentials is computationally expensive.
In this work we train a graph neural network on quantum chemical calculations of organic molecules and use the resulting model to estimate the transition state geometry via a nonlocal path-finding algorithm. Using a local surface walking algorithm with quantum chemical calculations we refine the guess to the exact transition state. 

Transition State Search Algorithms
======
Modern transition state search algorithms can be split into two steps, first, a nonlocal path-finding algorithm gives a guess of the transition state location on the potential energy surface. Second, a local surface walking algorithm refines the transition state guess to the exact transition state. The energy and gradient of the potential are evaluated at every location passed through by either algorithm, therefor in the number of gradient calculations is frequently uses as a metric for the efficiency of an algorithm.

In this work we use a new implementation of the freezing string method (FSM) as our nonlocal path-finding method and use QChems local surface walking algorithm to refine all guesses. The FSM takes a chain of states approach to locating transition states, which exist as first order saddle points connecting minima on the potential energy surface. The chain of states refers to a series of interpolated intermediate structures connecting the product and reactant, referred to as a string. In the FSM the string is constructed by interpolating a fixed distance in the direction of the opposing minima and creating a new structure. This structure is then optimized down the perpendicular component of the gradient n times, such that it lies approximately along the minimum energy path, and the process is repeated until the string is fully formed. The highest energy image in the string is taken as the transition state guess. In this work we use LST interpolation to generate realistic initial structures, and conjugate gradient optimization to efficiently move down the gradient. 
<center><img src='/images/fsm_gif.gif' width='800' height='700'></center>

Graph Neural Networks in Chemistry
======
Quantum chemical calculations predict the energies and forces of moderately sized chemical systems with high levels of accuracy, which has enabled detailed understanding of chemical processes at the atomic level. However, the high computational cost of these methods prohibits their use in simulations where many calculations are required. 
    Neural network potentials (NNPs) have emerged as an attractive alternative. Neural network potential energy surfaces are constructed by training neural networks on structure-energy data resulting from quantum chemical methods. Modern NNPs achieve chemical accuracy (error<2kcal/mol) while predicting the potential-energy of a system as a function of the atomic positions at a fraction of the computational cost.
	Graphs are mathematical structures consisting of nodes connected by edges and are frequently used to model pairwise relations between objects. Using graphs to represent molecules is intuitive, atoms are represented by nodes and bonds are represented by edges. Graph neural networks (GNNs) take graph structured data as the input and make a prediction about the entire graph, or individual nodes or edges. In this work we create molecular graphs, where each node contains the chemical identity and X,Y,Z coordinates of an atom, bonds are inferred from interatomic distances. Graph neural networks use a message passing scheme to construct a feature vector for each node that is representative of the electronic environment and predict the potential-energy of the molecule. 
	In this work we train SchNet, a GNN, on the ANI-1 dataset. The ANI-1 dataset consists of energies computed at the Ï‰B97X/6-31G* level of theory for minimum energy structures of small organic molecules and off-equilibrium conformations obtained by normal mode sampling. We use Bayesian optimization to search for optimal model parameters. When using an 80/20 training/validation split on the 23M conformers in the ANI-1 dataset we achieve a validation MSE of 1.42 with our best model. 

Results
======
We benchmark this methodology with a set of classic organic reactions, and a set of 50 organic elementary reactions. In the case of 50 organic reactions the GNNP based methods find the transition state in 44/50 reactions whereas ab initio based methods are successful in 49/50 methods, shown in figure 1. 
<center><img src='/images/MIT50_bayesian.jpeg' width='400' height='700'></center>
There are significant computational cost savings during the path finding optimization by replacing the electronic energy gradient calls with gradients of neural network potential energy function obtained by automatic differentiation. This is shown below in the results of the benchmarking on the set of classic organic test reactions. In each reaction the calculations using GNNP during the nonlocal path-finding step require significantly fewer gradient calculations, as the GNNP gradient calculations are negligible compared to ab initio gradient calculations. 
<center><img src='/images/Shaama_set_bayesian.jpeg' width='400' height='700'></center>
In some exceptions, marked by an asterisk, the transition state is not found. It should be noted that these exceptions do not exclusively occur with GNNP based calculations, and are a result of both algorithm hyperparameters and the potential.
