---
title: 'LDPC Edge Distribution Optimization I: Local Optimization'
date: 2020-07-13
permalink: /posts/2020/07/blog-post-1/
tags:
  - LDPC
  - Optimization
  - research
---

One can locally or globally optimize LDPC edge distribution. This post briefly discuss how to locally optimize edge distribution following [Richardson's work](https://ieeexplore.ieee.org/document/910578).

Notations
======
We want to optimize variable node edge distribution $$\lambda(x)=\sum_{i=2}^{d_v}\lambda_i x^{i-1}$$ and check node edge distribution $$\rho(x)$$ such that *threhold* is maximized under specific channels. 

> **Threshold is defined as the supremum of all channel parameters for which the probability of error under density evolution (DE)converges to zero**.

 In optimization, we set a finite maximum iteration $$m$$ and a target error probability $$\epsilon$$, therefore the threshold computed is a *lower bound* of true threshold.

As pointed out in the [paper](https://ieeexplore.ieee.org/document/910578), the implementation of DE inevitably involves quantization. This makes quantization error which accumlates in each iteration and fianlly makes the computation invalid. However, 

> By carefully performing the quantization, one can ensure that the quantized density evolution corresponds to the **exact density evolution of a quantized message passing algorithm**. 


-----

Hill-Climbing Approach
=====
This section talks about the implementation of local optimization. Richard's paper introcuced several local optimization methods, here we show two. One uses gradiant descent approach and the other one uses linear programming. $$\lambda(x)$$ will be used as an example to illustrate optimization.


Gradient Descent Approach
---
Let's define $$L(\lambda)$$ is the number of iterations to decrease error probability to $$\epsilon$$ under $$\lambda(x)$$. If $$L(\lambda)$$ is continuous w.r.t. $$\lambda$$, then we only need to find $$h=-\frac{dL}{d\lambda}$$ and take $$\tilde{\lambda}=\lambda+\eta$$.

Denote $$\{p_l\}_{l=0}^m$$ be a sequence of error probabilities as a result of density evolution. $$p_0$$ is raw probability and $$p_l$$ is the error probability after $$l^{th}$$ iteration. Especially, we have $$p_m\leq\epsilon\leq p_{m-1}$$. (**The $$\eplison$$ here is some error probability that between $m-1$ $m$ iteration, we can also see this optimization problem as minimize $$\epsilon$$ to our target.**)



