---
title: 'Nonparametric Bayesian Methods: Dirichlet distribution and Dirichlet process'
date: 2021-08-14
permalink: /posts/2021/08/blog-post-DPMM/
tags:
  - Bayesian Statistics
  - Machine Learning
---




## 1 Introduction

Nonparametric Bayes is Bayesian statistics that is not parametric. This doesn't mean that nonparametric models haven't any parameters, but the opposite,  they are models that have an infinite number of parameters, or a growing number of parameters.
So a Bayesian nonparametric (BNP) model is a Bayesian model on an infinite-dimensional parameter space, where the parameter space is usually chosen as the set of all possible solutions for a given learning problem. The model will use only a finite subset of parameters in order to explain a finite sample of observations, such that the complexity of our model depends on the data, thus the dimensions used by the model depend on the sample. The most popular BNP models are the Gaussian process for regression and the Dirichlet process mixture model (DPMM) for clustering. But BNP are not limited just to regression and clustering, in fact, BNP have been applied to a variety of statistical and machine learning problems such as density estimation, regression, classifications, image segmentation, and others.
A more intuitive way to define this nonparametric model is as a parametric model where the number of parameters increases with the number of data. The idea behind it is that the world is infinitely complicated, hence as we take more data we learn more parameters, learning more about the world.
The main motivation, but not the only one [1], to use Bayesian nonparametric, is that most traditional statistical models with a finite number of parameters must be subjected to model selection, otherwise, the model can suffer from under fitting or over fitting the data. It is often very difficult to know in advance how complex the model has to be in order to capture the important information in our dataset. This is a problem, because model selection, in general, is computationally expensive, whether we use cross-validation or other methods. If we choose the wrong number of parameters the model doesn't work properly. If too many clusters are used the model will easily over fit the data. On the other hand, if the number of clusters chosen is too small the model will under fit the data. With BNP, instead, the Bayesian approach of computing or approximating the posterior distribution over parameters mitigates overfitting. By using a very large Bayesian model or one that grows with the data, the model doesn't under fit either. Note that this doesn't solve our problem, incorrect specification of the model leads to a miss fit model and very low generalization performance.

A more theoretical motivation is $ De \ Finetti's \ theorem$, for which we first need to define what an infinitely exchangeable sequence is:

An infinite data sequence $X_1,X_2,X_3,...$  is infinitely exchangeble if the joint distribution of any N data points doesn't change when they are permuted:

$$P(X_1,X_2,...,X_N)=P(X_{ \sigma (1)},...,X_{\sigma (N)})$$

This is a weaker assumption than i.i.d. or simply independence. It can be easily seen that independence implies exchangeability, but exchangeability doesn't imply independence.

The $De\ Finetti's \ theorem$ \ [2] says that : a sequence $X_1,X_2,...$ of binary random variable is infinitely exchengeable if and only if, for all N and some measure $P$ on $\theta$ :

\begin{equation}
  \label{eq:1}
   P(X_1,X_2,...,X_N)=\int_{\theta} P(\theta) \prod_{i=1}^N P(X_i| \theta) d\theta 
\end{equation}

In other words, the theorem says that if and only if the data is an infinitely exchangeable sequence, then given the random parameter $\theta$, that is a prior, the data is conditionally independent and identically distributed. De Finetti showed this only for Bernoulli random variables in 1931, then Hewitt and Savage [8] generalized this result to arbitrarily distributed real-valued random variables in 1955. Note that the parameter $\theta$ is not restricted in complexity. This provides a motivation for the search of infinite-dimensional priors, or in other words, BNP methods.

The most popular Bayesian nonparametric model for clustering is the Dirichlet process (DP). Introduced by Ferguson [3] in 1973, the use of DP as prior for clustering methods has become computationally feasible only with the development of Markov chain Monte Carlo (MCMC) for sampling from the posterior distribution [4]. 

This section continues exploring the Dirichlet distribution and the DP, afterwards are introduces the Chinese restaurant process (CRP) and the stick-breaking process. Then, I will introduce DPMM, the section ends by outlining some approaches to posterior inference. 

## 1.2 Dirichlet Distribution 


The Dirichlet distribution is a multivariate probability distributions, it is a multivariate generalization of the Beta distribution. The Dirichlet distribution, in particular it is a distribution over the $K-1$ dimensional simplex, that is a $(K-1)$ dimensional object living in a $K$-dimensional space. It is parametrized by a $K$-dimensional vector $ \boldsymbol\alpha = ( \alpha_1,...,\alpha_K)$, a vector of reals, such that $a_k \geq 0$, $k = 1,...,K$, and $\sum_k^K \alpha_k  > 0$ .
The probability vector $\boldsymbol\pi$  is Dirichlet distributed, $\boldsymbol\pi \sim Dir(\boldsymbol\alpha)$, if it has probability density function :
\begin{equation}
  \label{eq:2}
       p(\boldsymbol\pi|\boldsymbol\alpha)= \frac{\Gamma (\sum_{k=1}^K \alpha_k)}{\prod_{k=1}^{K} \Gamma(\alpha_k)} \prod_{k=1}^K \pi_k^{\alpha_k -1}
\end{equation}

Let $\boldsymbol\pi \sim Dir(\boldsymbol\alpha)$, then $\boldsymbol\pi$ belongs to the $K-1$ simplex, that means: $\sum_{k=1}^K \pi_k = 1$ and $\pi_k \geq 0$ for each $k = 1,...,K$.
If $K = 2$ then a drawn from a Dirichlet distribution can be equally represented by a Beta distribution. In fact, given $(\pi_1,\pi_2) \sim Dir(\alpha_1, 1- \alpha_1)$, then $\pi_1 \sim Beta(\alpha_1)$ and $\pi_2$ is $1-\pi_1$.

Let $\boldsymbol\pi \sim Dir(\boldsymbol\alpha)$, with $\boldsymbol\pi = (\pi_1,...,\pi_K)$, and define $\alpha_0 = \sum_i \alpha_i$, then the main properties of the Dirichlet distribution \cite{5} will follow: 

\begin{equation}
  \label{eq:3}
  E[\pi_i] = \frac{\alpha_i}{\alpha_0} = \tilde{\alpha_i}
\end{equation}
\begin{equation}
  \label{eq:4}
  Var[\pi_i] = \frac{\tilde{\alpha_i}(1-\tilde{\alpha_i})}{\alpha_0 + 1}
\end{equation}
\begin{equation}
  \label{eq:5}
   \pi_i \sim Beta(\alpha_i, \alpha_0-\alpha_i)
\end{equation}
\begin{equation}
  \label{eq:6}
   (\pi_1,\pi_2,...,\pi_i+\pi_j,...\pi_K) \sim Dir(\alpha_1,\alpha_2,...,\alpha_i + \alpha_j,...,\alpha_K)
\end{equation}

An example of the Dirichlet distribution can be seen in \autoref{fig:1}.



![Dir](Dir.png)
|:--:| 
| *This is an example of a Dirichlet distribution with $K=3$, the yellow colour indicates higher density. Top left: the density is uniformly spreads, with $E[\boldsymbol\pi]=(1/3,1/3,1/3)$. Top right: when the values of $\boldsymbol\alpha$ are equal and greater than 1, the density is concentrated in the centre so again with $E[\boldsymbol\pi]=(1/3,1/3,1/3)$, but the variance is lower. The same behaviour is observed for the middle left figure. Middle right: the values of $\boldsymbol\alpha$ are greater than one, but one is greater than the others, now the density is concentrated toward the angle with a higher value of $\alpha$. Bottom right: when the values of $\boldsymbol\alpha$ are lower than one, the density concentrate to the extreme, an observation, in this case, will tend to have high mass in one of the 3 angles and very low ones in the other 2* |




The Dirichlet distribution is commonly used in Bayesian statistics as the prior distribution. This is because the Dirichlet distribution is the conjugate prior to the multinomial and categorical distribution. This means that if the prior distribution of the multinomial parameters is Dirichlet and the likelihood function is the one of a multinomial or categorical distribution, then the posterior distribution is also a Dirichlet distribution [5].  
Let $X \sim Multinomial_k(n,\mathbf{p})$, such that $n$ is a positive integer and $\mathbf{p}=(p_1,...,p_k)$ are the probability parameters, then the probability mass function of $X$ is :

\begin{equation}
  \label{eq:7}
  f(x_1,...,x_k|n,\mathbf{p}) = \frac{n!}{x_1!x_2!...x_k!} \prod_{i=1}^{k}p_i^{x_i}
\end{equation}

Now a Dirichlet  prior is used for the probability parameter $\mathbf{p}$.
That is, $ X | \mathbf{p} \sim Multinomial_k(n,\mathbf{p})$
and $P \sim Dir(\boldsymbol\alpha)$. The posterior distribution of $P$ is still a Dirichlet distribution with different parameters, $P | X=x \sim Dir(\alpha + x)$

## 1.3 Dirichlet process


In this section the Dirichlet process is introduced along with some of its properties [13] [14].
The Dirichlet process (DP) is a stochastic process. In order to define the DP consider $G_0$, which is a distribution over some space $\Theta$ and consider a positive real number $\alpha$. $G_0$ is called the base measure, and $\alpha$ is called the concentration parameter. Define DP($\alpha$,$G_0$) as the Dirichlet process parametrized by $\alpha$ and $G_0$. Its realization $G$ is a probability measure on $\Theta$. This means that for any finite partition ($A_1$,...,$A_n$ ) of $\Theta$, the vector ($G(A_1)$,...,$G(A_n)$ ) is a probability vector. G is DP distributed with base measure $G_0$ and concentration parameter $\alpha$, written $G \sim DP(\alpha,G_0)$, if :

\begin{equation}
  \label{eq:8}
  (G(A_1),...,G(A_n) ) \sim Dir(\alpha G_0 (A_1),..., \alpha G_0 (A_n ) )
\end{equation}


for every finite measurable partition $A_1$,..., $A_n$ of $\Theta$. Note that $\alpha G_0 (A_1) $ is just a scalar.
The marginal distributions of G are Dirichlet distributed. Therefore the DP is a distribution over distributions, meaning that each realization $G$ of a DP is itself a distribution. Even though there are other definitions of DP that give a better intuition about it, such as the Chinese restaurant process and the stick-breaking construction (we will see them in the next post), from the previous equation it is easier to derive some properties of the DP.
Indeed, by the properties of the Dirichlet distribution, it follows that for any measurable set $A \subset \Theta$, $E [G (A)] = G_0 (A) $. While the variance $VAR[G(A)]=\frac{G_0(A)(1-G_0(A)}{\alpha + 1}$. More intuitively, the base measure $G_0$ is the mean of the DP, instead, the concentration parameter $\alpha$ can be seen as the inverse of the variance of the DP As $\alpha $ approach infinity, $G (A) \rightarrow G_0 (A) $ for any measurable $A$. Note that this doesn't mean that $G \rightarrow G_0$. In fact, a draw from a DP is always discrete even if $G_0$ is smooth.

Let $G \sim DP(\alpha, G_0) $. Then, since G is itself a probability measure, a sequence of $N$ independent draws is sampled from it. This can be rewritten as the following hierarchical model: 
\begin{equation}
  \label{eq:9}
  G \sim DP(\alpha, G_0 )
\end{equation}

\begin{equation}
  \label{eq:10}
  \theta_i \sim G \ \ \  for \  i =1,...,N
\end{equation}

Note that since $G$ is a distribution over $\Theta$, then all the $\theta_i \in \Theta$. Let $A_1,..., A_m$ be a finite and measurable partition of $\Theta$. Consider $ \delta_{A_k} (.) $, which is the indicator function for the set $A_k$, define:
\begin{equation}
  \label{eq:11}
  n_k = \sum_{i=1}^{N} \delta_{A_k}(\theta_i)
\end{equation}
$n_k$ is the number of $\theta_i$ in $A_k$. Since the vector $ (n_1,..., n_m) $ is multinomial distributed and its probability vector has a Dirichlet prior, thanks to the conjugacy of the Dirichlet distribution, the posterior distribution is:

\begin{equation}
  \label{eq:12}
 (G(A_1),...,G(A_m) )| \theta_1,... ,\theta_N \sim Dir(\alpha G_0 (A_1)+n_1,..., \alpha G_0 (A_m )+n_m )
\end{equation}

This is true for any partition of $\theta$. This means that the posterior over $G$ is a stochastic process, with marginals that are Dirichlet distributed. Therefore the posterior distribution over G is again a DP. The new DP has updated concentration measure $\alpha + N$ and base distribution $( \frac{\alpha}{\alpha + N}G_0 +  \frac{1}{\alpha + N} \sum_{i=1}^N \delta_{\theta_i}   )$ : 

\begin{equation}
  \label{eq:13}
 G| \theta_1,...,\theta_N \sim DP(\alpha+N, \frac{\alpha}{\alpha + N}G_0 +  \frac{1}{\alpha + N} \sum_{i=1}^N \delta_{\theta_i}    )
\end{equation}

By rewriting the updated base distribution as: 
\begin{equation}
  \label{eq:14}
  \frac{\alpha}{\alpha + N}G_0 +  \frac{N}{\alpha + N} \frac{\sum_{i=1}^N \delta_{\theta_i} }{N}   
\end{equation}
It is easy to see that the posterior base distribution is a weighted average of the prior base function $G_0$ and the new empirical distribution $\sum_{i=1}^N \delta_{\theta_i}$, which is a sum of point masses. $G_0$ has weights proportional to $\alpha$, so if we let $\alpha \rightarrow 0 $ the prior becomes non-informative. $\sum_{i=1}^N \delta_{\theta_i}$ instead, has weights proportional to $N$. Therefore, as the number of observations increases, i.e. $N \> \> \alpha$, the empirical distribution dominates the posterior, approximating the true underlying distribution. Readers interested in exploring further this topic can see [6].

In the next chapter I will explore 2 ways of constructing the DP: the chinese restaurant process and the stick-breaking process.
Stay tuned!!
