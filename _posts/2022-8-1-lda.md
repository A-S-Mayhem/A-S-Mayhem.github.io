---
title: 'Latent Dirichlet Allocation'
date: 2022-8-1
permalink: /posts/lda
toc: true
---

Notation:
- $K$: number of topics.
- $W$: number of words in the vocabulary.
- $D$: number of documents.
- $\beta_k (k = 1, \ldots, k)$: topic-word distribution. $\beta_k$ is a non-negative vector of length $W$ that sums to 1.
- $\eta$: Dirichlet prior parameter for $\beta_k$. 
- $\theta_d$: document-topic distribution. $\theta_d$ is a non-negative vector of length $K$ that sums to 1.
- $\alpha$: Dirichlet prior parameter for $\theta_d$.
- $z_{di}$: topic assignment for word $i$ in document $d$. $z_{d_i} \in \{1, \ldots, K\}$.
- $w_{di}$: word $i$ in document $d$. $w_{di} \in \{1, \ldots, W\}$.

The generative process is as follows.
- Draw topic-word distributions $\beta_k \sim \text{Dir}(\eta)$ for $k = 1, \ldots, K$.
- For each document $d = 1, \ldots, D$:
  - Draw document-topic distribution for document $d$: $\theta_d \sim \text{Dir}(\alpha)$.
  - For each word $i$ in document $d$:
    - Draw a topic $z_{di} \sim \theta_d$.
    - Draw a word $w_{di} \sim \beta_{z_{di}}$.

The full model is
$$
\begin{align*}
p(w, z, \theta, \beta \mid \alpha, \eta) & = p(\beta \mid \eta) \prod_{d=1}^{D} p(\theta_d \mid \alpha) p(z_d \mid \theta_d) p(w_d \mid \theta_d, z_i, \beta) \\
& = \prod_{k=1}^{K} p(\beta_k \mid \eta) \prod_{d=1}^{D} p(\theta_d \mid \alpha) \prod_{i=1}^{N_d} p(z_{di} \mid \theta_d) p(w_{di} \mid \theta_d, z_{di}, \beta).
\end{align*}
$$

## Variational Inference

It is useful to remind ourselves that in this mixture model, the only observable variables are $w$, and the latent variables are $z$, $\theta$, and $\beta$. Inference, therefore, is the calculation of the probability $p(z, \theta, \beta \mid w, \alpha, \eta)$. We know that this probability is intractable, so we use variational inference to approximate it.

The first step is to define a simpler distribution $q(z, \theta, \beta)$. Further, we will impose the mean field assumption on this distribution, which allows us to factorize it into a product of three distributions:
- $q(z_{di}) = \phi_{d w_{di} k}$. The dimensionality of $\phi$ is $D \times W \times K$, and $\sum_{k=1}^{K} \phi_{d w k} = 1, \forall d, w$;
- $\theta_d \sim \text{Dir}(\gamma_d)$, where $\gamma_d$ is a vector of length $K$;
- $\beta_k \sim \text{Dir}(\lambda_k)$, where $\lambda_k$ is a vector of length $W$.

In summary, $q(z_d, \theta_d,\beta) = q(z_d) q(\theta_d)q(\beta)$, and we have three types of variational parameters: $\phi$ of size $D \times W \times K$; $\gamma_d$ of size $K$, for $d = 1, \ldots, D$; and $\lambda_k$ of size $W$, for $k = 1, \ldots, K$.

### Evidence lower bound (ELBO)

The "best" parameters are the ones that maximize the ELBO, or evidence lower bound, defined as

$$
\begin{align*}
\mathcal{L}(w, \phi, \gamma, \lambda) = \mathbb{E}_q\left[ \log p(w, z, \theta, \beta \mid \alpha, \eta) \right] - \mathbb{E}_q\left[ q(z, \theta, \beta) \right].
\end{align*}
$$

$\log p(w, z, \theta, \beta \mid \alpha, \eta) = \log p(\beta \mid \eta) + \sum_{d=1}^{D} \left[ \log p(\theta_d \mid \alpha) + \log p(z_d \mid \theta_d) + \log p(w_d \mid z_d, \theta_d, \beta) \right]$

$\log q(z_d, \theta_d,\beta) = \log q(z_d) + \log q(\theta_d) + \log q(\beta)$.

We can decompose the ELBO as follows:
$$
\begin{align}
\mathcal{L}(w, \phi, \gamma, \lambda) & = 
\sum_{d=1}^{D} 
\left\{ 
\mathbb{E}_q\left[ \log p(w_d \mid \theta_d, z_d, \beta) \right] + 
\mathbb{E}_q\left[ \log p(z_d \mid \theta_d) \right] - 
\mathbb{E}_q\left[ \log p(\theta_d \mid \alpha) \right] 
\right\} \nonumber \\
&~~~~ 
-
\sum_{d=1}^{D} 
\left\{ 
\mathbb{E}_q\left[ \log q(z_d \mid \theta_d) \right] + 
\mathbb{E}_q\left[ \log q(\theta_d) \right] 
\right\} \nonumber \\
&~~~~ + \mathbb{E}_q\left[ p(\beta \mid \eta) \right] - \mathbb{E}_q\left[ \log q(\beta) \right] \nonumber \\
& = \sum_{d=1}^{D} \left\{ \mathbb{E}_q\left[ \log p(w_d \mid \theta_d, z_d, \beta) \right] +  \mathbb{E}_q\left[ \log p(z_d \mid \theta_d) \right] - \mathbb{E}_q\left[ \log q(z_d \mid \theta_d) \right] \right.  \nonumber\\
&\quad \quad \quad ~
\left.\mathbb{E}_q\left[ \log p(\theta_d \mid \alpha) \right]  - \mathbb{E}_q\left[ \log q(\theta_d) \right]
\right\} \nonumber \\
& ~~~~ + (\mathbb{E}_q\left[ p(\beta \mid \eta) \right] - \mathbb{E}_q\left[ \log q(\beta) \right])  \label{eq:elbo} \\
\end{align}
$$

### ELBO as a function of variational parameters

Analyzing each term in the sum.
$$
\begin{align}
\mathbb{E}_q\left[ \log p(w_d \mid \theta_d, z_d, \beta) \right] & = \sum_{i=1}^{N_d} \mathbb{E}_q\left[ \log p(w_{di} \mid \theta_d, z_{di}, \beta) \right] \nonumber \\ 
& = \sum_{i=1}^{N_d} \sum_{k=1}^{K} q(z_{di} = k) \mathbb{E}_q\left[ \log p(w_{di} \mid \theta_d, z_{di}, \beta) \right] \nonumber \\
& = \sum_{i=1}^{N_d} \sum_{k=1}^{K} \phi_{d w_{di} k} \mathbb{E}_q\left[ \log \beta_{k w_{di}} \right], \nonumber
\end{align}
$$

where the expectation on the last row is with respect to $q(\beta_k)$. We can see that in this formula, the contribution of each word $w$ to the term is $\sum_{k=1}^{K} \phi_{d w k} \mathbb{E} \left[ \log \beta_{k w} \right]$, which is the same for regardless of the position of word $w$ in document d. Therefore, we can simply count the number of times $w$ appears in $d$, and then multiply it with this contribution to get the contribution of all occurrences of $w$. This gives us the equivalent expression:
$$
\begin{align}
\mathbb{E}_q\left[ \log p(w_d \mid \theta_d, z_d, \beta) \right] = \sum_{w=1}^{W} n_{dw} \sum_{k=1}^{K} \phi_{d w k} \mathbb{E}_q\left[ \log \beta_{k w} \right], \label{eq:elbo:1}
\end{align}
$$

where $n_{dw}$ is the number of occurrences of word $w$ in document $d$. Using the same trick, we have
$$
\begin{align}
\mathbb{E}_q\left[ \log p(z_d \mid \theta_d) \right] & = \sum_{w=1}^{W} n_{dw} \sum_{k=1}^{K} \phi_{d w k} \mathbb{E}_q\left[ \log \theta_{dk} \right], \text{and} \label{eq:elbo:2} \\
\mathbb{E}_q\left[ \log q(z_d) \right] & = \sum_{w=1}^{W} n_{dw} \sum_{k=1}^{K} \phi_{d w k} \log \phi_{d w k}. \label{eq:elbo:3}
\end{align}
$$

For the last two terms inside the sum, first note that $p(\theta_d \mid \alpha)$ is a Dirichlet distribution with symmetric parameter $\alpha$, i.e., $q(\theta_d \mid \alpha) = \frac{\Gamma(K \alpha)}{\Gamma(\alpha)^K} \prod_{k=1}^{K} \theta_{dk}^{\alpha-1}$. Therefore,
$$
\begin{align}
\mathbb{E}_q\left[ \log p(\theta_d \mid \alpha) \right] = \log \Gamma(K \alpha) - K \log \Gamma(\alpha) + (\alpha - 1) \sum_{k=1}^{K} \log \theta_{dk}. \label{eq:elbo:4}
\end{align}
$$

Similarly, because $q(\theta_d)$ is a Dirichlet distribution with asymmetric parameter $\gamma_d$, we have
$$
\begin{align}
\mathbb{E}_q\left[ \log q(\theta_d) \right] = \log \Gamma\left(\sum_{k=1}^{K} \gamma_{dk} \right) - \sum_{k=1}^{K} \log \Gamma(\gamma_{dk}) + \sum_{k=1}^{K} (\theta_{dk} - 1) \log \theta_{dk}. \label{eq:elbo:5}
\end{align}
$$

Now for the last two terms, also note that $p(\beta_k \mid \eta)$ is Dirichlet with symmetric $\eta$. Therefore,
$$
\begin{align}
\mathbb{E}_q\left[ \log p(\beta \mid \eta) \right] &= \sum_{k=1}^{K} \mathbb{E}_q\left[ \log p(\beta_k \mid \eta) \right] \nonumber \\
&= K [\log \Gamma(W \eta) - W \log \Gamma(\eta)] + \sum_{k=1}^{K} \sum_{w=1}^{W} (\eta - 1)  \mathbb{E}_q\left[ \log \beta_{k w} \right]. \label{eq:elbo:6}
\end{align}
$$

Simlarly, the final term is
$$
\begin{align}
\mathbb{E}_q\left[ \log q(\beta) \right] &= \sum_{k=1}^{K} \mathbb{E}_q\left[ \log q(\beta_k) \right] \nonumber \\
&= \sum_{k=1}^{K} \left( \log \Gamma \left( \sum_{w=1}^{W} \lambda_{kw} \right) - \sum_{w=1}^{W} \Gamma(\lambda_{kw}) + \sum_{w=1}^{W} (\lambda_{kw} - 1) \mathbb{E}_q\left[ \log \beta_{k w} \right]  \right). \label{eq:elbo:7}
\end{align}
$$

Plugging $\eqref{eq:elbo:1}, \eqref{eq:elbo:2}, \eqref{eq:elbo:3}, \eqref{eq:elbo:4}, \eqref{eq:elbo:5}, \eqref{eq:elbo:6}, \eqref{eq:elbo:7}$ into $\eqref{eq:elbo}$, we have the ELBO as a function of variational parameters:

$$
\begin{align}
\mathcal{L} &= \sum_{d=1}^{D} \left\{ \sum_{w=1}^{W} n_{dw} \sum_{k=1}^{K} \phi_{dwk} \left( \mathbb{E}_q\left[ \log \theta_{dk} \right] + \mathbb{E}_q\left[ \log \beta_{k w} \right] - \log \phi_{dwk} \right) \right. \nonumber\\
& \left. \quad \quad \quad ~ - \log \Gamma\left( \sum_{k=1}^{K} \gamma_{dk} \right) + \sum_{k=1}^{K}\left( \log \Gamma(\gamma_{dk}) + (\alpha - \gamma_{dk}) \mathbb{E}_q\left[ \log \theta_{dk} \right] \right) \right\} \nonumber \\
&~~~~ + \sum_{k=1}^{K} \left( - \log \Gamma\left( \sum_{w}^{W} \lambda_{kw} \right) + \sum_{w=1}^{W} \left( \log \Gamma(\lambda_{kw}) + (\eta - \lambda_{kw}) \mathbb{E}_q\left[ \log \beta_{k w} \right] \right) \right) \nonumber \\
&~~~~ + D [\log \Gamma(K \alpha) - K \log \Gamma(\alpha)] + K [\log \Gamma(W \eta) - W \log \Gamma(\eta)]. \label{eq:elbo:var}
\end{align}
$$

## Variational Bayes for LDA

The main objective here is to maximize the ELBO $\mathcal{L}$ with respect to the variational parameters $\phi$, $\gamma$ and $\lambda$. To do so, we will use a procedure called *coordinate ascent*, in which we maximize $\mathcal{L}$ with respect to one set of parameters, keeping the others fixed. We will then alternate to another set of variables, keeping others fixed, and so on. In our LDA example, we first keep $\gamma$ and $\lambda$ fixed, and maximize $\mathcal{L}$ as a function of $\phi$ only. Then we do the same for $\gamma$ and $\lambda$.

### Maximizing with respect to $\phi$
Only keeping the terms involving $\phi_{dwk}$ in $\eqref{eq:elbo:var}$, and treating everything else as constants, we have the objective function w.r.t. $\phi_{dwk}$ as

$$
\mathcal{L}_{[\phi_{dwk}]} = \phi_{dwk} \left( \mathbb{E}_q\left[ \log \theta_{dk} \right] + \mathbb{E}_q\left[ \log \beta_{k w} \right] - \log \phi_{dwk} \right) + \text{const},
$$

which gives the gradient:

$$
\frac{\partial \mathcal{L}}{\partial \phi_{dwk}} = \mathbb{E}_q\left[ \log \theta_{dk} \right] + \mathbb{E}_q\left[ \log \beta_{k w} \right] - \log \phi_{dwk} - 1.
$$

Setting the gradient to zero and solving for $\phi_{dwk}$, we get the update rule for $\phi_{dwk}$:

$$
\begin{align}
\phi_{dwk} \propto  \exp \left\{ \mathbb{E}_q\left[ \log \theta_{dk} \right] + \mathbb{E}_q\left[ \log \beta_{k w} \right] \right\}. \label{eq:update:phi}
\end{align}
$$

Where we have suppressed all multiplicative constants by using $\propto$. After this update for all $\phi_{dwk}$, we can simply rescale them so that $\sum_{k=1}^{K} \phi_{dwk} = 1, \forall d, w$.

The final thing to handle is the expectations inside $\exp$. How do we calculate them exactly? Lucklily, both of them can be calculated using the [*digamma function*](https://en.wikipedia.org/wiki/Digamma_function) $\Psi$---the first derivative of the logarithm of the gamma function--- as follows:

$$
\begin{align*}
\mathbb{E}_q\left[ \log \theta_{dk} \right] & = \Psi(\gamma_{dk}) - \Psi\left(\sum_{i=1}^{K} \gamma_{di}\right), \\
\mathbb{E}_q\left[ \log \beta_{k w} \right] & = \Psi(\lambda_{kw}) - \Psi\left(\sum_{i=1}^{W} \lambda_{ki}\right).
\end{align*}
$$

### Maximizing with respect to $\gamma$

Similarly, the objective function w.r.t. $\gamma_{dk}$ is

$$
\begin{align*}
\mathcal{L}_{[\gamma_{dk}]} & = \sum_{w=1}^{W} n_{dw} \phi_{dwk} \mathbb{E}_q \left[ \log \theta_{dk} \right] - \log \Gamma\left( \sum_{i=1}^{K} \gamma_{d_i} \right) \\
& ~~~~+ \log \Gamma(\gamma_{dk}) + (\alpha - \gamma_{dk}) \mathbb{E}_q \left[ \log \theta_{dk} \right] + \text{const} \\
& = \left( \alpha + \sum_{w=1}^{W} n_{dw} \phi_{dwk} - \gamma_{dk}  \right) \left( \Psi(\gamma_{dk}) - \Psi\left(\sum_{i=1}^{K} \gamma_{di}\right) \right) \\
& ~~~~ - \log \Gamma\left( \sum_{i=1}^{K} \gamma_{d_i} \right) + \log \Gamma(\gamma_{dk}) + \text{const},
\end{align*}
$$

where we have used the digamma function $\Psi$ similarly to the previous section. A simple manipulation gives the gradient:

$$
\begin{align*}
\frac{\partial \mathcal{L}}{\partial \gamma_{dk}} = \left( \Psi'(\gamma_{dk}) - \Psi'\left(\sum_{i=1}^{K} \gamma_{di}\right) \right) \left( \alpha + \sum_{w=1}^{W} n_{dw} \phi_{dwk} - \gamma_{dk}  \right).
\end{align*}
$$

Setting this gradient to zero and solving for $\gamma_{dk}$, we get the update rule for $\gamma_{dk}$:

$$
\begin{align}
\gamma_{dk} = \alpha + \sum_{w=1}^{W} n_{dw} \phi_{dwk}. \label{eq:update:gamma}
\end{align}
$$

The variational Bayes estimate of $\gamma$ has an intuitive explanation. The number of times document $d$ is assigned to topic $k$ is the weighted sum of the times each word in $d$ is assigned to topic $k$, where the weight $\phi_{dwk}$ is the probability that word $w$ in document $d$ belongs to topic $k$---plus the Dirichlet prior $\eta$.

### Maximizing with respect to $\lambda$

Similar to $\gamma$, we can use the digamma function $\Psi$ in the objective functin w.r.t. $\lambda_{kw}$ as follows

$$
\begin{align*}
\mathcal{L}_{[\lambda_{kw}]} & = \left( \eta + \sum_{d=1}^{D} n_{dw} \phi_{dwk} - \lambda_{kw} \right) \left( \Psi(\lambda_{kw}) - \Psi\left(\sum_{i=1}^{W} \lambda_{ki} \right) \right) \\
& = - \log \Gamma\left(\sum_{i=1}^{W} \lambda_{ki} \right) + \log \Gamma(\lambda_{kw}) + \text{const},
\end{align*}
$$

which gives the gradient:

$$
\begin{align*}
\frac{\partial \mathcal{L}}{\partial \lambda_{kw}} = \left( \Psi'(\lambda_{kw}) - \Psi'\left(\sum_{i=1}^{W} \lambda_{ki} \right) \right) \left( \eta + \sum_{d=1}^{D} n_{dw} \phi_{dwk} - \lambda_{kw} \right).
\end{align*}
$$

Setting the gradient to zero and solving for $\lambda_{kw}$, we get the update estimate:

$$
\begin{align}
\lambda_{kw} = \eta + \sum_{d=1}^{D} n_{dw} \phi_{dwk}. \label{eq:update:lambda}
\end{align}
$$

Similar to $\gamma_{dk}$, the variational Bayes estimate of $\lambda$ has an intuitive explanation. The count of word $w$ in topic $k$ the weighted sum of word count for $w$ in each document $d$, where the weight $\phi_{dwk}$ is the probability that word $w$ in document $d$ belongs to topic $k$---plus the Dirichlet prior $\eta$.