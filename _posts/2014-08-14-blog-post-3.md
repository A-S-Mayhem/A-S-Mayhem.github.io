---
title: 'Deep Neural Network and Gaussian process'
date: 2022-02-20
permalink: /posts/2022/02/NN/
tags:
  - Neural Network
  - Machine Learning
---

# Introduction
As the use of Neural Network (NN) is lead by their efficacy and impressive results, the theory behind them still very uncertain.
One way to study property of NN is to put priors on the networks parameters and study its property as the width of the layers goes to infinity.
In other words use hidden layer with an infinite number of hidden unit and study the limiting distribution.
Neal in the 1994 proved that a single layer fully connected NN with Gaussian i.i.d. priors on the network's parameters is equivalent to a Gaussian Process (GP) as the width of the hidden layer grow to infinity.

### Single-Layer Neural Network
Bayesian statistics allows us to integrate information about the parameters of the model by putting a distribution on them.
In a Neural Network, the parameters are the unit biases and the connection weights.
The problem with the usual Bayesian approach is that the meaning of each weight or bias is unknown. The role of each parameter in the prediction of the output is not known, this makes putting priors on NN very difficult.
The intuition now is that putting not informative priors on a small number of hidden units will represent only a limited set of functions. 
But then the NN will be misspecified and finding the posterior became very difficult. Different methods have been developed to find the exact posterior given the data, they range from MCMC to Variational Inference methods. But here I will not treat the computational aspects maybe in a future post.

The fundamental link between infinite width NN and GP is the Central Limit Theorem and in some minutes you will understand why.

Let $\phi$ be a non-linear pointswise function usually the ReLU or the hyperbolic tangent is used. Let x be the input vector such that $x \in R^n$. The weights and biases for the input-to-hidden component are $W_{i,j}^0$ and $b_j^0$. Define $N_1$ as the number of hidden unit, and $N_2$ the number of output. So we will let $N_1$ going to infinity while $N_2$ will remain finite.

Let $x$ be the input vector of length $n$ then we can define the j-th hidden unit as:

$$ x_j^1(x) = \phi (b_j^0 +\sum_{k=1}^n W_{j,k}^0 x_k )$$

We now define the i-th output of the network, $z_i^1$ as:

$$ z_i^1(x) = b_i^1 +\sum_{j=1}^{N_1} W_{i,j}^1 x_j^1(x) )$$

Since we are bayesian today, the weights of layer $l$, $W_{i,j}^l$  are drawn i.i.d. from a Gaussian distribution with zero mean and variance $\frac{\sigma^2_W}{N_l}$.
While the biases for both layer are also drawn i.i.d. from a Gaussian distribution with mean zero but with variance $\sigma^2_b$.
Scaling the variance as the width of the layer goes to infinity is very important otherwise we will not obtaina well defined limit in the input-to-hidden units.

### Limit of Gaussian priors

We want to calculate the expected value of $W_{i,j}^1 x_j^1(x)$, fortunatly for us it is easy to calculate it, indeed :


$$ \mathbb{E}[W_{i,j}^1 x_j^1(x)] = \mathbb{E}[W_{i,j}^1] \mathbb{E}[x_j^1]$$

The equality is due to the independence of the draw of $W_{i,j}^1$ and weight and biases of the previus layer.
Then since $\mathbb{E}[W_{i,j}^1]=0$  by definition we obtain that the expected contribution of each hidden unit is 0.
Now we need to calculate the variance of the contrinution of each hidden unit, since they have first moment equal to zero, we just need the second moment:

$$\mathbb{E}[W_{i,j}^1 x_j^1(x)] = \mathbb{E}[(W_{i,j}^1)^2] \mathbb{E}[(x_j^1)^2 =$$ 

$$\ \ \ \ \ \ = \frac{\sigma^2_W}{N_1}   \ V(x^1)$$

where $\mathbb{E}[(x_j^1)^2]=V(x^1)$ and is the same for each hidden unit.

The i.i.d. priors over the paramters makes each $x_i^1$ and $x_j^1$ independent for every $i \neq j$.
This means that the i-th output of the network $  z_i^1(x)$ will be Gaussian distributed as the $N_1 \rightarrow \infty$ by CLT.
Indeed:

$$ \sum_{j=1}^{N_1} W_{i,j}^1 x_j^1(x) ) \rightarrow \mathbb{N}(0, N_1 \frac{\sigma^2_W}{N_1} \ V(x^1)  $$

and since $$ b_i^1 \sim \mathbb{N}(0,\sigma^2_b)$$
\ we obtain that:

$$  z_i^1(x) = b_i^1 +\sum_{j=1}^{N_1} W_{i,j}^1 x_j^1(x) ) \sim \mathbb{N}(0,\sigma^2_b+ \sigma^2_W \ V(x^1))$$
as $N_1$ goes to infinity.

Note that also the different outputs z_i^1(x) and z_j^1(x) will be independent for all $i \neq j$.


Assume now to have any finite collection of inputs $x^{(1)},x^{(2)},...,x^{(n)}$. If we focus on the i-th output of the newtwork, the prior joint distribution of $( z_i^1(x^{(1)},...,z_i^1(x^{(n)}) $ will have a multivariate Gaussian distribution by the multidimensional Central Limit Theorem. 
This type of distributions over functions, where the joint distribution of the values of the functions for any finite number of points is a multivariate Gaussian are know as Gaussian Process.

We can conclude that $z_i^1 \sim GP(\mu,K)$, where $\mu$ is the mean vector and $K$ the covariance matrix. Also the parameter of the GP are independent of the i.

Obviously the mean vector $\mu$ will be the zero vector: $$\mu (x)= \mathbb{E}[z_i^1(x)]=0$$.

While the prior covariance $K$ between different input values is different than 0, this is actually what allows the network/GP to learn:
$$ K(x^*,x' ) = \mathbb{E}[z_i^1(x^*) \ z_i^1(x')]= \sigma^2_b + \sigma^2_W \mathbb{E}[x_i^1(x^*) \ x_i^1(x')] $$

$$\ \ \ \ \ \ \ = \sigma^2_b + \sigma^2_W C(x^*,x') $$

Where $C(x^*,x')$ is obtained after integrating out $W^0$ and $b^0$.

This is just the first step to understand Neural Network, the study of the limiting distribution has been extended to multilayer NNs.
This is as of today an active area of research in machine learning and statistics.

This blog post was inspired and based on:

[1] Bayesian Learning for Neural Networks. Radford M Neal, PhD thesis, University of Toronto, 1995.

[2] Deep Neural Networks as Gaussian Processes. Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri, 2018.
