---
title: 'Research record on Proximal Policy Optimization (PPO)'
date: 2020-11-16
permalink: /_posts/2020/11/23/blog-post-1/ 
tags:
  - study
  - research
---

>  This blog collects some scientific rethinking on PPO


1. [PPO-CMA: PROXIMAL POLICY OPTIMIZATION WITH COVARIANCE MATRIX ADAPTATION](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9231618)
    * PPO represents on-policy RL methods, i.e., experience is assumed to be collected on-policy and thus must be discarded after the policy is updated.
    * In PPO: Actions with a negative advantages may cause instability when performing multiple minibatch gradient steps, as each step drives the policy Gaussian further away from the negative-advantage actions.

2. [MAXIMUM A POSTERIORI POLICY OPTIMISATION](https://arxiv.org/pdf/1806.06920.pdf)
    * PPO is on-policy by design, reducing gradient variance through large batches and limiting the allowed change in parameters. They are robust, applicable to high-dimensional problems, and require moderate parameter tuning. However, as on-policy algorithms, they suffer from **poor sample efficiency**.
    
3. [Proximal Policy Optimization with Relative Pearson Divergence](https://arxiv.org/pdf/2010.03290.pdf)
    * In PPO, no explicit regularization term is given, thereby making the problem mathematically difficult to understand.
    * Inconsistency between symmetric design of the threshold and asymmetric domain of the density ratio would induce unbalanced regularization.
    
4. [Truly Proximal Policy Optimization](http://proceedings.mlr.press/v115/wang20b/wang20b.pdf)
    * PPO could neither strictly restrict the probability ratio as it attempts to do nor enforce a well-defined trust region constraint, which means that it may still suffer from the risk of performance instability. 

5. [IMPLEMENTATION MATTERS IN DEEP POLICY GRADIENTS: A CASE STUDY ON PPO AND TRPO](https://arxiv.org/pdf/2005.12729.pdf)
    * Additional optimization techniques (**CODE LEVEL**) behind the published PPO method.