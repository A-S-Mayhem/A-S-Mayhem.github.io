---
title: 'Research record on Proximal Policy Optimization (PPO)'
date: 2020-11-16
permalink: /_posts/2020/11/23/blog-post-1/ 
tags:
  - study
  - research
---

>  This blog collects development of PPO

### Comments

1. [PPO-CMA: PROXIMAL POLICY OPTIMIZATION WITH COVARIANCE MATRIX ADAPTATION](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9231618)
    * PPO represents on-policy RL methods, i.e., experience is assumed to be collected on-policy and thus must be discarded after the policy is updated.

2. [MAXIMUM A POSTERIORI POLICY OPTIMISATION](https://arxiv.org/pdf/1806.06920.pdf)
    * PPO is on-policy by design, reducing gradient variance through large batches and limiting the allowed change in parameters. They are robust, applicable to high-dimensional problems, and require moderate parameter tuning. However, as on-policy algorithms, they suffer from **poor sample efficiency**.
    