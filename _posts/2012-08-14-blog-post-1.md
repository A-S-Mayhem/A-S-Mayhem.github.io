---
title:  "Deep Q-Network -- DQN"
date:  2021-02-20
permalink: /posts/2021/02/DQN/
tags:
  - reinforcement learning
  - machine learning
---

# Introduction
One of the main goal of Reinforcement Learning is to generalize win problem with a very large state and/or action spaces, a popular approach to address this problem in the past year relied on hand-crafted features combined with linear value functions ( value function approximation with linear feature reresentation). It work well given the right set of features, but it requires hand design feature.
But in 2015 Deepmind change direction, and use Artificial Neural Network as function approximation, the recent advance in the fild had made possible to extract high level feature form raw data.
### Why use Deep NN:
  1. Use distributed representation instead of local representation
  2. Can learn parameter useing SGD
  3. Can potentially need exponentially less parameter, with respect to shallow net, to represent the same funtion
  4. Are relly good at generalization specially in large domains

### A bit of background
More formally in DQN are use Convolution Neural Network(CNN) to approximate the optimal action-value function:
$Q^* (s,a)= max_\pi E[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + ... | s_t = s, a_t = a, \pi] $.

This can be rewrite using an identity known as Bellman Equation:$$Q^* (s,a)= E[r + \gamma max_{a'} Q^*(s',a')] $$

Most RL algorithms use the Bellman Equation to estimate the value function through iterative uptdate of $$Q_{i+1}(s,a) = E[r + \gamma max_{a'} Q_i(s',a')] $$
this value iteration algorithm is prove to converge to the optimal value funtion, but is impossible to utilize it in practice, so instead we use a function(a parametrized funtion) approximation to estimate $$Q(s,a, \theta) \approx Q^*(s,a)$$.
Our parametrize funtion is a non-linear funtion, the Deep neural network and the parameter $$\theta$$ represent the weights.

(Architecture details (size of conv. layer, kernel size ,etc...) are on the original paper "Human-level control through deep reinforcement
learning" and "Playing Atari with Deep Reinforcement Learning").

# Training algotithm for DQN
DNNs and other non-linear functino have been avoided in the past, for function approximation, because learning and training tend to be very unstable and could diverge very easily, these problems has several couses:
 - The correlation between the input data observed (they come in sequence so are correlated in time)
 - The correlation between the target values $$r+\gamma max{a'} Q(s',a')$$ and the action-value Q.

Deepmind address these problem with the use of : Experence Replay and a separate Target Network.

### Experience replay
We store the agent's experience at each time step $$e_t = (s_t, a_t, r_t, s_{t+1})$$ in a dataset(or buffer) $$D_t$$ with fix size. The replay buffer store only the most k=1000000 recent experiences.So now we sample a minibatch of past experience steps from the buffer in a uniformly random way, this approach has several advantage:
1. Greater data efficency
2. Remove sample correaltion
3. avoiding oscillation or divengence.

But there is also a disadvantage : the replay buffer doesn't differentiate important/informative transitions and overwrites with the recent transition and the uniform sampling gives equal importance to all experiences in the buffer, a method to solve this problem is the Prioritized Replay(paper.
)

### Target Network
In order to deal with the non-stationary learning target in the Q-learning update, a separate target network is used for generating the target $$y^{DQN}$$. Essentially the DQN is learned by minimizing the following MSE:
$$L(\theta) = E_{(s_t,s_t,r_t,s_{t+1})}[y_{t}^{DQN}-Q(s_t,a_t,\theta)]$$
and $$y^{DQN}$$ is the one-step ahead learning rate :
$$y^{DQN} = r_t + \gamma max_{a'} Q(s_{t+1}, a',\theta^-)$$.
Where $$\theta^-$$ represent the weights(parameters) of the target network, and $$\theta$$ represent the weights of the online network, which are update every step using SGD on $$L(\theta) = 1/n (\sum_{j=1}^n y^{DQN}_j - Q(s_j, a_j, r_j, s_{j+1})$$ , the experiences are from the minibatch sampled from the buffer.

Every C steps the parameter of the target network $$\theta^-$$ are update by copying the parameters of the online network $$\theta$$.

## Deep Q-learning algortithm
<img src="https://github.com/filippofiocchi/filippofiocchi.github.io/tree/master/images/DQN.png">

