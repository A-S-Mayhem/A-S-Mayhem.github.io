---
title: "Text Visualization in Social Science"
author: "Yongjun Zhang"
date: ""
output:
  rmdformats::readthedown:
    highlight: pygments
---

```{=html}
<style type="text/css">
p{ /* Normal  */
   font-size: 18px;
}
body{ /* Normal  */
   font-size: 18px;
}
td {  /* Table  */
   font-size: 14px;
}
h1 { /* Header 1 */
 font-size: 32px;
}
h2 { /* Header 2 */
 font-size: 26px;
}
h3 { /* Header 3 */
 font-size: 22px;
}
code.r{ /* Code block */
  font-size: 14px;
}
pre { /* Code block */
  font-size: 14px
}
</style>
```
```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE)

```
> We will start with basic data visualization in R focusing on ggplot2. Kieran Healy's [Data Visualization A practical introduction](https://socviz.co/) is a good resource to learn basic visualization. You can follow his book website and install his package for learning purposes

> After we are familiar with basic data visualization in R, we will switch to visualize texts using these basic techniques. We don't cover other fancy visualization tools. If you are interested in those tools, you can learn more by checking shiny, plotly, etc. 

> There are a couple of books you should read. [R for Data Science](https://r4ds.had.co.nz/); [ggplot2 cookbook](http://www.cookbook-r.com/Graphs/). For disclosure, some of the example codes are from R for data science.


# DataViz Basics in R

We need to load some packages for use

```{r}
if (!requireNamespace("pacman"))
  install.packages('pacman')
library(pacman)
packages<-c("tidyverse","tidytext","haven")
p_load(packages,character.only = TRUE)
```


## Tidy data

Let us load doca data into R, the input is a csv file.

```{r}
nyt_doca <- read_csv("data_doca.csv")
```
We will also use the doca main dataset. You can go here to download the dataset <https://web.stanford.edu/group/collectiveaction/cgi-bin/drupal/node/21>.

We use the read_dta function in haven to read stata file

```{r}
main_doca <- read_dta("final_data_v10.dta",encoding = "latin1")
```

Let us merge two datasets together using the key identifier title and title_doca. We will use tidyverse fuction left_join. It is similar to stata merge function.

```{r}
data <- main_doca %>% 
  mutate(title_doca=tolower(title)) %>% 
  left_join(nyt_doca %>% 
              mutate(title_doca=tolower(title_doca)),
            by="title_doca") %>% 
  filter(!is.na(text)) %>% 
  select(title,title_doca,text,everything())

knitr::kable(data[1:5,1:2],cap="DoCA with NYT article")
```

Let us create a tidy dataset, keeping text, title_doca, eventid,event year, violence, participant size, what, purpose,whysm.
here is the [codebook](https://web.stanford.edu/group/collectiveaction/codebook_stats.xls)

```{r}
tidy_data <- data %>% 
  select(eventid,evyy,title_doca,text,what,purpose,whysm,particex,viold)
```

Let us do some cleaning about main news articles.

```{r}
# let us remove puncts, white spaces like \n \t etc
tidy_data <- tidy_data %>% 
  mutate(tidy_text=tolower(text) %>% 
           str_replace_all("[:punct:]|\\s|\\d"," ") %>% 
           str_replace_all("^.*?proquest historical newspapers the new york times|.{100}$","") %>% 
           str_squish %>% 
           str_trim
         )
```


## Understanding ggplot2

> If we need to be explicit about where a function (or dataset) comes from, we’ll use the special form package::function(). For example, ggplot2::ggplot() tells you explicitly that we’re using the ggplot() function from the ggplot2 package.

> We have to admit that ggplot2 is the most popular R graph package in social science community. If you don't know about it, you should read [R for Data Science](https://r4ds.had.co.nz/); [ggplot2 cookbook](http://www.cookbook-r.com/Graphs/). 

Let us load the package if you did not load it before.

```{r}
library(ggplot2)
```

> With ggplot2, you begin a plot with the function ggplot(). ggplot() creates a coordinate system that you can add layers to. The first argument of ggplot() is the dataset to use in the graph. So ggplot(data = tidy_data) creates an empty graph.

> You complete your graph by adding one or more layers to ggplot(). The function geom_point() adds a layer of points to your plot, which creates a scatterplot. ggplot2 comes with many geom functions that each add a different type of layer to a plot. 

> Each geom function in ggplot2 takes a mapping argument. This defines how variables in your dataset are mapped to visual properties. The mapping argument is always paired with aes(), and the x and y arguments of aes() specify which variables to map to the x and y axes. ggplot2 looks for the mapped variables in the data argument, in this case, mpg.

```{r}
# let us say, we want to see the number of articles by year
# we need to compute the yearly number of articles first
# we pass tidy_data to ggplot and do a scatterplot
# we assign the ggplot object to variable P
tidy_data %>% 
  distinct(title_doca,.keep_all = T) %>% 
  filter(!is.na(evyy)) %>% 
  group_by(evyy) %>% 
  summarise(articles_n=n()) %>% 
  ggplot()+
  geom_point(aes(x=evyy,y=articles_n))->p
p
```

Obviously this is an ugly plot. Let us do some extra work to beautify it... let us change x and y axis title and add a caption

```{r}
p <- p +
  # Add titles, subtitles, caption, change x, y axis label
 labs(title = "Annual NYT Coverage of Protest in the U.S. 1960-1995",
      subtitle = "Based on a random sample (N=2000)",
      caption = "Data source: Dynamic of Collective Action and ProQuest",
      x = "Event Year",
      y = "Number of News Articles"
      )+
  # Format the title, subtitle, and caption
  theme(
    plot.title = element_text(
      color = "red", 
      size = 12, 
      face = "bold"
    ),
    plot.subtitle = element_text(color = "blue"),
    plot.caption = element_text(color = "blue", 
                              face = "italic"))+
  # relabel x and y axis
  scale_x_continuous(breaks=seq(1960,1995,5), limits=c(1960,1995))+
  scale_y_continuous(breaks=seq(0,100,10), limits=c(0,100))
  
p
```
I don't like the background. Let us change the theme to the classic

```{r}
# use black white theme, you can use different themes
p <-  p+
  theme_bw()
p
```



How about adding a smooth line?

```{r}
p <- p + 
  geom_smooth(aes(x=evyy,y=articles_n))
p
```

> This new plot contains the same x variable, the same y variable, and both line and points describe the same data. But they are not identical. They use a different visual object to represent the data. In ggplot2 syntax, we say that they use different geoms.

> A geom is the geometrical object that a plot uses to represent data. People often describe plots by the type of geom that the plot uses. For example, bar charts use bar geoms, line charts use line geoms, boxplots use boxplot geoms, and so on. Scatterplots break the trend; they use the point geom. As we see above, you can use different geoms to plot the same data. The plot on the left uses the point geom, and the plot on the right uses the smooth geom, a smooth line fitted to the data.

let us try a histogram plot.
```{r}
tidy_data %>% 
  distinct(title_doca,.keep_all = T) %>% 
  filter(!is.na(evyy)) %>% 
  ggplot()+
  geom_histogram(aes(x=evyy),binwidth = 0.5)+
  theme_classic()->p1
p1
```

Let us try a bar chart. 

```{r}
tidy_data %>% 
  distinct(title_doca,.keep_all = T) %>% 
  filter(!is.na(evyy)) %>% 
  ggplot()+
  geom_bar(aes(x=evyy),binwidth = 0.5)+
  theme_classic()->p2
p2
```

I am tired of graphing number of articles by year. Let us try purpose.
We only care about those with at least 2 articles

```{r}
p3 <- tidy_data %>% 
  mutate(purpose=tolower(purpose)) %>% 
  filter(purpose!="") %>% 
  group_by(purpose) %>% 
  summarise(purpose_n=n()) %>% 
  filter(purpose_n>2) %>% 
  ggplot(aes(x=purpose,y=purpose_n))+
  geom_bar(stat="identity")

p3  
```

Totally a mess. You can flip the x-y axis. Let us switch our x axis to y axis

```{r}
p3 <- p3 +
  coord_flip()
p3
```

**You can check the ggplot cookbook for more details**

## Understanding ggplot2 supplement

> There are a lot of ggplot2 related packages you can use to visualize your data. For instance, gganimate, ggnet2, gganimate, ggdendro, ggthemes, ggpubr, Plotly, patchwork, ggridges,ggmap,ggrepel,ggradar,ggcorrplot,GGally

let us install them all. You should spend some time to explore these packages. If some of these packages you cannot install. You should try devtools::install_github()

```{r}
ggpackages <-  c("gganimate", "ggnet2", "gganimate", "ggdendro", "ggthemes", "ggpubr", "plotly", "patchwork","ggridges","ggmap","ggrepel","ggradar","ggcorrplot","GGally")
p_load(ggpackages,character.only = T)
#devtools::install_github("ropensci/plotly")
#devtools::install_github("briatte/ggnet")
```

Let us try plotly
```{r}
p4 <- ggplotly(p)
p4
```

# TextViz Basics in R

## We use TidyText with ggpplot2 to do some basic textvizs

> tidytext::unnest_tokens provides us with a function to tokenize words: unnest_tokens(
  tbl,
  output,
  input,
  token = "words",
  format = c("text", "man", "latex", "html", "xml"),
  to_lower = TRUE,
  drop = TRUE,
  collapse = NULL,
  ...
)


```{r}
library(tidytext)
library(SnowballC)
# we need to process text data
token_data <- tidy_data %>% 
  # create a unique id
  mutate(doca_id=row_number()) %>% 
  # Let us tokenize tidy_data text field
  unnest_tokens(output = word,input = tidy_text,token="words") %>%
  # get rid of stop words
  anti_join(tidytext::get_stopwords("en",source="snowball")) %>% 
  # let us do some stemming
  mutate(word_stem = wordStem(word)) %>% 
  filter(word_stem!="")

```


```{r}
# let us count the word
token_data %>%
    count(word_stem, sort = TRUE)
```


let us make a document-term matrix first

```{r}
dtm_data <- token_data %>%
    count(doca_id, word_stem, sort = TRUE) %>%
  cast_dtm(doca_id, word_stem, n)
dtm_data
```

What are the highest tf-idf words in our documents? Let us plot them

```{r}
tfidf_data <- token_data %>%
    count(doca_id, word_stem, sort = TRUE) %>%
    bind_tf_idf(word_stem, doca_id, n) %>%
    arrange(-tf_idf) %>%
    group_by(doca_id) %>%
    top_n(10) %>%
    ungroup 

knitr::kable(tfidf_data[1:10,],cap="DoCA with NYT article, TF-IDF")
```


## Replicate fighting words article metrics

```{r, fig.align='center', fig.cap='fighting words'}
knitr::include_graphics('fig1.png')
```

Let us see whether violence influences media coverage of protest...
The goal here is to compare the words in two corpora: news articles with violence in protest and news articles without violence. We are trying to see what words are more likely to associate with violence and nonviolence...

We use the following formula:

$$f_{kw}^{(V)}-f_{kw}^{(NV)}$$
where
$$f_{kw}^{(V)}=y_{kw}^{(V)}/n_k^{(V)}$$
```{r}
# first we need to compute these two metrics for each word in two corpora
metric_data <- token_data %>% 
  count(word_stem, name="tot_count",sort = TRUE) %>% 
  left_join(token_data %>% 
              filter(!is.na(viold)) %>% 
              mutate(viold=ifelse(viold==1,"V","NV")) %>% 
              count(viold,word_stem) %>% 
              pivot_wider(names_from=viold,values_from=n,values_fil=0),
            by="word_stem") %>% 
  mutate(
    fv=V/tot_count,
    fnv=NV/tot_count,
    fv_fnv=fv-fnv,
    weight=abs(fv_fnv)
    ) %>% 
    # drop top common words or rare words
  filter(tot_count<1500,tot_count>50)

knitr::kable(metric_data[1:10,],cap="DoCA with NYT article, Volenceor not")  
```

let us get top 10 violent words and top nonviolent words

```{r}
top50_words <- metric_data %>% 
  top_n(10,fv_fnv) %>% 
  bind_rows(metric_data %>% 
  top_n(-10,fv_fnv))
```

let us replicate the graph

```{r}
metric_data %>% 
  filter(!is.na(fv_fnv)) %>% 
  ggplot(aes(x=tot_count,
             y=fv_fnv))+
  geom_point()+
  theme_bw()+
  theme(legend.position = "none")
  
```


```{r}
metric_data %>% 
  filter(!is.na(fv_fnv)) %>% 
  left_join(top50_words %>% transmute(word_stem,top50_words=word_stem),by="word_stem") %>% 
  ggplot(aes(x=tot_count,
             y=fv_fnv))+
  geom_point()+
  ggrepel::geom_label_repel(aes(label=top50_words))+
  theme_bw()+
  theme(legend.position = "none")
  
```

# Lab 5 Problem Set

We will continue to use NYT doca 2000 news articles as our dataset for text viz. You need to train a topic model using stm.

Then you should visualize the topics using some extra packags we provided in stm lab tutorial.

Send me a screen-shot before Tuesday 6 PM.

# **The End**
