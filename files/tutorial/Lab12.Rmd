---
title: "Lexicon-Based Sentiment Analysis"
author: "Yongjun Zhang"
date: ""
output:
  rmdformats::readthedown:
    highlight: pygments
--- = =
---

```{=html}
<style type="text/css">
p{ /* Normal  */
   font-size: 18px;
}
body{ /* Normal  */
   font-size: 18px;
}
td {  /* Table  */
   font-size: 14px;
}
h1 { /* Header 1 */
 font-size: 32px;
}
h2 { /* Header 2 */
 font-size: 26px;
}
h3 { /* Header 3 */
 font-size: 22px;
}
code.r{ /* Code block */
  font-size: 14px;
}
pre { /* Code block */
  font-size: 14px
}
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Learning Objects

This tutorial aims to introduce basic lexicon-based sentiment analysis in R.


## We need to load some packages for use

```{r}
if (!requireNamespace("pacman"))
  install.packages('pacman')
library(pacman)
packages<-c("tidyverse",
            "sentimentr",
            "lexicon",
            "syuzhet",
            "parallel",
            "magrittr")
p_load(packages,character.only = TRUE)
```

## Load sample data on earnings call transcripts

R tidyverse package provides a series of useful data wrangling tools. You can 
check it here <https://www.tidyverse.org/>. 


```{r}
# load data
# load(url("https://yongjunzhang.com/files/tutorial/ect_sample.RData"))
# you can also download the data and then load it
load("ect_sample.RData")
ect_sample <- ect_sample %>% 
  filter(!is.na(tic))
```

```{r}
# Show a table for visual check
knitr::kable(ect_sample[1:10,],cap="Earnings call parsed data")
```

## R syuzhet Package

Let us use syuzhet package as the baseline. Matthew Jockers created the syuzhet package that utilizes dictionary lookups for the Bing, NRC, and Afinn methods as well as a custom dictionary. He also utilizes a wrapper for the Stanford coreNLP which uses much more sophisticated analysis. 

You can check here for a tutorial for syuzhet: <https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html>

We are going to use its functions:

- `get_sentences()`:implements the openNLP sentence tokenizer

- `get_tokens`:tokenize by words instead of sentences

- `get_sentiment()`:includes two parameters--a character vector (of sentences or words) and a “method.” The method you select determines which of the four available sentiment extraction methods to employ. In the example that follows below, the “syuzhet” (default) method is called.Other methods include “bing”, “afinn”, “nrc”, and “stanford”. 

- `get_nrc_sentiment`: implements Saif Mohammad’s NRC Emotion lexicon. The NRC emotion lexicon is a list of words and their associations with eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive) (See <http://www.purl.org/net/NRCemotionlexicon>). 

- `get_percentage_values`:divides a text into an equal number of “chunks” and then calculates the mean sentiment valence for each.

- `simple_plot`:akes a sentiment vector and applies three smoothing methods. The smoothers include a moving average, loess, and discrete cosine transformation. 

```{r}
# Let use tokenize our texts first
my_example_text <- ect_sample$text
s_v <- get_sentences(my_example_text)
head(s_v)
```

```{r}
syuzhet_vector <- get_sentiment(s_v, method="syuzhet")
head(syuzhet_vector)
```
Let us try different methods (use different lexicons)

```{r}
bing_vector <- get_sentiment(s_v, method="bing")
head(bing_vector)
```

```{r}
afinn_vector <- get_sentiment(s_v, method="afinn")
head(afinn_vector)
```

```{r}
nrc_vector <- get_sentiment(s_v, method="nrc", lang = "english")
head(nrc_vector)
```

Let us compare the difference

```{r}
rbind(
  sign(head(syuzhet_vector)),
  sign(head(bing_vector)),
  sign(head(afinn_vector)),
  sign(head(nrc_vector))
)
```

Let us plot the result

```{r}
plot(
  syuzhet_vector, 
  type="h", 
  main="Example Plot Trajectory", 
  xlab = "Narrative Time", 
  ylab= "Emotional Valence"
  )
```

Let us see get_nrc_sentiment results

```{r}
nrc_data <- get_nrc_sentiment(s_v)
```

```{r}
angry_items <- which(nrc_data$anger > 0)
s_v[angry_items][1:10]
```

```{r}
joy_items <- which(nrc_data$joy > 0)
s_v[joy_items][1:10]
```

```{r}
valence <- (nrc_data[, 9]*-1) + nrc_data[, 10]
valence[1:10]
```

```{r}
barplot(
  sort(colSums(prop.table(nrc_data[, 1:8]))), 
  horiz = TRUE, 
  cex.names = 0.7, 
  las = 1, 
  main = "Emotions in Sample Earnings Call Transcripts", xlab="Percentage"
  )
```

You can also customize your lexicons

```{r}
my_text <- "I love when I see something beautiful.  I hate it when ugly feelings creep into my head."
char_v <- get_sentences(my_text)
method <- "custom"
custom_lexicon <- data.frame(word=c("love", "hate", "beautiful", "ugly"), value=c(1,-1,1, -1))
my_custom_values <- get_sentiment(char_v, method = method, lexicon = custom_lexicon)
my_custom_values
```


## Let Us Move to sentimentR package

Josh's comment: because the Syuzhet package does not account for negation and valence shifter. I typically use sentimentr as the default package for sentiment analysis in R. Note that sentimentr use some functions from Syuzhet, for instance, `get_sentences()`.

This tutorial is based on **Tyler Rinke**'s amazing one: <https://github.com/trinker/sentimentr>

-> sentimentr attempts to take into account valence shifters (i.e., negators, amplifiers (intensifiers), de-amplifiers (downtoners), and adversative conjunctions) while maintaining speed. Simply put, sentimentr is an augmented dictionary lookup. The next questions address why it matters.

-> A negator flips the sign of a polarized word (e.g., "I do not like it."). See lexicon::hash_valence_shifters[y==1] for examples. An amplifier (intensifier) increases the impact of a polarized word (e.g., "I really like it."). See lexicon::hash_valence_shifters[y==2] for examples. A de-amplifier (downtoner) reduces the impact of a polarized word (e.g., "I hardly like it."). See lexicon::hash_valence_shifters[y==3] for examples. An adversative conjunction overrules the previous clause containing a polarized word (e.g., "I like it but it's not worth it."). See lexicon::hash_valence_shifters[y==4] for examples.

-> Well valence shifters affect the polarized words. In the case of negators and adversative conjunctions the entire sentiment of the clause may be reversed or overruled. So if valence shifters occur fairly frequently a simple dictionary lookup may not be modeling the sentiment appropriately. You may be wondering how frequently these valence shifters co-occur with polarized words, potentially changing, or even reversing and overruling the clause's sentiment. The table below shows the rate of sentence level co-occurrence of valence shifters with polarized words across a few types of texts.


### Functions

There are two main functions (top 2 in table below) in **sentimentr** with several helper functions summarized in the table below:


| Function         |  Description                                          |
|------------------|-------------------------------------------------------|
| `sentiment`      | Sentiment at the sentence level                       |
| `sentiment_by`   | Aggregated sentiment by group(s)                      |
| `profanity`      | Profanity at the sentence level                       |
| `profanity_by`   | Aggregated profanity by group(s)                      |
| `emotion`        | Emotion at the sentence level                         |
| `emotion_by`     | Aggregated emotion by group(s)                        |
| `uncombine`      | Extract sentence level sentiment from  `sentiment_by` |
| `get_sentences`  | Regex based string to sentence parser (or get sentences from `sentiment`/`sentiment_by`)|
| `replace_emoji`           | repalcement | Replace emojis with word equivalent or unique identifier |
| `replace_emoticon`| Replace emoticons with word equivalent               |
| `replace_grade`  | Replace grades (e.g., "A+") with word equivalent     |
| `replace_internet_slang`  | replacment  | Replace Internet slang with word equivalents |
| `replace_rating` | Replace ratings (e.g., "10 out of 10", "3 stars") with word equivalent |
| `as_key`         | Coerce a `data.frame` lexicon to a polarity hash key  |
| `is_key`         | Check if an object is a hash key                      |
| `update_key`     | Add/remove terms to/from a hash key                   |
| `highlight`      | Highlight positive/negative sentences as an HTML document |
| `general_rescale` | Generalized rescaling function to rescale sentiment scoring |
| `sentiment_attribute` | Extract the sentiment based attributes from a text |
| `validate_sentiment` | Validate sentiment score sign against known results |


### The Equation 

The equation below describes the augmented dictionary method of **sentimentr** that may give better results than a simple lookup dictionary approach that does not consider valence shifters.  The equation used by the algorithm to assign value to polarity of each sentence fist utilizes a sentiment dictionary (e.g., Jockers, [(2017)](https://github.com/mjockers/syuzhet)) to tag polarized words.  Each paragraph ($p_i = \{s_1, s_2, ..., s_n\}$) composed of sentences, is broken into element sentences ($s_i,j = \{w_1, w_2, ..., w_n\}$) where $w$ are the words within sentences.  Each sentence ($s_j$) is broken into a an ordered bag of words.  Punctuation is removed with the exception of pause punctuations (commas, colons, semicolons) which are considered a word within the sentence.  I will denote pause words as $cw$ (comma words) for convenience.  We can represent these words as an i,j,k notation as $w_{i,j,k}$.  For example $w_{3,2,5}$ would be the fifth word of the second sentence of the third paragraph.  While I use the term paragraph this merely represent a complete turn of talk.  For example it may be a cell level response in a questionnaire composed of sentences.

The words in each sentence ($w_{i,j,k}$) are searched and compared to a dictionary of polarized words (e.g., a combined and augmented version of Jocker's (2017) [originally exported by the [**syuzhet**](https://github.com/mjockers/syuzhet) package] & Rinker's augmented Hu & Liu (2004) dictionaries in the [**lexicon**](https://cran.r-project.org/package=lexicon) package). Positive ($w_{i,j,k}^{+}$) and negative ($w_{i,j,k}^{-}$) words are tagged with a $+1$ and $-1$ respectively (or other positive/negative weighting if the user provides the sentiment dictionary).  I will denote polarized words as $pw$ for convenience. These will form a polar cluster ($c_{i,j,l}$) which is a subset of the a sentence ($c_{i,j,l} \subseteq s_i,j$).

The polarized context cluster ($c_{i,j,l}$) of words is pulled from around the polarized word ($pw$) and defaults to 4 words before and two words after $pw$ to be considered as valence shifters.  The cluster can be represented as ($c_{i,j,l} = \{pw_{i,j,k - nb}, ..., pw_{i,j,k} , ..., pw_{i,j,k - na}\}$), where $nb$ & $na$ are the parameters `n.before` and `n.after` set by the user.  The words in this polarized context cluster are tagged as neutral ($w_{i,j,k}^{0}$), negator ($w_{i,j,k}^{n}$),
amplifier [intensifier] ($w_{i,j,k}^{a}$), or de-amplifier [downtoner] ($w_{i,j,k}^{d}$). Neutral words hold no value in the equation but do affect word count ($n$).  Each polarized word is then weighted ($w$) based on the weights from the `polarity_dt` argument and then further weighted by the function and number of the valence shifters directly surrounding the positive or negative word ($pw$).  Pause ($cw$) locations (punctuation that denotes a pause including commas, colons, and semicolons) are indexed and considered in calculating the upper and lower bounds in the polarized context cluster. This is because these marks indicate a change in thought and words prior are not necessarily connected with words after these punctuation marks.  The lower bound of the polarized context cluster is constrained to $\max \{pw_{i,j,k - nb}, 1, \max \{cw_{i,j,k} < pw_{i,j,k}\}\}$ and the upper bound is constrained to $\min \{pw_{i,j,k + na}, w_{i,jn}, \min \{cw_{i,j,k} > pw_{i,j,k}\}\}$ where $w_{i,jn}$ is the number of words in the sentence.

The core value in the cluster, the polarized word is acted upon by valence shifters. Amplifiers increase the polarity by 1.8 (.8 is the default weight ($z$)).  Amplifiers ($w_{i,j,k}^{a}$) become de-amplifiers if the context cluster contains an odd number of negators ($w_{i,j,k}^{n}$).  De-amplifiers work to decrease the polarity.  Negation ($w_{i,j,k}^{n}$) acts on amplifiers/de-amplifiers as discussed but also flip the sign of the polarized word.  Negation is determined by raising $-1$ to the power of the number of negators ($w_{i,j,k}^{n}$) plus $2$.  Simply, this is a result of a belief that two negatives equal a positive, 3 negatives a negative, and so on.

The adversative conjunctions (i.e., 'but', 'however', and 'although') also weight the context cluster.  An adversative conjunction before the polarized word ($w_{adversative\,conjunction}, ..., w_{i, j, k}^{p}$) up-weights the cluster by 1 + $z_2 * \{|w_{adversative\,conjunction}|, ..., w_{i, j, k}^{p}\}$ (.85 is the default weight ($z_2$) where $|w_{adversative\,conjunction}|$ are the number of adversative conjunctions before the polarized word).  An adversative conjunction after the polarized word down-weights the cluster by 1 + $\{w_{i, j, k}^{p}, ..., |w_{adversative\,conjunction}| * -1\} * z_2$.  This corresponds to the belief that an adversative conjunction makes the next clause of greater values while lowering the value placed on the prior clause.

The researcher may provide a weight ($z$) to be utilized with amplifiers/de-amplifiers (default is $.8$; de-amplifier weight is constrained to $-1$ lower bound).  Last, these weighted context clusters ($c_{i,j,l}$) are summed ($c'_{i,j}$) and divided by the square root of the word count (&radic;$w_{i,jn}$) yielding an **unbounded polarity score** ($\delta_{i,j}$) for each sentence.


$\delta$<sub>*i**j*</sub> = <em>c</em>'<sub>*i**j*</sub>/&radic;*w*<sub>*i**j**n*</sub>


Where:

$$c'_{i,j} = \sum{((1 + w_{amp} + w_{deamp})\cdot w_{i,j,k}^{p}(-1)^{2 + w_{neg}})}$$

$$w_{amp} = \sum{(w_{neg}\cdot (z \cdot w_{i,j,k}^{a}))}$$

$$w_{deamp} = \max(w_{deamp'}, -1)$$

$$w_{deamp'} = \sum{(z(- w_{neg}\cdot w_{i,j,k}^{a} + w_{i,j,k}^{d}))}$$



$$w_{b} = 1 + z_2 * w_{b'}$$

$$w_{b'} = \sum{(|w_{adversative\,conjunction}|, ..., w_{i, j, k}^{p}, w_{i, j, k}^{p}, ..., |w_{adversative\,conjunction}| * -1})$$

$w_{neg}$ = ($\sum{w_{i,j,k}^{n}}$ ) mod 2

To get the mean of all sentences ($s_{i,j}$) within a paragraph/turn of talk ($p_{i}$) simply take the average sentiment score $p_{i,\delta_{i,j}}$ = 1/n $\cdot$ $\sum$ $\delta_{i,j}$ or use an available weighted average (the default `average_weighted_mixed_sentiment` which upweights the negative values in a vector while also downweighting the zeros in a vector or `average_downweighted_zero` which simply downweights the zero polarity scores).

### Installation

```r
if (!require("pacman")) install.packages("pacman")
pacman::p_load_current_gh("trinker/lexicon", "trinker/sentimentr")
```

### Examples

```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(sentimentr, dplyr, magrittr)
```

## Preferred Workflow
Here is a basic `sentiment` demo.  Notice that the first thing you should do is to split your text data into sentences (a process called sentence boundary disambiguation) via the `get_sentences` function.  This can be handled within `sentiment` (i.e., you can pass a raw character vector) but it slows the function down and should be done one time rather than every time the function is called.  Additionally, a warning will be thrown if a larger raw character vector is passed.  The preferred workflow is to spit the text into sentences with `get_sentences` before any sentiment analysis is done.  


```{r}
mytext <- c(
    'do you like it?  But I hate really bad dogs',
    'I am the best friend.',
    'Do you really like it?  I\'m not a fan'
)
mytext <- get_sentences(mytext)
sentiment(mytext)
```

To aggregate by element (column cell or vector element) use `sentiment_by` with `by = NULL`.

```{r}
mytext <- c(
    'do you like it?  But I hate really bad dogs',
    'I am the best friend.',
    'Do you really like it?  I\'m not a fan'
)
mytext <- get_sentences(mytext)
sentiment_by(mytext)
```

To aggregate by grouping variables use `sentiment_by` using the `by` argument.

```{r}
out <- with(
    ect_sample, 
    sentiment_by(
        get_sentences(text), 
        list(tic)
    )
)
head(out)
```

### Plotting at Aggregated Sentiment

```{r, warning=FALSE}
# let us plot random 20 companies
out_20 <- sample_n(out,20)
plot(out_20)
```

### You can Use the Custimized Dictionary as well

Because earnings call transcripts relate to financial domain, we use financial specific dictionary.

```{r}
out <- with(
    ect_sample, 
    sentiment_by(get_sentences(text),
                 polarity_dt=lexicon::hash_sentiment_loughran_mcdonald,
                 list(tic)) 
    )

plot(sample_n(out,30))
```

### Plotting at the Sentence Level

The `plot` method for the class `sentiment` uses **syuzhet**'s `get_transformed_values` combined with **ggplot2** to make a reasonable, smoothed plot for the duration of the text based on percentage, allowing for comparison between plots of different texts.  This plot gives the overall shape of the text's sentiment.  The user can see `syuzhet::get_transformed_values` for more details.

```{r}
plot(uncombine(sample_n(out,30)))
```


### THE END...