# -*- coding: utf-8 -*-
"""Lab2a.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yQ7vC9XYwn6I-NdbWWL54JUNGIJGHTky

# LAB 2: BASIC INTRO TO PYTHON AND WEB-SCRAPING

## Part 1. Basic Intro to Python
We will use Python to collect data, but use R to preprocess data. in the last Tutorial, we covered how to install Python/Spyder.

**If you system does not install pip, you can check here for more details:https://pip.pypa.io/en/stable/installing/**

Similarly, you can install modules like pandas, numpy, sci-learn, selenium,beautifulsoup, tensorflow, pytorch, keras, etc.

We are not going to cover those Python Basics today. You can read **NLP with Python** Chapters1-3 for more details <https://www.nltk.org/book/>. Next Week, we will briefly cover those basics when we learn topic modeling. 

Today, we only introduce some basics related to webscraping.

### 1. Play with your working directory and then open and save files
"""

# get current working directory
import os
path=os.getcwd()
print(path)

# change the current working directory to soc591
os.chdir(new_path)
# check the current wd
print("Current wd: ",os.getcwd())

# create a new folder for our course
new_path="./soc591/"
os.makedirs(new_path)

# let us create a new file in current WD, write some texts into the file, and then close it
f = open("soc591.txt",mode="w+")
col_vars = "id;text\n"
f.write(col_vars)
f.write("1;This is a demo for writing some texts\n")
f.close()

# Let us read the soc591.txt file and assign it to variable text_df
text_df = open("soc591.txt", "r").read()
print(text_df)

# list file content
os.listdir(".")

# Let us remove the soc591.txt file
os.remove("soc591.txt")

"""## 2. Play with Regular Expression
In this module, we learn about the basic skills to understand, recognize, and write your own python regular expression codes.

1. What is regular expression?
2. Why should we learn RegEx?
3. How to write your own RegEx?
If you're interested in text analysis or broad NLP, RegEX would be one element of your necessary toolkit. 

### What is REGULAR EXPRESSION?
Regular expressions, or RegEXes, are an abstract way to express patterns that match strings. It is widely used in natural language processing, for instance, to find and replace certain strings with certain patterns.

One simple example is, if you are a data scientist, to create a variable that contains all email address from a large amount of texual data. You can use certain email name pattern to capture all potential email addresses.
"""

import re
# re module has a lot of useful function allowing users to search, match, locate,
# and replace certain pattern in a string.

# Here is an email example
email_text = "Josh Zhang's email is example@email.com"
# RegEx for email address
email_regex = "[a-z]{7}@.*$"
# extract the address
email_search = re.search(email_regex,email_text)
if email_search:
  print("Email gets a match: ",email_search)
else:
  print("OOPS...SOMETHING IS WRONG")

"""Here, we used re.search() function to search pattern within the email_text. The method returns a match object if the search is successful. You can access the matched text like email_search[0]

### Why should we use or learn RegEX?
If you a data scientist or quantitative scholar, most of your time is to construct some databases based on a large amount of raw data. Obviously, it is not plausible to do manual coding. Even if you could, it would be very expensive and time costly in the big data era. To improve our efficiency, it is better for us to become a master of RegEX.

There are couple of ways to generate RegEX automatically using third party algorithms. For instance, you can use RegEx tester tools such as regex101<https://regex101.com/>. 

But before doing that, you have to be able to understand the meaning of the RegEX pattern.

### How to write your own RegEX?
There are certain basic RegEXes we need to familier with, including anchors, quantifiers, operators, character classes,boundaries,flags, groupings,back-referencing, look forward and backward,etc.We will go through these topics one by one.

#### Let us first see some Sepcial Characters
Regular expressions can contain both special and ordinary characters. 

Most ordinary characters, like 'A', 'a', or '0', are the simplest regular expressions; they simply match themselves. 

Special characters are characters that are interpreted in a special way by a RegEx engine. 

The special characters are:

**[] . ^ $ * + ? {} () \ |**

##### [ ] - Square brackets

Square brackets specifies a set of characters you want to match.

For instance, [ABCabc] matches if the string contains any of the A, B, C, a, b, or c.

You can also specify a range of characters using - inside square brackets.

> [A-Ca-c] is the same as [ABCabc].

> [1-9] is the same as [123456789].

> [0-38] is the same as [01238].

You can use caret ^ symbol at the start of a square-bracket to exclude certain characters (equate to saying "NOT SOME CHARS").

> [^ABCabc] : any character except A OR B OR C OR a OR b OR c.

> [^0-9] : any non-digit character.

##### .- Dot

The dot symbol . matches any single character (except newline '\n').

##### ^ - Caret

The caret symbol ^ checks if a string starts with a certain character.

##### $ - Dollar

The dollar symbol $ is used to check if a string ends with a certain character.

##### \* - Star

The star symbol * matches zero or more occurrences of the pattern left to it.

##### \+ - Plus

The plus symbol + matches one or more occurrences of the pattern left to it.

##### ? - Question Mark

The question mark symbol ? matches zero or one occurrence of the pattern left to it

##### {} - Braces

Consider this code: {n,m}. This means at least n, and at most m repetitions of the pattern left to it.

##### | - Alternation

Vertical bar | is used for alternation (or operator).

##### () - Group

Parentheses () is used to group sub-patterns. For example, (a|b|c)xz match any string that matches either a or b or c followed by xz

##### \ - Backslash

Backlash \ is used to escape various characters including all metacharacters. For example,

\$a match if a string contains \$ followed by a. Here, \$ is not interpreted by a RegEx engine in a special way.

If you are unsure if a character has special meaning or not, you can put \ in front of it. This makes sure the character is not treated in a special way.

### let check some special sequence
Special sequences make commonly used patterns easier to write. Here's a list of special sequences:

\A - Matches if the specified characters are at the start of a string.

\b - Matches if the specified characters are at the beginning or end of a word.

\B - Opposite of \b. Matches if the specified characters are not at the beginning or end of a word.

\d - Matches any decimal digit. Equivalent to [0-9]

\D - Matches any non-decimal digit. Equivalent to [^0-9]

\s - Matches where a string contains any whitespace character. Equivalent to [ \t\n\r\f\v].

\S - Matches where a string contains any non-whitespace character. Equivalent to [^ \t\n\r\f\v].

\w - Matches any alphanumeric character (digits and alphabets). Equivalent to [a-zA-Z0-9_]. By the way, underscore _ is also considered an alphanumeric character.

\W - Matches any non-alphanumeric character. Equivalent to [^a-zA-Z0-9_]

\Z - Matches if the specified characters are at the end of a string.

#### **Anchors: ^, $, \b, \B**

> All the patterns youe’ve seen so far will find a match anywhere within a string, which is usually - but not always - what you want. This is the purpose of an anchor; to make sure that you are at a certain boundary before you continue the match. 

> The caret symbol ^ matches the beginning of a line, and the dollar sign \$ matches the end of a line. 

> The other two anchors are \b and \B, which stand for a “word boundary” and “non-word boundary”. Here is an exmaple:

> > "\bame" will capture "i am american", but not "i am an blamer".

> > "\bnational" will capture "national flag is abcde" but not "internatonal organizations are abbbb".

#### **Quantifiers: {}, +, *, ?

> These symbols are quantifiers used in regEX to indicate the repetition or the frequency of certain characters 

> > {a,b} a times at least, b times at most

> > \+ at least once

> > \* zero times or more

> > ? zero times or once

#### **Greedy Match or Lazy Match**

We use * or ? to do greedy (longest search) or lazy match (shortest search).

> > For instance, pattern "i.*nat" will capture i am a national representative in an internat" in the string "i am a national representative in an international organization".

>> But "i.*?nat" will only capture "i am a nat".

**Let us do a quick test**

r"(\d|[a-c]){2}.\*? \bame.\*$"

### Let us check python re module's fuctions that we often use

If you want to learn more python regEX, you can visit this link: <https://docs.python.org/3/library/re.html>. 

Here we only cover several widely used ones:

**re.findall(pattern, string, flags=0)**

> This methods returns all non-overlapping matches of pattern in string, as a list of strings. 

> The string is scanned left-to-right, and matches are returned in the order found. So we often use this function to retrieve data.
"""

import re 
# A sample text string where regular expression is searched. 
string = """STONY BROOK UNIVERSITY ZIPCODE IS 11794, 
            MY ID IS 12398774, YOU ID IS 8966AGVGG"""
# A sample regular expression to find digits. 
pattern = '\d+'			
match = re.findall(pattern, string) 
print(match)

"""**re.match(pattern, string, flags=0)**

> If zero or more characters at the beginning of string match the regular expression pattern, return a corresponding match object. 

> Return None if the string does not match the pattern; note that this is different from a zero-length match.

> Note that even in MULTILINE mode, re.match() will only match at the beginning of the string and not at the beginning of each line.
"""

import re

pattern =  r"^\d+"
string = """11794 is a valid stony brook address.\n 
            this is another test\n 
            889997 is my phone number."""
result = re.match(pattern,string,re.M)
print(result)
print(result.group(0))

# Note that even in MULTILINE mode, 
# re.match() will only match at the beginning of the string 
# and not at the beginning of each line.
result_m = re.match(pattern,string,re.M)
print(result_m)
print(result_m.group(0))

"""**re.search(pattern, string, flags=0)**

> Scan through string looking for the first location where the regular expression pattern produces a match, and return a corresponding match object. 

> Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string.
"""

import re

pattern = r"^\d+"
string = """11794 is a valid stony brook address.\n 
            this is another test\n 
            889997 is my phone number."""
result=re.search(pattern,string)
print(result)

"""**re.sub(pattern, repl, string, count=0, flags=0)**

> Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl. If the pattern isn’t found, string is returned unchanged. 

> repl can be a string or a function; if it is a string, any backslash escapes in it are processed. That is, \n is converted to a single newline character, \r is converted to a carriage return, and so forth. Unknown escapes of ASCII letters are reserved for future use and treated as errors. Other unknown escapes such as \& are left alone. Backreferences, such as \6, are replaced with the substring matched by group 6 in the pattern.
"""

import re
# replace all digits to DIGITS
pattern = r"\d+"
repl = "DIGITS"
string = """11794 is a valid stony brook address.\n 
            this is another test\n 
            889997 is my phone number."""
result = re.sub(pattern,repl,string)
print(result)

"""**re.split(pattern, string, maxsplit=0, flags=0)**

> Split string by the occurrences of pattern. 

> If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. 

> If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list.
"""

print(re.split(r'\W+', 'Words, words, words.'))

print(re.split(r'(\W+)', 'Words, words, words.'))

print(re.split(r'\W+', 'Words, words, words.', 1))

print(re.split('[a-f]+', '0a3B9', flags=re.IGNORECASE))

"""#### regEX is especially useful when you are using lexicon methods to create variables from large texual data. Let us go through some examples

> I was working on a project focusing on social movement organizations testifying before U.S. Congress. We need to detect whether certain SMO appears in some hearings. For instance, we need to make whether afl-cio was mentioned in some hearings. So we could specify a regEX to capture afl-cio.

## Part 2: Using Webscraping to Collect Text Data

### 1. Basic info on webpages

Web-scraping is automating the ways how we gather data from websites. It is more efficient, but imposing burdens on servers. That is why a lot of websites develop anti-robot measures to prevent automate data gathering. You should always check robots.txt from the target website to see whether it allows you to scrape. 

For most of the time, we can scrape those government websites because they disclose massive data for the public such as FEC and SEC websites. 

Some of these websites are straightfoward. They are static. You can go their webpage and scrape all their stuff easily.
"""

# Here is an example of static html page
<!DOCTYPE html>
<html>
<head>
<title>Page Title</title>
</head>
<body>

<h1>This is a Heading</h1>
<p>This is a paragraph.</p>

</body>
</html>

"""But sometimes we have dynamic and interactive websites built on JavaScript, PHP, etc. For instance, a lot of data visualizaton websites, you have to click on something, then the website will return some results. 

One solution to this is to use Selenium to simulate browser behavior.

### 2. Scraping Static Webpages
"""

import pandas as pd
from bs4 import BeautifulSoup as bs
import urllib.parse
import urllib.request

def get_crp_industry_list(url):
  ''' Access Opensecrets.org website and return industry names and ids.
  '''
  # Specify userheader
  userHeader = {"User-Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/600.7.12 (KHTML, like Gecko) Version/8.0.7 Safari/600.7.12"}
  req = urllib.request.Request(url, headers=userHeader)
  # open url and read web page
  response = urllib.request.urlopen(req)
  the_page = response.read()
  # beautifulsoup parse html
  soup=bs(the_page,"html.parser")
  #print(soup)
  # get all industry links and names
  indList=soup.find("div",{"id":"rightColumn"}).find_all("a")
  #print(indList)
  # clean raw data
  indLinks = []
  indNames = []
  for link in indList[1:]:
      indLinks.append(link['href'].replace("indus.php?ind=",""))
      indNames.append(link.contents[0].strip())
  #print(indLinks,indNames)
  # create a dataset
  indDF = pd.DataFrame({"indLinks":indLinks,"indNames":indNames})
  return indDF

url="https://www.opensecrets.org/industries/slist.php"
data_ind_list=get_crp_industry_list(url)
print(data_ind_list)

"""### 3. Using Selenium to Scrape Dynamic and Interactive Websites

So what is selenium? Selenium is an open-source web-based automation tool used for testing in the industry. But it can also used for wescraping, or "crawling/spiderig". 

Selenium can control your web browser and automate your browsing behavior. We will use Google chrome driver, but you can also other drivers like firefox,IE,etc.

#### First we need to install necessary modules for webscraping via terminal or commind line
"""

#!pip install selenium
!pip install webdriver_manager
!pip install bs4

"""#### The Goal here is to scrape Black Lives Matter Data from https://elephrame.com/

This website tracks the occurence of BLM protests. We cannot use the previous way to directly get the data because it is dynamic and you have to scroll down or click next page to get more data.

Let us build our cralwer from scratches...

1. We need to install webdriver to control our browser
2. We need to use our webdriver to control brower to establish a connection with the target website (sometimes you have to do someth log-in stuff, send pwd, etc.)
3. We need to check the target webpage to locate the info we need
4. Scrape the target info, open a file on local computer or your server, and save that info.
5. We click the next page and repeat the scraping process until the end
5. Close your webdriver
6. We encapsulate the whole process (def a function or class to automate the whole process)
"""

# Import modules for use
import os
import selenium
from selenium import webdriver
import time
import requests
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import ElementClickInterceptedException
from bs4 import BeautifulSoup as bs

# Install Driver
driver = webdriver.Chrome(ChromeDriverManager().install())

# Open the url and establish a connection
url = "https://elephrame.com/textbook/BLM/chart"
driver.implicitly_wait(5)
driver.maximize_window()
driver.get(url)

# Scroll down to the bottom of the page
#driver.execute_script("window.scrollTo(0,window.scrollY+300)")
driver.execute_script("window.scrollTo(0,document.body.scrollHeight)")

# Read and parse the first page
first_page = driver.page_source
soup = bs(first_page,"html.parser")

# Use google developer inspect to check the source codes
# locate the key info we need
# it stores ad div class = "item chart"
items = soup.findAll("div",{"class":"item chart"})
print(items)

# find necessay elements, including id, item-protest-location,protest-start,protest-end,item-protest-subject
# item-protest-participants (li), item-protest-time,item-protest-description, item-protest-url
import re
for item in items:
    try:
        id=re.findall(r'id="([0-9].*?)"',str(item))[0]
        print(id)
    except:
        id=NA
    try:
        protest_location=' '.join(item.find("div",{"class":"item-protest-location"}).text.split())
        print(protest_location)
    except:
        protest_location=""
    try:
        protest_start=' '.join(item.find("div",{"class":"protest-start"}).text.split())
        print(protest_start)
    except:
        protest_start=""
    try:
        protest_end=' '.join(item.find("div",{"class":"protest-end"}).text.split())
        print(protest_end)
    except:
        protest_end=""
    try:
        protest_subject=' '.join(item.find("div",{"class":"item-protest-subject"}).text.split())
        print(protest_subject)
    except:
        protest_subject=""
    try:
        protest_participants=' '.join(item.find("li",{"class":"item-protest-participants"}).text.split())
        print(protest_participants)
    except:
        protest_participants=""
    try:
        protest_time=' '.join(item.find("li",{"class":"item-protest-time"}).text.split())
        print(protest_time)
    except:
        protest_time=""
    try:
        protest_description=' '.join(item.find("li",{"class":"item-protest-description"}).text.split())
        print(protest_description)
    except:
        protest_description=""
    try:
        protest_urls='##'.join(item.find("li",{"class":"item-protest-url"}).text.split())
        print(protest_urls,"\n")
    except:
        protest_urls=""

# save the last item content into a tsv file for check
# check current dir
os.getcwd()
#os.chdir()
import csv 
with open('blm-data.tsv','w+') as f:
    tsv_writer = csv.writer(f, delimiter='\t')
    # write column names
    var_names=["protest_id", "protest_location","protest_start","protest_end","protest_subject","protest_participants", 
            "protest_time","protest_description", "protest_urls"]
    tsv_writer.writerow(var_names)
    # write actual data
    data=[protest_id, protest_location,protest_start,protest_end,protest_subject,protest_participants, 
            protest_time,protest_description, protest_urls]
    tsv_writer.writerow(data)

# Open tsv file for manual check
!open blm-data.tsv

# click the next page
# you can check here for more info on selenium how to locate elements 
# https://selenium-python.readthedocs.io/locating-elements.html
import time
from selenium.webdriver.common.by import By
next_page = driver.find_element(By.XPATH, '//*[@id="blm-results"]/div[3]/ul/li[4]')
next_page.click()
time.sleep(5)
# then we repeat the process to the end

# Because we have 229 pages, so we need a loop to automate the process
soup = bs(driver.page_source,"html.parser")
# locate the page id
page_id = soup.find("input",{"class":"page-choice"})["value"]
page_id = int(page_id)
print(page_id)
'''
while page_id <=229:
    # do first page scraping 
    # click next page
    # repeat the scraping
    # if page_id>229, then stop
'''

"""#### So we need to put the whole process into a couple of functions and automate the whole scraping process."""

# let us encapsulate the whole process
import os,re,csv
import selenium
from selenium import webdriver
import time
import requests
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import ElementClickInterceptedException
from bs4 import BeautifulSoup as bs
from selenium.webdriver.common.by import By

# Let us define a couple of functions for ease of writing the loop
# first let us define a function to set up the driver
def driver_setup(url):
    # install the driver and initiate it
    driver = webdriver.Chrome(ChromeDriverManager().install())
    # url = "https://elephrame.com/textbook/BLM/chart"
    #driver.implicitly_wait(10)
    #driver.maximize_window()
    driver.get(url)
    time.sleep(20)
    return driver

# second let us define a function to scrape each page
def scrape_page(driver):
    '''Access driver page source, and scrape all the necessary info,
    return a list of vars named data'''
    # we get the first page, voila, initiate page_id=1
    page_id=1
    # Let us open a tsv file for saving our data
    with open('blm-data.tsv','w+') as f:
        tsv_writer = csv.writer(f, delimiter='\t')
        # write column names
        var_names=["page_id","protest_id", "protest_location","protest_start","protest_end","protest_subject","protest_participants", 
                "protest_time","protest_description", "protest_urls"]
        tsv_writer.writerow(var_names)
        # loop the website to get all pages
        while page_id<=229:
            try:
                driver.execute_script("window.scrollTo(0,document.body.scrollHeight)")
                page = driver.page_source
                soup = bs(page,"html.parser")
                items = soup.findAll("div",{"class":"item chart"})
                # iterating all items
                for item in items:
                    try:
                        protest_id=re.findall(r'id="([0-9].*?)"',str(item))[0]
                        print(protest_id)
                    except:
                        protest_id=""
                    try:
                        protest_location=' '.join(item.find("div",{"class":"item-protest-location"}).text.split())
                        print(protest_location)
                    except:
                        protest_location=""
                    try:
                        protest_start=' '.join(item.find("div",{"class":"protest-start"}).text.split())
                        print(protest_start)
                    except:
                        protest_start=""
                    try:
                        protest_end=' '.join(item.find("div",{"class":"protest-end"}).text.split())
                        print(protest_end)
                    except:
                        protest_end=""
                    try:
                        protest_subject=' '.join(item.find("div",{"class":"item-protest-subject"}).text.split())
                        print(protest_subject)
                    except:
                        protest_subject=""
                    try:
                        protest_participants=' '.join(item.find("li",{"class":"item-protest-participants"}).text.split())
                        print(protest_participants)
                    except:
                        protest_participants=""
                    try:
                        protest_time=' '.join(item.find("li",{"class":"item-protest-time"}).text.split())
                        print(protest_time)
                    except:
                        protest_time=""
                    try:
                        protest_description=' '.join(item.find("li",{"class":"item-protest-description"}).text.split())
                        print(protest_description)
                    except:
                        protest_description=""
                    try:
                        protest_urls='##'.join(item.find("li",{"class":"item-protest-url"}).text.split())
                        print(protest_urls,"\n")
                    except:
                        protest_urls=""
                    data=[page_id,protest_id, protest_location,protest_start,protest_end,protest_subject,protest_participants, 
                        protest_time,protest_description, protest_urls]
                    tsv_writer.writerow(data)
            except:
                print("SOMETHING IS WRONG...\n")
                break
            # click next page
            try:
                if page_id==229:
                  break
                else:
                  pass
                driver.execute_script("window.scrollTo(0,document.body.scrollHeight)")
                driver.execute_script("window.scrollTo(0,document.body.scrollHeight)")
                time.sleep(3)
                next_page = driver.find_element(By.XPATH, '//*[@id="blm-results"]/div[3]/ul/li[4]')
                next_page.click()
                time.sleep(3)
                # update page_id
                page = driver.page_source
                soup = bs(page,"html.parser")
                page_id = soup.find("input",{"class":"page-choice"})["value"]
                page_id = int(page_id)
                print(page_id)
            except:
                print("CLICKING NEXT PAGE FAILS...\n")
                break

def main():
    # set up the driver
    url = "https://elephrame.com/textbook/BLM/chart"
    driver = driver_setup(url)
    scrape_page(driver)
    
main()

"""#### Sometimes your webdriver keeps refreshing is annoying. You can use headless to not show your browser running"""

chrome_options = webdriver.ChromeOptions()
#chrome_options.add_argument('--headless')
#chrome_options.add_argument('--no-sandbox')
#chrome_options.add_argument('--disable-dev-shm-usage')
driver =webdriver.Chrome('chromedriver',chrome_options=chrome_options)

"""## We don't have enough time to learn NLTK in python. Please read the NLP with Python Book first three chapters for more tech details."""