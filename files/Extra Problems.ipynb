{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Problems -- Applied Linear Algebra\n",
    "\n",
    "This document contains the computational component of exercises in Allaire and Kaber, _Numerical Linear Algebra_, Springer. The problems are intended to be done in MATLAB, but I choose to do them using Python. These extra problems are not assigned as homework for the course named above, and are purely for my practice with the computational aspect of numerical linear algebra. I tried these problems out as an exercise in preparation for the final exam and my answers may be subject to errors.\n",
    "\n",
    "### Exercise 2.15\n",
    "$$\n",
    "P = \\begin{bmatrix} 1 & 2 & 2 & 1 \\\\ 2 & 3 & 3 & 2\\\\ -1 & 1 & 2 & -2 \\\\ 1 & 3 & 2 & 1\\end{bmatrix},\\;\\; D = \\begin{bmatrix} 2 & 1 & 0 & 0\\\\ 0 & 2 & 1 & 0 \\\\ 0 & 0 & 3 & 0\\\\ 0 & 0 & 0 & 4\\end{bmatrix}, \\;\\; A = PDP^{-1}\n",
    "$$\n",
    "\n",
    "1. $\\sigma(A) = \\sigma(D) = \\{2, 3, 4\\}$ ($\\because D$: upper triangular)\n",
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([[1, 2, 2, 1], [2, 3, 3, 2], [-1, 1, 2, -2], [1, 3, 2, 1]])\n",
    "D = np.array([[2, 1, 0, 0], [0, 2, 1, 0], [0, 0, 3, 0], [0, 0, 0, 4]])\n",
    "A = P.dot(D.dot(la.inv(P)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.+0.00000000e+00j, 3.+0.00000000e+00j, 2.+1.65960863e-07j,\n",
       "       2.-1.65960863e-07j])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam, U = la.eig(A)\n",
    "lam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find that the function `eig` is subject to some machine precision and thus recognizes two separate values for the eigenvalue of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 693., -307.,  -56., -127.],\n",
       "       [1256., -568., -112., -216.],\n",
       "       [-734.,  398.,  120.,   50.],\n",
       "       [ 742., -326.,  -56., -138.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.matrix_power(A, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9231335.00000188,  -4475213.00000088,  -1047552.00000016,\n",
       "         -1327437.00000031],\n",
       "       [ 18112472.00000345,  -8833352.00000161,  -2095104.00000029,\n",
       "         -2538824.00000056],\n",
       "       [-16392202.00000272,   8259246.00000124,   2096128.00000023,\n",
       "          1968814.00000043],\n",
       "       [  9404386.00000201,  -4533238.00000093,  -1047552.00000017,\n",
       "         -1384438.00000033]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.matrix_power(A, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine precision errors get inflated as we take higher matrix powers. This is probably due to errors accumulating from each multiplication.\n",
    "\n",
    "### Exercise 2.21\n",
    "\n",
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SymmetricMat(n):\n",
    "    \"\"\"\n",
    "    returns a symmetric n\\times n matrix\n",
    "    \"\"\"\n",
    "    A_ = np.random.randn(n, n)\n",
    "    A = A_ + A_.T\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.13991674,  1.36084063, -0.97331958, -0.20948179, -0.15519334],\n",
       "       [ 1.36084063, -0.1783174 , -1.55205954,  0.44404328, -1.68380325],\n",
       "       [-0.97331958, -1.55205954,  0.43291873, -1.93496778,  0.52039052],\n",
       "       [-0.20948179,  0.44404328, -1.93496778,  2.74160215,  1.83700503],\n",
       "       [-0.15519334, -1.68380325,  0.52039052,  1.83700503,  0.37035364]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 5\n",
    "A = SymmetricMat(n)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.13991674,  1.36084063, -0.97331958, -0.20948179, -0.15519334],\n",
       "       [ 1.36084063, -0.1783174 , -1.55205954,  0.44404328, -1.68380325],\n",
       "       [-0.97331958, -1.55205954,  0.43291873, -1.93496778,  0.52039052],\n",
       "       [-0.20948179,  0.44404328, -1.93496778,  2.74160215,  1.83700503],\n",
       "       [-0.15519334, -1.68380325,  0.52039052,  1.83700503,  0.37035364]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam, U = la.eig(A) # eigendecomposition of A\n",
    "\n",
    "# Sum using decomposition\n",
    "S = np.zeros(n)\n",
    "for i in range(n):\n",
    "    S = S + lam[i]*np.outer(U[:, i], U[:, i])\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.9634419216236965e-15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.norm(S - A) # check the norm of difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S$ is equivalent to $A$ up to machine precision.\n",
    "\n",
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.50186036,  1.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  3.80560903,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  1.1056723 ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , -2.30331375,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        , -1.60335408]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = np.diag(lam)\n",
    "D[0, 1] = 1\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.88424341, -3.81346609, -0.59098895,  0.45099422,  0.98425252],\n",
       "       [-3.81346609,  5.97222943, -0.22641825, -0.18647433, -1.90256606],\n",
       "       [-0.59098895, -0.22641825,  1.16929196,  1.51333317, -1.18568612],\n",
       "       [ 0.45099422, -0.18647433,  1.51333317, -1.8362864 , -1.13499942],\n",
       "       [ 0.98425252, -1.90256606, -1.18568612, -1.13499942,  2.29349871]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = U @ D @ U.T\n",
    "mu, V = la.eig(B)\n",
    "\n",
    "S2 = np.zeros(n)\n",
    "for i in range(n):\n",
    "    S2 = S2 + mu[i]*np.outer(V[:, i], V[:, i])\n",
    "S2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $B$ appears to be very slightly different from $A$. The values look similar but are off by a small amount. I now check the 2-norm of the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.norm(B - A, ord = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some other norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius norm: 0.9999999999999998\n",
      "1-norm: 1.1331729635073913\n",
      "Infinity norm: 1.6572504065929505\n"
     ]
    }
   ],
   "source": [
    "print(\"Frobenius norm: {}\".format(la.norm(B - A, ord = \"fro\")))\n",
    "print(\"1-norm: {}\".format(la.norm(B - A, ord = 1)))\n",
    "print(\"Infinity norm: {}\".format(la.norm(B - A, ord = np.inf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in 2-norm between $B$ and $A$ are 1. I now justify this result. First, define\n",
    "$$\n",
    "E_{12} = (e_{ij}) = \\begin{cases} 1 & i=1,j=2 \\\\ 0 & \\text{otherwise}\\end{cases}.\n",
    "$$\n",
    "Then, we can write $A = UDU^\\top, B = U(D+E_{12})U^\\top$. This gives\n",
    "$$\n",
    "\\lVert B - A \\rVert_2 = \\lVert U(D+E_{12})U^\\top - UDU^\\top\\rVert_2 = \\lVert UE_{12}U^\\top\\rVert_2 = \\lVert E_{12}\\rVert_2 = 1,\n",
    "$$\n",
    "since $U$ is unitary.\n",
    "\n",
    "Therefore, we have $\\lVert B - A\\rVert_2 = 1$.\n",
    "\n",
    "### Exercise 2.23\n",
    "\n",
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.13543842,  0.53523625, -3.07739753, -1.98488579, -0.9448822 ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 5\n",
    "A = np.random.randn(n, n)\n",
    "lam, U = la.eig(A)\n",
    "lam # spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0773975319616436"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(abs(lam)) # spectral radius of A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.83398591, -0.52278093,  0.8868695 , -1.25797755, -1.22115511],\n",
       "       [-0.03385667, -0.59546792, -1.30801879, -1.26162425, -0.42912919],\n",
       "       [ 2.64197445, -0.77925375, -1.45211983, -0.70251011,  0.3804059 ],\n",
       "       [ 0.43960925, -0.41377582,  0.06989936, -1.31847039, -0.11599152],\n",
       "       [ 0.26813113, -0.74085515,  1.37390446, -0.42839547, -1.80441861]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.3835715054031383,\n",
       " 2.456665654292216,\n",
       " 3.6386921092677222,\n",
       " 3.650507379514934,\n",
       " 2.1466817180952926]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = []\n",
    "for i in range(n):\n",
    "    gamma.append(sum(abs(A[:, i])) - abs(A[i, i]))\n",
    "    \n",
    "gamma # calculated $\\gamma_i$s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-2.5495855946424726, 4.217557416163804),\n",
       " (-3.0521335776599945, 1.8611977309244376),\n",
       " (-5.090811941029193, 2.1865722775062517),\n",
       " (-4.9689777703616205, 2.3320369886682473),\n",
       " (-3.9511003314553674, 0.3422631047352176)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = []\n",
    "for i in range(n):\n",
    "    D.append((A[i,i] - gamma[i], A[i,i] + gamma[i]))\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_di = []\n",
    "for i in range(n):\n",
    "    for_lam = []\n",
    "    for j in range(n):\n",
    "        for_lam.append(abs(lam[i] - A[j, j]) <= gamma[j]) # check whether \\lambda_i \\in D_j\n",
    "    \n",
    "    in_di.append(max(for_lam))\n",
    "in_di"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find that, indeed, the eigenvalues fall into at least one of the Gershgorin disk (Gershgorin theorem).\n",
    "\n",
    "To prove this rigorously, let $\\lambda\\in\\sigma(A)$ and $x\\in\\mathbb{R}^n$ be its corresponding eigenvector. Suppose $\\lvert x_i\\rvert \\ge \\lvert x_j\\rvert, \\forall j=1,\\dots,n$ i.e., the $i$th element of $x$ has the largest modulus. Since we have $Ax = \\lambda x$, looking at the $i$th element yields\n",
    "$$\n",
    "\\sum_{j=1}^n a_{ij}x_j = \\lambda x_i \\Rightarrow \\sum_{j\\ne i} a_{ij}x_j = (\\lambda - a_{ii})x_i\n",
    "$$\n",
    "Then, by triangle inequality,\n",
    "$$\n",
    "\\lvert\\lambda - a_{ii}\\rvert = \\left\\lvert\\sum_{j\\ne i} a_{ij}\\frac{x_j}{x_i}\\right\\rvert \\le \\sum_{j\\ne i}\\left\\lvert a_{ij}\\frac{x_j}{x_i}\\right\\rvert = \\sum_{j\\ne i} \\lvert a_{ij}\\rvert \\left\\lvert\\frac{x_j}{x_i}\\right\\rvert \\le \\sum_{j\\ne i} \\lvert a_{ij}\\rvert\n",
    "$$\n",
    "\n",
    "Therefore, for this particular $i$, we have $\\lambda\\in D_i$. Since $\\lambda$ was arbitrary, we have the desired.\n",
    "\n",
    "3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DiagDomMat(n):\n",
    "    \"\"\"\n",
    "    returns a diagonally dominant matrix of size n\\times n\n",
    "    \"\"\"\n",
    "    A = np.random.randn(n, n)\n",
    "    for i in range(n):\n",
    "        A[i, i] = np.random.rand(1) + sum(abs(A[i, :])) - abs(A[i, i])\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.71498062,  1.68936973, -0.23156278],\n",
       "       [-0.45721152,  1.12730861, -0.05158287],\n",
       "       [ 1.15115462, -1.61038548,  2.87366852]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = DiagDomMat(3)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.818990412811266"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.det(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460501452.37675214"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.det(DiagDomMat(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5313084286764557"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.det(DiagDomMat(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.439479738485481"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.det(DiagDomMat(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.9873022258113"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.det(DiagDomMat(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I notice that all the determinants are strictly positive. This can be proved with the Gershgorin theorem above.\n",
    "\n",
    "Suppose $A\\in\\mathcal{M}_n(\\mathbb{R}^n)$ is diagonally dominant. Let $\\lambda\\in\\sigma(A)$. Then, by Gershgorin theorem, we have $\\lambda\\in D_i$ for some Gershgorin disk $D_i$. This gives\n",
    "$$\n",
    "\\lvert \\lambda - a_{ii}\\rvert \\le \\sum_{j\\ne i}\\lvert a_{ij}\\rvert < \\lvert a_{ii}\\rvert.\n",
    "$$\n",
    "By triangle inequality, we have\n",
    "$$\n",
    "\\left\\lvert\\lvert\\lambda\\rvert - \\lvert a_{ii}\\rvert\\right\\rvert \\le \\lvert \\lambda - a_{ii}\\rvert < \\lvert a_{ii}\\rvert \\Rightarrow -\\lvert a_{ii}\\rvert < \\lvert\\lambda\\rvert - \\lvert a_{ii}\\rvert < \\lvert a_{ii}\\rvert.\n",
    "$$\n",
    "Therefore, we have\n",
    "$$\n",
    "0 < \\lvert \\lambda\\rvert < 2\\lvert a_{ii}\\rvert,\n",
    "$$\n",
    "which implies $\\lambda \\ne 0$.\n",
    "\n",
    "### Exercise 3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9965717124228335"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "A = np.random.randn(n, n)\n",
    "rho = max(abs(la.eig(A)[0]))\n",
    "rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.171994189780434,\n",
       " 3.084045731445063,\n",
       " 3.054361165218563,\n",
       " 3.039828496453234,\n",
       " 3.031124788558357,\n",
       " 3.0253383035411328,\n",
       " 3.0212119531664925,\n",
       " 3.018120871684719,\n",
       " 3.015718888446027,\n",
       " 3.013798678160939]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks = np.linspace(10, 100, num = 10)\n",
    "res = []\n",
    "for k in ks:\n",
    "    A_k = la.matrix_power(A, int(k))\n",
    "    res.append(la.norm(A_k, ord = 2)**(1/k))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ks, res)\n",
    "plt.axhline(rho, color = \"red\")\n",
    "plt.title(\"Norm of Matrix Power\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"$||A^k||^\\\\frac{1}{k}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot seems to suggest that the value of the norm converges to $\\rho(A)$ as $k\\rightarrow\\infty$.\n",
    "\n",
    "Suppose $\\lambda\\in\\sigma(A)$ s.t. $\\lvert\\lambda\\rvert = \\rho(A)$, i.e., the eigenvalue with the largest modulus.\n",
    "\n",
    "Let $x\\in\\mathbb{R}^n$ be a corresponding eigenvector and $y\\in\\mathbb{R}^n$ be any vector. Then, by definition of a matrix norm, we have\n",
    "$$\n",
    "\\lVert A^k xy^\\top\\rVert \\le \\lVert A^k\\rVert \\lVert xy^\\top\\rVert.\n",
    "$$\n",
    "Since $x$ is an eigenvector,\n",
    "$$\n",
    "\\lVert A^k xy^\\top\\rVert = \\lVert \\lambda^k xy^\\top\\rVert = \\lvert \\lambda^k\\rvert \\lVert xy^\\top\\rVert \\le \\lVert A^k\\rVert \\lVert xy^\\top\\rVert\n",
    "$$\n",
    "$$\n",
    "\\Rightarrow \\lvert\\lambda\\rvert^k = \\lvert\\lambda^k\\rvert \\le \\lVert A^k\\rVert \\Rightarrow \\rho(A) \\le \\lVert A^k\\rVert^\\frac1k\n",
    "$$\n",
    "as desired.\n",
    "\n",
    "### Exercise 4.4\n",
    "\n",
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def matmul_ikj(B, C):\n",
    "    \"\"\"\n",
    "    returns the matrix product multiplying in i, k, j order\n",
    "    \"\"\"\n",
    "    p = B.shape[0]\n",
    "    q = C.shape[1]\n",
    "    r = B.shape[1]\n",
    "    A = np.zeros((p, q))\n",
    "    for i in range(p):\n",
    "        for k in range(r):\n",
    "            for j in range(q):\n",
    "                A[i,j] = A[i,j] + B[i,k] * C[k,j]\n",
    "    return A\n",
    "\n",
    "@njit\n",
    "def matmul_strassen(B, C):\n",
    "    \"\"\"\n",
    "    returns the matrix product using Strassen's algorithm\n",
    "    \"\"\"\n",
    "    n = B.shape[0]\n",
    "    A = np.zeros((n, n))\n",
    "    s1, s2 = slice(0, n//2), slice(n//2, n)\n",
    "    \n",
    "    if n <= 64:\n",
    "        A = matmul_ikj(B, C)\n",
    "    \n",
    "    else:\n",
    "        B11, B12, B21, B22 = B[s1,s1], B[s1,s2], B[s2, s1], B[s2, s2]\n",
    "        C11, C12, C21, C22 = C[s1,s1], C[s1,s2], C[s2, s1], C[s2, s2]\n",
    "        M1 = matmul_strassen((B11 + B22), (C11 + C22))\n",
    "        M2 = matmul_strassen((B21 + B22), C11)\n",
    "        M3 = matmul_strassen(B11, (C12 - C22))\n",
    "        M4 = matmul_strassen(B22, (C21 - C11))\n",
    "        M5 = matmul_strassen((B11 + B12), C22)\n",
    "        M6 = matmul_strassen((B21 - B11), (C11 + C12))\n",
    "        M7 = matmul_strassen((B12 - B22), (C21 + C22))\n",
    "        A[s1, s1] = M1 + M4 - M5 + M7\n",
    "        A[s1, s2] = M3 + M5\n",
    "        A[s2, s1] = M2 + M4\n",
    "        A[s2, s2] = M1 - M2 + M3 + M6\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.random.randn(50, 70)\n",
    "C = np.random.randn(70, 50)\n",
    "A1 = B @ C\n",
    "A2 = matmul_strassen(B, C)\n",
    "la.norm(A1 - A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.13 (Hager's Algorithm)\n",
    "\n",
    "$S = \\{x\\in\\mathbb{R}^n: \\lVert x\\rVert_1 = 1\\}$, $f(x) = \\lVert A^{-1}x\\rVert_1$, $A\\in\\mathcal{M}_n(\\mathbb{R})$: nonsingular. Consider the 1-norm conditioning\n",
    "$$\n",
    "\\operatorname{cond}_1(A) = \\lVert A\\rVert_1 \\max_{x\\in S}f(x)\n",
    "$$\n",
    "\n",
    "#### 1. \n",
    "$\\lVert A\\rVert_1$ is the largest column sum.\n",
    "\n",
    "\n",
    "#### 2. \n",
    "$f$ attains maximum at $e_j$ for some $j\\in\\{1,\\dots,n\\}$\n",
    "\n",
    "Proof: Let $A^{-1} = (\\tilde a_{ij})$. Note that for some $j^*$, we have\n",
    "$$\n",
    "\\lVert A^{-1}\\rVert_1 = \\sum_{i=1}^n \\lvert \\tilde a_{ij^*}\\rvert = \\lVert A^{-1}_{j^*}\\rVert,\n",
    "$$\n",
    "where $A^{-1}_{j^*}$ is the $j^*$th column of $A^{-1}$ column of $A^{-1}$. In other words, $j^*$th column maximizes the column sum of $A^{-1}$. Then, we have\n",
    "$$\n",
    "\\lVert A^{-1}e_{j^*}\\rVert_1 = \\lVert A^{-1}_{j^*}\\rVert_1 = \\lVert A^{-1}\\rVert_1,\n",
    "$$\n",
    "where $e_i$ is the $i$th canonical basis vector for $\\mathbb{R}^n$.\n",
    "\n",
    "By definition of a subordinate norm, $e_{j^*}$ maxmizes $f$.\n",
    "\n",
    "\n",
    "#### 3.\n",
    "Given $x\\in\\mathbb{R}^n$, $\\tilde x$ solves $A\\tilde x = x$ and $\\bar x$ solves $A^\\top \\bar x = s$, where $s$ is the sign vector of $\\tilde x$. Then, $f(x) = \\langle \\tilde x, s\\rangle$.\n",
    "\n",
    "Proof: Since $A$ is given to be nonsingular, we have $\\tilde x = A^{-1}x$. Then, we have\n",
    "$$\n",
    "\\langle \\tilde x, s\\rangle = \\lvert \\tilde x_1\\rvert + \\lvert \\tilde x_2\\rvert + \\cdots + \\lvert \\tilde x_n\\rvert = \\sum_{i=1}^n\\lvert \\tilde x_i\\rvert = \\lVert \\tilde x\\rVert_1\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "f(x) = \\lVert A^{-1}x\\rVert_1 = \\lVert \\tilde x\\rVert_1 = \\langle \\tilde x, s\\rangle\n",
    "$$\n",
    "as desired.\n",
    "\n",
    "\n",
    "#### 4.\n",
    "$\\forall a\\in\\mathbb{R}^n, f(x) = \\bar x^\\top(a-x)\\le f(a)$.\n",
    "\n",
    "Proof: Fix $a\\in\\mathbb{R}^n$. Then, $f(a) = \\lVert A^{-1}a\\rVert_1$. Note that $\\bar x^\\top = s^\\top A^{-1}$. Then,\n",
    "$$\n",
    "\\bar x^\\top (a-x) = s^\\top A^{-1}(a-x) = s^\\top A^{-1}a - s^\\top A^{-1}x \\le \\lVert A^{-1}a\\rVert_1 - \\lVert A^{-1}x\\rVert_1,\n",
    "$$\n",
    "where the last inequality follows from $a_j \\le \\lvert a_j\\rvert,\\;\\;-a_j \\le \\lvert a_j\\rvert,\\;\\; a = (a_1, \\dots, a_n)$.\n",
    "Therefore,\n",
    "$$\n",
    "f(x) + \\bar x^\\top(a-x) \\le \\lVert A^{-1}x\\rVert_1 + \\lVert A^{-1}a\\rVert_1 - \\lVert A^{-1}x\\rVert_1 = \\lVert A^{-1}a\\rVert_1 = f(a).\n",
    "$$\n",
    "Hence, $f(x) + \\bar x^\\top(a-x)\\le f(a)$ as desired. Since $a\\in\\mathbb{R}^n$ was arbitrary, this holds for any vector $a$.\n",
    "\n",
    "\n",
    "#### 5.\n",
    "If $\\bar x_j > \\langle x, \\bar x\\rangle$ for some index $j$, then $f(e_j) > f(x)$.\n",
    "\n",
    "Proof: Note that \n",
    "$$\n",
    "\\langle x, \\bar x\\rangle = \\langle x, (A^\\top)^{-1}s\\rangle = \\langle x, (A^{-1})^\\top s\\rangle = \\langle A^{-1}x, s\\rangle = \\langle \\tilde x, s\\rangle = f(x).\n",
    "$$\n",
    "Also,\n",
    "$$\n",
    "\\bar x_j = \\langle e_j, \\bar x\\rangle = \\langle e_j, (A^\\top)^{-1}s\\rangle = \\langle A^{-1}e_j, s\\rangle \\le \\lVert A^{-1}e_j\\rVert_1 = f(e_j).\n",
    "$$\n",
    "Therefore, if $\\bar x_j > \\langle x, \\bar x\\rangle$, we have\n",
    "$$\n",
    "f(e_j) > f(x).\n",
    "$$\n",
    "\n",
    "#### 6.\n",
    "##### a) \n",
    "Note that\n",
    "$$\n",
    "f(x) + s^\\top A^{-1}(y-x) = \\lVert A^{-1}x\\rVert_1 + s^\\top A^{-1}y - \\lVert A^{-1}x\\rVert_1\n",
    "$$\n",
    "and $f(y) = \\lVert A^{-1}y\\rVert_1$.\n",
    "\n",
    "Suppose $y\\approx x$ so that $\\operatorname{sign}(\\tilde y) = \\operatorname{sign}(\\tilde x)$. Then, $s = \\operatorname{sign}(\\tilde y)$, where $\\tilde y = A^{-1}y$. This gives\n",
    "$$\n",
    "s^\\top A^{-1}y = \\lVert A^{-1}y\\rVert_1.\n",
    "$$\n",
    "Hence, $f(y) = f(x) + s^\\top A^{-1}(y-x)$ as desired.\n",
    "\n",
    "##### b)\n",
    "$\\lVert\\bar x\\rVert_\\infty \\le \\langle x, \\bar x\\rangle \\Rightarrow x$: local maximum of $f$ on $S$.\n",
    "\n",
    "Proof: Let $\\bar x = (\\bar x_1, \\dots, \\bar x_n)$. Suppose the $j$th element maximizes $\\lvert \\bar x_i\\rvert$ for $i\\in\\{1,\\dots,n\\}$. Then, $\\lVert \\bar x\\rVert_\\infty = \\lvert \\bar x_j\\rvert$.\n",
    "\n",
    "From 5, we know that $\\lvert\\tilde x_j\\rvert = \\lvert f(e_j)\\rvert \\le f(x)$ for all $j\\in\\{1,\\dots,n\\}$.\n",
    "\n",
    "By the result of 2, we know $f$ is maximized for some $e_{j^*}, j^*\\in\\{1,\\dots,n\\}$. Since $f(e_j)\\le f(x)$ for all $j\\in\\{1,\\dots,n\\}$, $x$ must maximize $f$.\n",
    "\n",
    "#### 7.\n",
    "The algorithm is laid in the function defined for Part 8.\n",
    "\n",
    "#### 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hager(A):\n",
    "    \"\"\"\n",
    "    returns the 1-norm conditioning of matrix A using Hager's algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "    A: $n\\times n$ array, the matrix to find the 1-norm conditioning for. Must be nonsingular.\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    \n",
    "    x0 = np.repeat(1/n, n) # initial vector (x above)\n",
    "    \n",
    "    f = 0 # initial value of f(x)\n",
    "    x_curr = x0\n",
    "    \n",
    "    while(True):\n",
    "        tilde_x = la.solve(A, x_curr) # solve A\\tilde x = x above\n",
    "        \n",
    "        if la.norm(tilde_x, ord =1) <= f:\n",
    "            break\n",
    "        else:\n",
    "            f = la.norm(tilde_x, ord = 1)\n",
    "            \n",
    "        s = np.sign(tilde_x) # s\n",
    "        bar_x = la.solve(A.T, s) # solve A^\\top \\bar x = s above\n",
    "        \n",
    "        if max(abs(bar_x)) <= np.dot(bar_x, x_curr): # check for \\lvert x_j\\rvert \\le \\langle x, \\bar x\\rangle (6b)\n",
    "            break\n",
    "        else:\n",
    "            x_curr = np.zeros(n)\n",
    "            x_curr[np.argmax(bar_x)] = 1 # set new x as e_j\n",
    "                       \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1566276430133953"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "n = 5\n",
    "A_ = np.random.randn(n, n)\n",
    "A = A_ + A_.T # to ensure A is nonsingular\n",
    "hager(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1566276430133953"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.norm(la.inv(A), ord = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find that my formulation of Hager's algorithm works correctly and gives the same result as the 1-norm of the inverse using the default functions in `numpy`. This is valuable in the sense that we do not have to directly compute the inverse to obtain the 1-norm conditioning, as calculating inverses are numerically unstable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
