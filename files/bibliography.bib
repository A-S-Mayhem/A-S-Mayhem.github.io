%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Ayush Pandey at 2023-10-13 12:44:57 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{katsarakis_zeus_2021,
	abstract = {State-of-the-art distributed in-memory datastores (FaRM, FaSST, DrTM) provide strongly-consistent distributed transactions with high performance and availability. Transactions in those systems are fully general; they can atomically manipulate any set of objects in the store, regardless of their location. To achieve this, these systems use complex distributed transactional protocols. Meanwhile, many workloads have a high degree of locality. For such workloads, distributed transactions are an overkill as most operations only access objects located on the same server - if sharded appropriately. In this paper, we show that for these workloads, a single-node transactional protocol combined with dynamic object re-sharding and asynchronously pipelined replication can provide the same level of generality with better performance, simpler protocols, and lower developer effort. We present Zeus, an in-memory distributed datastore that provides general transactions by acquiring all objects involved in the transaction to the same server and executing a single-node transaction on them. Zeus is fault-tolerant and strongly-consistent. At the heart of Zeus is a reliable dynamic object sharding protocol that can move 250K objects per second per server, allowing Zeus to process millions of transactions per second and outperform more traditional distributed transactions on a wide range of workloads that exhibit locality.},
	address = {New York, NY, USA},
	author = {Katsarakis, Antonios and Ma, Yijun and Tan, Zhaowei and Bainbridge, Andrew and Balkwill, Matthew and Dragojevic, Aleksandar and Grot, Boris and Radunovic, Bozidar and Zhang, Yongguang},
	booktitle = {Proceedings of the {Sixteenth} {European} {Conference} on {Computer} {Systems}},
	doi = {10.1145/3447786.3456234},
	file = {Submitted Version:/Users/pandey/Zotero/storage/YG6ST628/Katsarakis et al. - 2021 - Zeus locality-aware distributed transactions.pdf:application/pdf},
	isbn = {978-1-4503-8334-9},
	keywords = {transactions, availability, dynamic sharding, locality, pipelining, replication, strict serializability},
	month = apr,
	pages = {145--161},
	publisher = {Association for Computing Machinery},
	series = {{EuroSys} '21},
	shorttitle = {Zeus},
	title = {Zeus: locality-aware distributed transactions},
	url = {https://doi.org/10.1145/3447786.3456234},
	urldate = {2022-04-05},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3447786.3456234}}

@article{kulkarni_exploiting_2011,
	abstract = {Speculative execution is a promising approach for exploiting parallelism in many programs, but it requires efficient schemes for detecting conflicts between concurrently executing threads. Prior work has argued that checking semantic commutativity of method invocations is the right way to detect conflicts for complex data structures such as kd-trees. Several ad hoc ways of checking commutativity have been proposed in the literature, but there is no systematic approach for producing implementations. In this paper, we describe a novel framework for reasoning about commutativity conditions: the commutativity lattice. We show how commutativity specifications from this lattice can be systematically implemented in one of three different schemes: abstract locking, forward gatekeeping and general gatekeeping. We also discuss a disciplined approach to exploiting the lattice to find different implementations that trade off precision in conflict detection for performance. Finally, we show that our novel conflict detection schemes are practical and can deliver speedup on three real-world applications.},
	author = {Kulkarni, Milind and Nguyen, Donald and Prountzos, Dimitrios and Sui, Xin and Pingali, Keshav},
	doi = {10.1145/1993316.1993562},
	file = {Kulkarni et al. - Exploiting the Commutativity Lattice.pdf:/Users/pandey/Zotero/storage/AHEKP9KB/Kulkarni et al. - Exploiting the Commutativity Lattice.pdf:application/pdf},
	issn = {0362-1340},
	journal = {ACM SIGPLAN Notices},
	keywords = {commutativity lattice, optimistic parallelism, transactions},
	month = jun,
	number = {6},
	pages = {542--555},
	title = {Exploiting the commutativity lattice},
	url = {https://doi.org/10.1145/1993316.1993562},
	urldate = {2022-04-05},
	volume = {46},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1145/1993316.1993562}}

@inproceedings{herlihy_transactional_2008,
	abstract = {We describe a methodology for transforming a large class of highly-concurrent linearizable objects into highly-concurrent transactional objects. As long as the linearizable implementation satisfies certain regularity properties (informally, that every method has an inverse), we define a simple wrapper for the linearizable implementation that guarantees that concurrent transactions without inherent conflicts can synchronize at the same granularity as the original linearizable implementation.},
	address = {New York, NY, USA},
	author = {Herlihy, Maurice and Koskinen, Eric},
	booktitle = {Proceedings of the 13th {ACM} {SIGPLAN} {Symposium} on {Principles} and practice of parallel programming},
	doi = {10.1145/1345206.1345237},
	file = {Herlihy and Koskinen - 2008 - Transactional boosting a methodology for highly-c.pdf:/Users/pandey/Zotero/storage/XCTPT85N/Herlihy and Koskinen - 2008 - Transactional boosting a methodology for highly-c.pdf:application/pdf},
	isbn = {978-1-59593-795-7},
	keywords = {abstract locks, commutativity, non-blocking algorithms, transactional boosting, transactional memory},
	month = feb,
	pages = {207--216},
	publisher = {Association for Computing Machinery},
	series = {{PPoPP} '08},
	shorttitle = {Transactional boosting},
	title = {Transactional boosting: a methodology for highly-concurrent transactional objects},
	url = {https://doi.org/10.1145/1345206.1345237},
	urldate = {2022-04-05},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1145/1345206.1345237}}

@inproceedings{koskinen_coarse-grained_2010,
	abstract = {Traditional transactional memory systems suffer from overly conservative conflict detection, yielding so-called false conflicts, because they are based on fine-grained, low-level read/write conflicts. In response, the recent trend has been toward integrating various abstract data-type libraries using ad-hoc methods of high-level conflict detection. These proposals have led to improved performance but a lack of a unified theory has led to confusion in the literature. We clarify these recent proposals by defining a generalization of transactional memory in which a transaction consists of coarse-grained (abstract data-type) operations rather than simple memory read/write operations. We provide semantics for both pessimistic (e.g. transactional boosting) and optimistic (e.g. traditional TMs and recent alternatives) execution. We show that both are included in the standard atomic semantics, yet find that the choice imposes different requirements on the coarse-grained operations: pessimistic requires operations be left-movers, optimistic requires right-movers. Finally, we discuss how the semantics applies to numerous TM implementation details discussed widely in the literature.},
	address = {New York, NY, USA},
	author = {Koskinen, Eric and Parkinson, Matthew and Herlihy, Maurice},
	booktitle = {Proceedings of the 37th annual {ACM} {SIGPLAN}-{SIGACT} symposium on {Principles} of programming languages},
	doi = {10.1145/1706299.1706304},
	file = {Koskinen et al. - 2010 - Coarse-grained transactions.pdf:/Users/pandey/Zotero/storage/LEJAWITH/Koskinen et al. - 2010 - Coarse-grained transactions.pdf:application/pdf},
	isbn = {978-1-60558-479-9},
	keywords = {commutativity, transactional boosting, transactional memory, abstract data-types, coarse-grained transactions, movers},
	month = jan,
	pages = {19--30},
	publisher = {Association for Computing Machinery},
	series = {{POPL} '10},
	title = {Coarse-grained transactions},
	url = {https://doi.org/10.1145/1706299.1706304},
	urldate = {2022-04-05},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1145/1706299.1706304}}

@inproceedings{diniz_lock_1997,
	abstract = {Atomic operations are a key primitive in parallel computing systems. The standard implementation mechanism for atomic operations uses mutual exclusion locks. In an object-based programming system the natural granularity is to give each object its own lock. Each operation can then make its execution atomic by acquiring and releasing the lock for the object that it accesses. But this fine lock granularity may have high synchronization overhead. To achieve good performance it may be necessary to reduce the overhead by coarsening the granularity at which the computation locks objects.},
	address = {Berlin, Heidelberg},
	author = {Diniz, Pedro and Rinard, Martin},
	booktitle = {Languages and {Compilers} for {Parallel} {Computing}},
	doi = {10.1007/BFb0017259},
	editor = {Sehr, David and Banerjee, Utpal and Gelernter, David and Nicolau, Alex and Padua, David},
	file = {Springer Full Text PDF:/Users/pandey/Zotero/storage/BJJTGQKB/Diniz and Rinard - 1997 - Lock coarsening Eliminating lock overhead in auto.pdf:application/pdf},
	isbn = {978-3-540-69128-0},
	language = {en},
	pages = {285--299},
	publisher = {Springer},
	shorttitle = {Lock coarsening},
	title = {Lock coarsening: {Eliminating} lock overhead in automatically parallelized object-based programs},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1007/BFb0017259}}

@inproceedings{gotsman_cause_2016,
	abstract = {Large-scale distributed systems often rely on replicated databases that allow a programmer to request different data consistency guarantees for different operations, and thereby control their performance. Using such databases is far from trivial: requesting stronger consistency in too many places may hurt performance, and requesting it in too few places may violate correctness. To help programmers in this task, we propose the first proof rule for establishing that a particular choice of consistency guarantees for various operations on a replicated database is enough to ensure the preservation of a given data integrity invariant. Our rule is modular: it allows reasoning about the behaviour of every operation separately under some assumption on the behaviour of other operations. This leads to simple reasoning, which we have automated in an SMT-based tool. We present a nontrivial proof of soundness of our rule and illustrate its use on several examples.},
	address = {New York, NY, USA},
	author = {Gotsman, Alexey and Yang, Hongseok and Ferreira, Carla and Najafzadeh, Mahsa and Shapiro, Marc},
	booktitle = {Proceedings of the 43rd {Annual} {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	doi = {10.1145/2837614.2837625},
	file = {Submitted Version:/Users/pandey/Zotero/storage/ZB8T37B9/Gotsman et al. - 2016 - 'Cause I'm strong enough Reasoning about consiste.pdf:application/pdf},
	isbn = {978-1-4503-3549-2},
	keywords = {causal consistency, integrity invariants, Replication},
	month = jan,
	pages = {371--384},
	publisher = {Association for Computing Machinery},
	series = {{POPL} '16},
	shorttitle = {'{Cause} {I}'m strong enough},
	title = {'{Cause} {I}'m strong enough: {Reasoning} about consistency choices in distributed systems},
	url = {https://doi.org/10.1145/2837614.2837625},
	urldate = {2022-04-05},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1145/2837614.2837625}}

@article{houshmand_hamsaz_2019,
	abstract = {Distributed system replication is widely used as a means of fault-tolerance and scalability. However, it provides a spectrum of consistency choices that impose a dilemma for clients between correctness, responsiveness and availability. Given a sequential object and its integrity properties, we automatically synthesize a replicated object that guarantees state integrity and convergence and avoids unnecessary coordination. Our approach is based on a novel sufficient condition for integrity and convergence called well-coordination that requires certain orders between conflicting and dependent operations. We statically analyze the given sequential object to decide its conflicting and dependent methods and use this information to avoid coordination. We present novel coordination protocols that are parametric in terms of the analysis results and provide the well-coordination requirements. We implemented a tool called Hamsaz that can automatically analyze the given object, instantiate the protocols and synthesize replicated objects. We have applied Hamsaz to a suite of use-cases and synthesized replicated objects that are significantly more responsive than the strongly consistent baseline.},
	author = {Houshmand, Farzin and Lesani, Mohsen},
	doi = {10.1145/3290387},
	file = {Houshmand and Lesani - 2019 - Hamsaz replication coordination analysis and synt.pdf:/Users/pandey/Zotero/storage/RW5KKI5I/Houshmand and Lesani - 2019 - Hamsaz replication coordination analysis and synt.pdf:application/pdf},
	issn = {2475-1421},
	journal = {Proceedings of the ACM on Programming Languages},
	language = {en},
	month = jan,
	number = {POPL},
	pages = {1--32},
	shorttitle = {Hamsaz},
	title = {Hamsaz: replication coordination analysis and synthesis},
	url = {https://dl.acm.org/doi/10.1145/3290387},
	urldate = {2022-04-11},
	volume = {3},
	year = {2019},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3290387},
	bdsk-url-2 = {https://doi.org/10.1145/3290387}}

@inproceedings{marcelino_bringing_2017,
	abstract = {Hybrid consistency is a new consistency model that tries to combine the bene ts of weak and strong consistency. To implement hybrid consistency, programmers have to identify con icting operations in applications and instrument them, which is a di cult and error prone task. More recent approaches automatize the process through the use of static analysis over a speci cation of the application.},
	address = {Belgrade Serbia},
	author = {Marcelino, Gon{\c c}alo and Balegas, Valter and Ferreira, Carla},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Principles} and {Practice} of {Consistency} for {Distributed} {Data}},
	doi = {10.1145/3064889.3064896},
	file = {Marcelino et al. - 2017 - Bringing Hybrid Consistency Closer to Programmers.pdf:/Users/pandey/Zotero/storage/V4Z3ZYCW/Marcelino et al. - 2017 - Bringing Hybrid Consistency Closer to Programmers.pdf:application/pdf},
	isbn = {978-1-4503-4933-8},
	language = {en},
	month = apr,
	pages = {1--4},
	publisher = {ACM},
	title = {Bringing {Hybrid} {Consistency} {Closer} to {Programmers}},
	url = {https://dl.acm.org/doi/10.1145/3064889.3064896},
	urldate = {2022-04-11},
	year = {2017},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3064889.3064896},
	bdsk-url-2 = {https://doi.org/10.1145/3064889.3064896}}

@inproceedings{nair_invariant_2019,
	abstract = {We study a proof methodology for verifying the safety of data invariants of highly-available distributed applications that replicate state. The proof is (1) modular: one can reason about each individual operation separately, and (2) sequential: one can reason about a distributed application as if it were sequential. We automate the methodology and illustrate the use of the tool with a representative example.},
	address = {New York, NY, USA},
	author = {Nair, Sreeja S. and Petri, Gustavo and Shapiro, Marc},
	booktitle = {Proceedings of the 6th {Workshop} on {Principles} and {Practice} of {Consistency} for {Distributed} {Data}},
	doi = {10.1145/3301419.3323970},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/QEH3MLPP/Nair et al. - 2019 - Invariant Safety for Distributed Applications.pdf:application/pdf},
	isbn = {978-1-4503-6276-4},
	keywords = {Automatic verification, Consistency, Distributed application design, Replicated data, Tool support},
	month = mar,
	pages = {1--7},
	publisher = {Association for Computing Machinery},
	series = {{PaPoC} '19},
	title = {Invariant {Safety} for {Distributed} {Applications}},
	url = {https://doi.org/10.1145/3301419.3323970},
	urldate = {2022-04-11},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3301419.3323970}}

@article{nair_proving_nodate,
	abstract = {To provide high availability in distributed systems, object replicas allow concurrent updates. Although replicas eventually converge, they may diverge temporarily, for instance when the network fails. This makes it difficult for the developer to reason about the object's properties, and in particular, to prove invariants over its state. For the subclass of state-based distributed systems, we propose a proof methodology for establishing that a given object maintains a given invariant, taking into account any concurrency control. Our approach allows reasoning about individual operations separately. We demonstrate that our rules are sound, and we illustrate their use with some representative examples. We automate the rule using Boogie, an SMT-based tool.},
	author = {Nair, Sreeja and Petri, Gustavo and Shapiro, Marc},
	file = {Nair et al. - Proving the safety of highly-available distributed.pdf:/Users/pandey/Zotero/storage/VKVSVL3P/Nair et al. - Proving the safety of highly-available distributed.pdf:application/pdf},
	language = {en},
	pages = {29},
	title = {Proving the safety of highly-available distributed objects}}

@article{bailis_coordination_2014,
	abstract = {Minimizing coordination, or blocking communication between concurrently executing operations, is key to maximizing scalability, availability, and high performance in database systems. However, uninhibited coordination-free execution can compromise application correctness, or consistency. When is coordination necessary for correctness? The classic use of serializable transactions is sufficient to maintain correctness but is not necessary for all applications, sacrificing potential scalability. In this paper, we develop a formal framework, invariant confluence, that determines whether an application requires coordination for correct execution. By operating on application-level invariants over database states (e.g., integrity constraints), invariant confluence analysis provides a necessary and sufficient condition for safe, coordination-free execution. When programmers specify their application invariants, this analysis allows databases to coordinate only when anomalies that might violate invariants are possible. We analyze the invariant confluence of common invariants and operations from real-world database systems (i.e., integrity constraints) and applications and show that many are invariant confluent and therefore achievable without coordination. We apply these results to a proof-of-concept coordination-avoiding database prototype and demonstrate sizable performance gains compared to serializable execution, notably a 25-fold improvement over prior TPC-C New-Order performance on a 200 server cluster.},
	author = {Bailis, Peter and Fekete, Alan and Franklin, Michael J. and Ghodsi, Ali and Hellerstein, Joseph M. and Stoica, Ion},
	doi = {10.14778/2735508.2735509},
	file = {Bailis et al. - 2014 - Coordination avoidance in database systems.pdf:/Users/pandey/Zotero/storage/JYNZVTUT/Bailis et al. - 2014 - Coordination avoidance in database systems.pdf:application/pdf},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	language = {en},
	month = nov,
	number = {3},
	pages = {185--196},
	title = {Coordination avoidance in database systems},
	url = {https://dl.acm.org/doi/10.14778/2735508.2735509},
	urldate = {2022-04-11},
	volume = {8},
	year = {2014},
	bdsk-url-1 = {https://dl.acm.org/doi/10.14778/2735508.2735509},
	bdsk-url-2 = {https://doi.org/10.14778/2735508.2735509}}

@inproceedings{tomas_fmke_2017,
	abstract = {Standard benchmarks are essential tools to enable developers to validate and evaluate their systems' design in terms of both relevant properties and performance. Benchmarks provide the means to evaluate a system with workloads that mimics real use cases. Although a large number of benchmarks exist for database system, there is a lack of standard benchmarks for an increasingly relevant class of storage systems: geo-replicated key-value stores providing weak consistency guarantees. This has led developers and researchers to rely on ad-hoc tools, whose results are both hard to reproduce and compare. In this paper, we propose the first standardized benchmark specially tailored for weakly consistent key-value stores. The benchmark, named FMKe, is modeled after a real application: the Danish National Joint Medicine Card. The benchmark is scalable, it can be parameterized to emulate a large number of access patterns, and it is also highly flexible, enabling its application on systems that offer different consistency guarantees and mechanisms.},
	address = {New York, NY, USA},
	author = {Tom{\'a}s, Gon{\c c}alo and Zeller, Peter and Balegas, Valter and Akkoorath, Deepthi and Bieniusa, Annette and Leit{\~a}o, Jo{\~a}o and Pregui{\c c}a, Nuno},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Principles} and {Practice} of {Consistency} for {Distributed} {Data}},
	doi = {10.1145/3064889.3064897},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/W7QKQ3L4/Tom{\'a}s et al. - 2017 - FMKe a Real-World Benchmark for Key-Value Data St.pdf:application/pdf},
	isbn = {978-1-4503-4933-8},
	keywords = {Benchmark, Key-Value Store},
	month = apr,
	pages = {1--4},
	publisher = {Association for Computing Machinery},
	series = {{PaPoC} '17},
	shorttitle = {{FMKe}},
	title = {{FMKe}: a {Real}-{World} {Benchmark} for {Key}-{Value} {Data} {Stores}},
	url = {https://doi.org/10.1145/3064889.3064897},
	urldate = {2022-04-11},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3064889.3064897}}

@book{zhan_big_2014,
	address = {Cham},
	doi = {10.1007/978-3-319-13021-7},
	editor = {Zhan, Jianfeng and Han, Rui and Weng, Chuliang},
	file = {Zhan et al. - 2014 - Big Data Benchmarks, Performance Optimization, and.pdf:/Users/pandey/Zotero/storage/XC8UMNZV/Zhan et al. - 2014 - Big Data Benchmarks, Performance Optimization, and.pdf:application/pdf},
	isbn = {978-3-319-13020-0 978-3-319-13021-7},
	language = {en},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	shorttitle = {Big {Data} {Benchmarks}, {Performance} {Optimization}, and {Emerging} {Hardware}},
	title = {Big {Data} {Benchmarks}, {Performance} {Optimization}, and {Emerging} {Hardware}: 4th and 5th {Workshops}, {BPOE} 2014, {Salt} {Lake} {City}, {USA}, {March} 1, 2014 and {Hangzhou}, {China}, {September} 5, 2014, {Revised} {Selected} {Papers}},
	url = {http://link.springer.com/10.1007/978-3-319-13021-7},
	urldate = {2022-04-12},
	volume = {8807},
	year = {2014},
	bdsk-url-1 = {http://link.springer.com/10.1007/978-3-319-13021-7},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-13021-7}}

@book{david_benchmarking_2014,
	abstract = {Cloud storage services and NoSQL systems typically offer only ""Eventual Consistency"", a rather weak guarantee covering a broad range of potential data consistency behavior. The degree of actual (in-)consistency, however, is unknown. This work presents novel solutions for determining the degree of (in-)consistency via simulation and benchmarking, as well as the necessary means to resolve inconsistencies leveraging this information.},
	author = {David, Bermbach},
	isbn = {978-3-7315-0186-2},
	language = {en},
	month = jul,
	note = {Google-Books-ID: 7rQVBAAAQBAJ},
	publisher = {KIT Scientific Publishing},
	title = {Benchmarking, {Consistency}, {Distributed} {Database} {Management} {Systems}, {Distributed} {Systems}, {Eventual} {Consistency}},
	year = {2014}}

@article{xiao_implementation_nodate,
	abstract = {While snapshots have been commonly used in data storages for backup and data protections, little is known in the open literature how such snapshots impact application performance. This paper presents an implementation and performance evaluation of two snapshot techniques: copy-on-write snapshot and redirect-on-write snapshot. Our implementation is carried out at block level on a standard iSCSI target. We carry out quantitative performance evaluations and comparisons of the two snapshot implementations using TPC-C, TPC-W, IoMeter, and PostMark benchmarks. Our measurements reveal many interesting observations regarding the performance characteristics of the two snapshot techniques. Depending on the applications and different I/O workloads, the two snapshot techniques perform quite differently. In general, copy-on-write performs well on read-intensive applications while redirect-on-write performs well on write-intensive applications.},
	author = {Xiao, Weijun and Liu, Yinan and Ren, Jin and Xie, Changsheng},
	file = {Xiao et al. - Implementation and Performance Evaluation of Two S.pdf:/Users/pandey/Zotero/storage/VEUZZ8LH/Xiao et al. - Implementation and Performance Evaluation of Two S.pdf:application/pdf},
	language = {en},
	pages = {10},
	title = {Implementation and {Performance} {Evaluation} of {Two} {Snapshot} {Methods} on {iSCSI} {Target} {Storages}}}

@inproceedings{llanos_tpcc-uva_2006,
	abstract = {This paper presents TPCC-UVa, an open-source implementation of the TPC-C benchmark intended to be used in parallel and distributed systems. TPCC-UVa is written entirely in C language and it uses the Post-greSQL database engine. This implementation includes all the functionalities described by the TPC-C standard specification for the measurement of both uni- and multiprocessor systems performance. The major characteristics of the TPC-C specification are discussed, together with a description of the TPCC-UVa implementation and architecture and real examples of performance measurements},
	author = {Llanos, D.R. and Palop, B.},
	booktitle = {Proceedings 20th {IEEE} {International} {Parallel} {Distributed} {Processing} {Symposium}},
	doi = {10.1109/IPDPS.2006.1639646},
	file = {Full Text:/Users/pandey/Zotero/storage/AW63UI2C/Llanos and Palop - 2006 - TPCC-UVa an open-source TPC-C implementation for .pdf:application/pdf},
	keywords = {Benchmark testing, Computer architecture, Councils, Engines, Measurement standards, Multiprocessing systems, Open source software, System performance, System testing, Transaction databases},
	month = apr,
	note = {ISSN: 1530-2075},
	pages = {8 pp.--},
	shorttitle = {{TPCC}-{UVa}},
	title = {{TPCC}-{UVa}: an open-source {TPC}-{C} implementation for parallel and distributed systems},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/IPDPS.2006.1639646}}

@article{spirovska_optimistic_2021,
	abstract = {Causal consistency (CC) is an attractive consistency model for geo-replicated data stores because it hits a sweet spot in the ease-of-programming versus performance trade-off. We present a new approach for implementing CC in geo-replicated data stores, which we call Optimistic Causal Consistency (OCC). OCC's main design goal is to maximize data freshness. The optimism in our approach lies in the fact that the updates replicated to a remote data center are made visible immediately, without checking if their causal dependencies have been received. Servers perform the dependency check needed to enforce CC only upon serving a client operation, rather than on receipt of a replicated data item as in existing systems. OCC offers a significant gain in data freshness, which is of crucial importance for various types of applications, such as real-time systems. OCC's potentially blocking behavior makes it vulnerable to network partitions. We therefore propose a recovery mechanism that allows an OCC system to fall back on a pessimistic protocol to continue operating during network partitions. We implement POCC, the first causally consistent geo-replicated multi-master key-value data store designed to maximize data freshness. We show that POCC improves data freshness, while offering comparable or better performance than its pessimistic counterparts.},
	author = {Spirovska, Kristina and Didona, Diego and Zwaenepoel, Willy},
	doi = {10.1109/TPDS.2020.3026778},
	file = {IEEE Xplore Abstract Record:/Users/pandey/Zotero/storage/IZNVS4CB/9206094.html:text/html;Submitted Version:/Users/pandey/Zotero/storage/W5FZVMNX/Spirovska et al. - 2021 - Optimistic Causal Consistency for Geo-Replicated K.pdf:application/pdf},
	issn = {1558-2183},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	keywords = {causal consistency, Clocks, Data centers, data freshness, Data integrity, Data models, Distributed databases, geo-replication, key-value data stores, Optimistic causal consistency, Protocols, read-only transactions, Servers},
	month = mar,
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	number = {3},
	pages = {527--542},
	title = {Optimistic {Causal} {Consistency} for {Geo}-{Replicated} {Key}-{Value} {Stores}},
	volume = {32},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TPDS.2020.3026778}}

@article{hellerstein_keeping_2019,
	abstract = {A key concern in modern distributed systems is to avoid the cost of coordination while maintaining consistent semantics. Until recently, there was no answer to the question of when coordination is actually required. In this paper we present an informal introduction to the CALM Theorem, which answers this question precisely by moving up from traditional storage consistency to consider properties of programs. CALM is an acronym for "consistency as logical monotonicity". The CALM Theorem shows that the programs that have consistent, coordination-free distributed implementations are exactly the programs that can be expressed in monotonic logic. This theoretical result has practical implications for developers of distributed applications. We show how CALM provides a constructive application-level counterpart to conventional "systems" wisdom, such as the apparently negative results of the CAP Theorem. We also discuss ways that monotonic thinking can influence distributed systems design, and how new programming language designs and tools can help developers write consistent, coordination-free code.},
	author = {Hellerstein, Joseph M. and Alvaro, Peter},
	file = {arXiv Fulltext PDF:/Users/pandey/Zotero/storage/K2DQJRKW/Hellerstein and Alvaro - 2019 - Keeping CALM When Distributed Consistency is Easy.pdf:application/pdf;arXiv.org Snapshot:/Users/pandey/Zotero/storage/GA3TINJB/1901.html:text/html},
	journal = {arXiv:1901.01930 [cs]},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Programming Languages, Computer Science - Software Engineering},
	month = jan,
	note = {arXiv: 1901.01930},
	shorttitle = {Keeping {CALM}},
	title = {Keeping {CALM}: {When} {Distributed} {Consistency} is {Easy}},
	url = {http://arxiv.org/abs/1901.01930},
	urldate = {2022-04-13},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1901.01930}}

@article{wang_locking_2021,
	abstract = {Suspension-based locks are widely used in realtime systems to coordinate simultaneous accesses to exclusive shared resources. Although suspension-based locks have been well studied for sequential real-time tasks, little work has been done on this topic for parallel real-time tasks. This paper for the first time studies the problem of how to extend existing sequentialtask locking protocols and their analysis techniques to the parallel task model. More specifically, we extend two locking protocols OMLP and OMIP, which were designed for clustered scheduling of sequential real-time tasks, to federated scheduling of parallel real-time tasks. We present corresponding blocking analysis techniques, and develop path-oriented techniques to analyze and count blocking time. Schedulability tests with different efficiency and accuracy are further developed. Experiments are conducted to evaluate the performance of our proposed approaches against the state-of-the-art.},
	author = {Wang, Yang and Jiang, Xu and Guan, Nan and Tang, Yue and Liu, Weichen},
	doi = {10.1109/TCAD.2021.3114261},
	file = {IEEE Xplore Abstract Record:/Users/pandey/Zotero/storage/JU8SV4CH/9542981.html:text/html;IEEE Xplore Full Text PDF:/Users/pandey/Zotero/storage/EPUQDFNC/Wang et al. - 2021 - Locking Protocols for Parallel Real-Time Tasks wit.pdf:application/pdf},
	issn = {1937-4151},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {Protocols, Blocking Analysis., Parallel Tasks, Program processors, Real-Time Scheduling, Real-time systems, Scheduling algorithms, Semaphores, Task analysis, Time factors, Timing},
	note = {Conference Name: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	pages = {1--1},
	title = {Locking {Protocols} for {Parallel} {Real}-{Time} {Tasks} with {Semaphores} under {Federated} {Scheduling}},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TCAD.2021.3114261}}

@inproceedings{jiang_suspension-based_2019,
	abstract = {Suspension-based locks are widely used in realtime systems to coordinate simultaneous accesses to exclusive shared resources. Although suspension-based locks have been well studied for sequential real-time tasks, little work has been done on this topic for parallel real-time tasks. This paper for the first time studies the problem of how to extend existing sequential-task locking protocols and their analysis techniques to the parallel task model. More specifically, we extend two locking protocols OMLP and OMIP, which were designed for clustered scheduling of sequential real-time tasks, to federated scheduling of parallel real-time tasks, and develop path-oriented techniques to analyze and count blocking time. Experiments are conducted to evaluate the performance of our proposed approaches and compare them against the state-of-the-art.},
	author = {Jiang, Xu and Guan, Nan and Tang, Yue and Liu, Weichen and Duan, Hancong},
	booktitle = {2019 {IEEE} {Real}-{Time} {Systems} {Symposium} ({RTSS})},
	doi = {10.1109/RTSS46320.2019.00033},
	file = {IEEE Xplore Full Text PDF:/Users/pandey/Zotero/storage/JNRCLZPJ/Jiang et al. - 2019 - Suspension-Based Locking Protocols for Parallel Re.pdf:application/pdf},
	keywords = {DAG, locking protocol, parallel, real time},
	month = dec,
	note = {ISSN: 2576-3172},
	pages = {274--286},
	title = {Suspension-{Based} {Locking} {Protocols} for {Parallel} {Real}-{Time} {Tasks}},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/RTSS46320.2019.00033}}

@inproceedings{ding_reducing_2014,
	abstract = {As multi-core platforms with hundreds or even more quantities of cores are popular, system optimization issues, including lock contentions, start to puzzle programmers who work on multi-core platforms. Locks are more convenient and clear than lock-free operations (for example, transactional memory) for multi-core programmers. However, lock contention has been recognized as a typical impediment to the performance of shared-memory parallel programs. This paper mainly discusses two important reasons that cause lock contention, including large critical sections and frequent lock requests. For current solutions, it is hard for programmers to find the locations of large critical sections and good scheme to reduce lock contentions on hot critical sections. This paper proposes FFlocker, a series of runtime solutions that reduce lock contention caused by the two issues. FFlocker includes a profiling algorithm to find the locations of large critical sections. Based on the profiling scheme, it binds the threads acquiring the same locks onto the same core. We evaluate our techniques with three benchmarks. The results show that FFlocker offers better performance than Function Flow and OpenMP.},
	author = {Ding, Haimiao and Liao, Xiaofei and Jin, Hai and Lv, Xinqiao and Guo, Rentong},
	booktitle = {2014 20th {IEEE} {International} {Conference} on {Parallel} and {Distributed} {Systems} ({ICPADS})},
	doi = {10.1109/PADSW.2014.7097804},
	file = {IEEE Xplore Abstract Record:/Users/pandey/Zotero/storage/PIQG3W47/7097804.html:text/html;IEEE Xplore Full Text PDF:/Users/pandey/Zotero/storage/KVSR3GFL/Ding et al. - 2014 - Reducing lock contention on multi-core platforms.pdf:application/pdf},
	keywords = {Instruction sets, Libraries, lock contention, Multi-core, Multicore processing, parallel programming, Parallel programming, runtime, Runtime, Switches, Time complexity},
	month = dec,
	note = {ISSN: 1521-9097},
	pages = {158--165},
	title = {Reducing lock contention on multi-core platforms},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/PADSW.2014.7097804}}

@article{yan_leveraging_2016,
	abstract = {Locking is one of the predominant costs in transaction processing. While much work has focused on designing efficient concurrency control mechanisms, not much has been done on understanding how transaction applications issue queries and leveraging application semantics to improve application performance. This paper presents Quro, a query-aware compiler that automatically reorders queries in transaction code to improve performance. Observing that certain queries within a transaction are more contentious than others as they require locking the same tuples as other concurrently executing transactions, Quro automatically changes the application such that contentious queries are issued as late as possible. We have evaluated Quro on various transaction benchmarks, and our results show that Quro-generated implementations can increase transaction throughput by up to 6.53x, while reduce transaction latency by up to 85\%.},
	author = {Yan, Cong and Cheung, Alvin},
	doi = {10.14778/2876473.2876479},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/RABR95CA/Yan and Cheung - 2016 - Leveraging lock contention to improve OLTP applica.pdf:application/pdf},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	month = jan,
	number = {5},
	pages = {444--455},
	title = {Leveraging lock contention to improve {OLTP} application performance},
	url = {https://doi.org/10.14778/2876473.2876479},
	urldate = {2022-04-12},
	volume = {9},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.14778/2876473.2876479}}

@inproceedings{han_scalable_2014,
	abstract = {Since 1990's, Snapshot Isolation (SI) has been widely studied, and it was successfully deployed in commercial and open-source database engines. Berenson et al. showed that data consistency can be violated under SI. Recently, a new class of Serializable SI algorithms (SSI) has been proposed to achieve serializable execution while still allowing concurrency between reads and updates.},
	author = {Han, Hyuck and Park, SeongJae and Jung, Hyungsoo and Fekete, Alan and R{\"o}hm, Uwe and Yeom, Heon Y.},
	booktitle = {2014 {IEEE} 30th {International} {Conference} on {Data} {Engineering}},
	doi = {10.1109/ICDE.2014.6816693},
	file = {IEEE Xplore Abstract Record:/Users/pandey/Zotero/storage/6T8LPZLE/6816693.html:text/html;IEEE Xplore Full Text PDF:/Users/pandey/Zotero/storage/9C8UJNU7/Han et al. - 2014 - Scalable serializable snapshot isolation for multi.pdf:application/pdf},
	keywords = {Engines, Multicore processing, Concurrency control, Databases, Latches, Scalability, Silicon},
	month = mar,
	note = {ISSN: 2375-026X},
	pages = {700--711},
	title = {Scalable serializable snapshot isolation for multicore systems},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/ICDE.2014.6816693}}

@article{sadoghi_reducing_2014,
	abstract = {In multi-version databases, updates and deletions of records by transactions require appending a new record to tables rather than performing in-place updates. This mechanism incurs non-negligible performance overhead in the presence of multiple indexes on a table, where changes need to be propagated to all indexes. Additionally, an uncommitted record update will block other active transactions from using the index to fetch the most recently committed values for the updated record. In general, in order to support snapshot isolation and/or multi-version concurrency, either each active transaction is forced to search a database temporary area (e.g., roll-back segments) to fetch old values of desired records, or each transaction is forced to scan the entire table to find the older versions of the record in a multi-version database (in the absence of specialized temporal indexes). In this work, we describe a novel kV-Indirection structure to enable efficient (parallelizable) optimistic and pessimistic multi-version concurrency control by utilizing the old versions of records (at most two versions of each record) to provide direct access to the recent changes of records without the need of temporal indexes. As a result, our technique results in higher degree of concurrency by reducing the clashes between readers and writers of data and avoiding extended lock delays. We have a working prototype of our concurrency model and kV-Indirection structure in a commercial database and conducted an extensive evaluation to demonstrate the benefits of our multi-version concurrency control, and we obtained orders of magnitude speed up over the single-version concurrency control.},
	author = {Sadoghi, Mohammad and Canim, Mustafa and Bhattacharjee, Bishwaranjan and Nagel, Fabian and Ross, Kenneth A.},
	doi = {10.14778/2733004.2733006},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/7EIYYLCC/Sadoghi et al. - 2014 - Reducing database locking contention through multi.pdf:application/pdf},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	month = aug,
	number = {13},
	pages = {1331--1342},
	title = {Reducing database locking contention through multi-version concurrency},
	url = {https://doi.org/10.14778/2733004.2733006},
	urldate = {2022-04-12},
	volume = {7},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.14778/2733004.2733006}}

@inproceedings{coulden_performance_2013,
	abstract = {Most performance evaluation studies of database systems are high level studies limited by the expressiveness of their modelling formalisms. In this paper, we illustrate the potential of Queueing Petri Nets as a successor of traditionally-adopted modelling formalisms in evaluating the complexities of database systems. This is demonstrated through the construction and analysis of a Queueing Petri Net model of table-level database locking. We show that this model predicts mean response times better than a corresponding Petri net model.},
	address = {New York, NY, USA},
	author = {Coulden, David and Osman, Rasha and Knottenbelt, William J.},
	booktitle = {Proceedings of the 4th {ACM}/{SPEC} {International} {Conference} on {Performance} {Engineering}},
	doi = {10.1145/2479871.2479919},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/QEU936CA/Coulden et al. - 2013 - Performance modelling of database contention using.pdf:application/pdf},
	isbn = {978-1-4503-1636-1},
	keywords = {database locking, performance modelling, queueing petri nets},
	month = apr,
	pages = {331--334},
	publisher = {Association for Computing Machinery},
	series = {{ICPE} '13},
	title = {Performance modelling of database contention using queueing petri nets},
	url = {https://doi.org/10.1145/2479871.2479919},
	urldate = {2022-04-12},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1145/2479871.2479919}}

@inproceedings{merkle_modelling_2014,
	abstract = {Databases are the origin of many performance problems found in transactional information systems. Performance suffers especially when databases employ locking to isolate concurrent transactions. Software performance models therefore need to reflect lock contention in order to be a credible source for guiding design decisions. We propose a hybrid simulation approach that integrates a novel locking model into the Palladio software architecture performance simulator. Our model operates on a row level and is tailored to be used with architecture-level performance models. An experimental evaluation leads to promising results close to the measured performance.},
	address = {New York, NY, USA},
	author = {Merkle, Philipp and Stier, Christian},
	booktitle = {Proceedings of the 5th {ACM}/{SPEC} international conference on {Performance} engineering},
	doi = {10.1145/2568088.2576762},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/HWDQ845I/Merkle and Stier - 2014 - Modelling database lock-contention in architecture.pdf:application/pdf},
	isbn = {978-1-4503-2733-6},
	keywords = {lock contention, database, palladio component model, performance prediction, simulation},
	month = mar,
	pages = {285--288},
	publisher = {Association for Computing Machinery},
	series = {{ICPE} '14},
	title = {Modelling database lock-contention in architecture-level performance simulation},
	url = {https://doi.org/10.1145/2568088.2576762},
	urldate = {2022-04-12},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1145/2568088.2576762}}

@article{yu_staring_2014,
	abstract = {Computer architectures are moving towards an era dominated by many-core machines with dozens or even hundreds of cores on a single chip. This unprecedented level of on-chip parallelism introduces a new dimension to scalability that current database management systems (DBMSs) were not designed for. In particular, as the number of cores increases, the problem of concurrency control becomes extremely challenging. With hundreds of threads running in parallel, the complexity of coordinating competing accesses to data will likely diminish the gains from increased core counts. 
To better understand just how unprepared current DBMSs are for future CPU architectures, we performed an evaluation of concurrency control for on-line transaction processing (OLTP) workloads on many-core chips. We implemented seven concurrency control algorithms on a main-memory DBMS and using computer simulations scaled our system to 1024 cores. Our analysis shows that all algorithms fail to scale to this magnitude but for different reasons. In each case, we identify fundamental bottlenecks that are independent of the particular database implementation and argue that even state-of-the-art DBMSs suffer from these limitations. We conclude that rather than pursuing incremental solutions, many-core chips may require a completely redesigned DBMS architecture that is built from ground up and is tightly coupled with the hardware.},
	author = {Yu, Xiangyao and Bezerra, George and Pavlo, Andrew and Devadas, Srinivas and Stonebraker, Michael},
	copyright = {Creative Commons Attribution-Noncommercial-Share Alike},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/YNCTWE4P/Yu et al. - 2014 - Staring into the abyss An evaluation of concurren.pdf:application/pdf;Snapshot:/Users/pandey/Zotero/storage/ISWBKDBH/100022.html:text/html},
	issn = {21508097},
	journal = {MIT web domain},
	language = {en\_US},
	month = nov,
	note = {Accepted: 2015-11-24T14:09:45Z Publisher: Association for Computing Machinery (ACM)},
	shorttitle = {Staring into the abyss},
	title = {Staring into the abyss: {An} evaluation of concurrency control with one thousand cores},
	url = {https://dspace.mit.edu/handle/1721.1/100022},
	urldate = {2022-04-19},
	year = {2014},
	bdsk-url-1 = {https://dspace.mit.edu/handle/1721.1/100022}}

@inproceedings{bailis_feral_2015,
	abstract = {The rise of data-intensive ``Web 2.0'' Internet services has led to a range of popular new programming frameworks that collectively embody the latest incarnation of the vision of Object-Relational Mapping (ORM) systems, albeit at unprecedented scale. In this work, we empirically investigate modern ORM-backed applications' use and disuse of database concurrency control mechanisms. Specifically, we focus our study on the common use of feral, or application-level, mechanisms for maintaining database integrity, which, across a range of ORM systems, often take the form of declarative correctness criteria, or invariants. We quantitatively analyze the use of these mechanisms in a range of open source applications written using the Ruby on Rails ORM and find that feral invariants are the most popular means of ensuring integrity (and, by usage, are over 37 times more popular than transactions). We evaluate which of these feral invariants actually ensure integrity (by usage, up to 86.9\%) and which---due to concurrency errors and lack of database support---may lead to data corruption (the remainder), which we experimentally quantify. In light of these findings, we present recommendations for database system designers for better supporting these modern ORM programming patterns, thus eliminating their adverse effects on application integrity.},
	address = {Melbourne Victoria Australia},
	author = {Bailis, Peter and Fekete, Alan and Franklin, Michael J. and Ghodsi, Ali and Hellerstein, Joseph M. and Stoica, Ion},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	doi = {10.1145/2723372.2737784},
	file = {Bailis et al. - 2015 - Feral Concurrency Control An Empirical Investigat.pdf:/Users/pandey/Zotero/storage/NZ9CIDKH/Bailis et al. - 2015 - Feral Concurrency Control An Empirical Investigat.pdf:application/pdf},
	isbn = {978-1-4503-2758-9},
	language = {en},
	month = may,
	pages = {1327--1342},
	publisher = {ACM},
	shorttitle = {Feral {Concurrency} {Control}},
	title = {Feral {Concurrency} {Control}: {An} {Empirical} {Investigation} of {Modern} {Application} {Integrity}},
	url = {https://dl.acm.org/doi/10.1145/2723372.2737784},
	urldate = {2022-04-19},
	year = {2015},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/2723372.2737784},
	bdsk-url-2 = {https://doi.org/10.1145/2723372.2737784}}

@article{agrawal_helper_2010,
	abstract = {Helper locks allow programs with large parallel critical sections, called parallel regions, to execute more efficiently by enlisting processors that might otherwise be waiting on the helper lock to aid in the execution of the parallel region. Suppose that a processor p is executing a parallel region A after having acquired the lock L protecting A. If another processor p′ tries to acquire L, then instead of blocking and waiting for p to complete A, processor p′ joins p to help it complete A. Additional processors not blocked on L may also help to execute A. The HELPER runtime system can execute fork-join computations augmented with helper locks and parallel regions. HELPER supports the unbounded nesting of parallel regions. We provide theoretical completion-time and space-usage bounds for a design of HELPER based on work stealing. Specifically, let V be the number of parallel regions in a computation, let T1 be its work, and let T∞ be its "aggregate span" --- the sum of the spans (critical-path lengths) of all its parallel regions. We prove that HELPER completes the computation in expected time O(T1/PP + T∞+ PV on P processors. This bound indicates that programs with a small number of highly parallel critical sections can attain linear speedup. For the space bound, we prove that HELPER completes a program using only O(PS1 stack space, where S1 is the sum, over all regions, of the stack space used by each region in a serial execution. Finally, we describe a prototype of HELPER implemented by modifying the Cilk multithreaded runtime system. We used this prototype to implement a concurrent hash table with a resize operation protected by a helper lock.},
	author = {Agrawal, Kunal and Leiserson, Charles E. and Sukha, Jim},
	doi = {10.1145/1837853.1693487},
	file = {Full Text:/Users/pandey/Zotero/storage/UQNB4TCD/Agrawal et al. - 2010 - Helper locks for fork-join parallel programming.pdf:application/pdf},
	issn = {0362-1340},
	journal = {ACM SIGPLAN Notices},
	keywords = {cilk, dynamic multithreading, helper lock, nested parallelism, parallel region, scheduling, work stealing},
	month = jan,
	number = {5},
	pages = {245--256},
	title = {Helper locks for fork-join parallel programming},
	url = {https://doi.org/10.1145/1837853.1693487},
	urldate = {2022-04-16},
	volume = {45},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1145/1837853.1693487}}

@article{aguilera_sinfonia_2009,
	abstract = {We propose a new paradigm for building scalable distributed systems. Our approach does not require dealing with message-passing protocols, a major complication in existing distributed systems. Instead, developers just design and manipulate data structures within our service called Sinfonia. Sinfonia keeps data for applications on a set of memory nodes, each exporting a linear address space. At the core of Sinfonia is a new minitransaction primitive that enables efficient and consistent access to data, while hiding the complexities that arise from concurrency and failures. Using Sinfonia, we implemented two very different and complex applications in a few months: a cluster file system and a group communication service. Our implementations perform well and scale to hundreds of machines.},
	author = {Aguilera, Marcos K. and Merchant, Arif and Shah, Mehul and Veitch, Alistair and Karamanolis, Christos},
	doi = {10.1145/1629087.1629088},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/G5HEEE9M/Aguilera et al. - 2009 - Sinfonia A new paradigm for building scalable dis.pdf:application/pdf},
	issn = {0734-2071},
	journal = {ACM Transactions on Computer Systems},
	keywords = {transactions, Distributed systems, fault tolerance, scalability, shared memory, two-phase commit},
	month = nov,
	number = {3},
	pages = {5:1--5:48},
	shorttitle = {Sinfonia},
	title = {Sinfonia: {A} new paradigm for building scalable distributed systems},
	url = {https://doi.org/10.1145/1629087.1629088},
	urldate = {2022-04-21},
	volume = {27},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1145/1629087.1629088}}

@inproceedings{alvaro_blazes_2014,
	abstract = {Distributed consistency is perhaps the most discussed topic in distributed systems today. Coordination protocols can ensure consistency, but in practice they cause undesirable performance unless used judiciously. Scalable distributed architectures avoid coordination whenever possible, but undercoordinated systems can exhibit behavioral anomalies under fault, which are often extremely difficult to debug. This raises significant challenges for distributed system architects and developers. In this paper we present BLAZES, a cross-platform program analysis framework that (a) identifies program locations that require coordination to ensure consistent executions, and (b) automatically synthesizes application-specific coordination code that can significantly outperform general-purpose techniques. We present two case studies, one using annotated programs in the Twitter Storm system, and another using the Bloom declarative language.},
	author = {Alvaro, Peter and Conway, Neil and Hellerstein, Joseph M. and Maier, David},
	booktitle = {2014 {IEEE} 30th {International} {Conference} on {Data} {Engineering}},
	doi = {10.1109/ICDE.2014.6816639},
	file = {Full Text:/Users/pandey/Zotero/storage/N64JRWGG/Alvaro et al. - 2014 - Blazes Coordination analysis for distributed prog.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/pandey/Zotero/storage/S739VTB2/6816639.html:text/html},
	keywords = {Servers, Fault tolerance, Fault tolerant systems, Semantics, Storms, Topology, Twitter},
	month = mar,
	note = {ISSN: 2375-026X},
	pages = {52--63},
	shorttitle = {Blazes},
	title = {Blazes: {Coordination} analysis for distributed programs},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/ICDE.2014.6816639}}

@inproceedings{li_automating_2014,
	author = {Li, Cheng and Leit{\~a}o, Joao and Clement, Allen and Pregui{\c c}a, Nuno and Rodrigues, Rodrigo and Vafeiadis, Viktor},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/YVAPI2LS/Li et al. - 2014 - Automating the Choice of Consistency Levels in Rep.pdf:application/pdf;Snapshot:/Users/pandey/Zotero/storage/8KEMIN9N/li_cheng_2.html:text/html},
	isbn = {978-1-931971-10-2},
	language = {en},
	pages = {281--292},
	title = {Automating the {Choice} of {Consistency} {Levels} in {Replicated} {Systems}},
	url = {https://www.usenix.org/conference/atc14/technical-sessions/presentation/li_cheng_2},
	urldate = {2022-04-21},
	year = {2014},
	bdsk-url-1 = {https://www.usenix.org/conference/atc14/technical-sessions/presentation/li_cheng_2}}

@inproceedings{zellag_how_2012,
	abstract = {Current cloud datastores usually trade consistency for performance and availability. However, it is often not clear how an application is affected when it runs under a low level of consistency. In fact, current application designers have basically no tools that would help them to get a feeling of which and how many inconsistencies actually occur for their particular application. In this paper, we propose a generalized approach for detecting consistency anomalies for arbitrary cloud applications accessing various types of cloud datastores in transactional or non-transactional contexts. We do not require any knowledge on the business logic of the studied application nor on its selected consistency guarantees. We experimentally verify the effectiveness of our approach by using the Google App Engine and Cassandra datastores.},
	address = {San Jose, California},
	author = {Zellag, Kamal and Kemme, Bettina},
	booktitle = {Proceedings of the {Third} {ACM} {Symposium} on {Cloud} {Computing} - {SoCC} '12},
	doi = {10.1145/2391229.2391235},
	file = {Zellag and Kemme - 2012 - How consistent is your cloud application.pdf:/Users/pandey/Zotero/storage/Z84QLP6N/Zellag and Kemme - 2012 - How consistent is your cloud application.pdf:application/pdf},
	isbn = {978-1-4503-1761-0},
	language = {en},
	pages = {1--14},
	publisher = {ACM Press},
	title = {How \textit{consistent} is your cloud application?},
	url = {http://dl.acm.org/citation.cfm?doid=2391229.2391235},
	urldate = {2022-04-21},
	year = {2012},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=2391229.2391235},
	bdsk-url-2 = {https://doi.org/10.1145/2391229.2391235}}

@article{fekete_quantifying_2009,
	abstract = {Choosing a weak isolation level such as Read Committed is understood as a trade-off, where less isolation means that higher performance is gained but there is an increased possibility that data integrity will be lost. Previously, one side of this trade-off has been carefully studied quantitatively -- there are well-known metrics for performance such as transactions per minute, standardized benchmarks that measure these in a controlled way, and analytic models that can predict how performance is influenced by system parameters like multiprogramming level. This paper contributes to quantifying the other aspect of the trade-off. We define a novel microbenchmark that measures how rapidly integrity violations are produced at different isolation levels, for a simple set of transactions. We explore how this rate is impacted by configuration factors such as multiprogramming level, or contention frequency. For the isolation levels in multi-version platforms (Snapshot Isolation and the multiversion variant of Read Committed), we offer a simple probabilistic model that predicts the rate of integrity violations in our microbenchmark from configuration parameters. We validate the predictive model against measurements from the microbenchmark. The model identifies a region of the configuration space where a surprising inversion occurs: for these parameter settings, more integrity violations happen with Snapshot Isolation than with multi-version Read Committed, even though the latter is considered a lower isolation level.},
	author = {Fekete, Alan and Goldrei, Shirley N. and Asenjo, Jorge P{\'e}rez},
	doi = {10.14778/1687627.1687681},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/9CQG83ZB/Fekete et al. - 2009 - Quantifying isolation anomalies.pdf:application/pdf},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	month = aug,
	number = {1},
	pages = {467--478},
	title = {Quantifying isolation anomalies},
	url = {https://doi.org/10.14778/1687627.1687681},
	urldate = {2022-04-21},
	volume = {2},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.14778/1687627.1687681}}

@inproceedings{van_der_linde_legion_2017,
	abstract = {Many web applications are built around direct interactions among users, from collaborative applications and social networks to multi-user games. Despite being user-centric, these applications are usually supported by services running on servers that mediate all interactions among clients. When users are in close vicinity of each other, relying on a centralized infrastructure for mediating user interactions leads to unnecessarily high latency while hampering fault-tolerance and scalability. In this paper, we propose to extend user-centric Internet services with peer-to-peer interactions. We have designed a framework named Legion that enables client web applications to securely replicate data from servers, and synchronize these replicas directly among them. Legion allows for client-side modules, that we dub adapters, to leverage existing web platforms for storing data and to assist in Legion operation. Using these adapters, legacy applications accessing directly the web platforms can co-exist with new applications that use our framework, while accessing the same shared objects.Our experimental evaluation shows that, besides supporting direct client interactions, even when disconnected from the servers, Legion provides lower latency for update propagation with decreased network traffic for servers.},
	address = {Republic and Canton of Geneva, CHE},
	author = {van der Linde, Albert and Fouto, Pedro and Leit{\~a}o, Jo{\~a}o and Pregui{\c c}a, Nuno and Casti{\~n}eira, Santiago and Bieniusa, Annette},
	booktitle = {Proceedings of the 26th {International} {Conference} on {World} {Wide} {Web}},
	doi = {10.1145/3038912.3052673},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/95IBA6XP/van der Linde et al. - 2017 - Legion Enriching Internet Services with Peer-to-P.pdf:application/pdf},
	isbn = {978-1-4503-4913-0},
	keywords = {crdts, frameworks, peer-to-peer systems, web applications},
	month = apr,
	pages = {283--292},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {{WWW} '17},
	shorttitle = {Legion},
	title = {Legion: {Enriching} {Internet} {Services} with {Peer}-to-{Peer} {Interactions}},
	url = {https://doi.org/10.1145/3038912.3052673},
	urldate = {2022-04-21},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3038912.3052673}}

@inproceedings{ni_open_2007,
	abstract = {Transactional memory (TM) promises to simplify concurrent programming while providing scalability competitive to fine-grained locking. Language-based constructs allow programmers to denote atomic regions declaratively and to rely on the underlying system to provide transactional guarantees along with concurrency. In contrast with fine-grained locking, TM allows programmers to write simpler programs that are composable and deadlock-free. TM implementations operate by tracking loads and stores to memory and by detecting concurrent conflicting accesses by different transactions. By automating this process, they greatly reduce the programmer's burden, but they also are forced to be conservative. Incertain cases, conflicting memory accesses may not actually violate the higher-level semantics of a program, and a programmer may wish to allow seemingly conflicting transactions to execute concurrently. Open nested transactions enable expert programmers to differentiate between physical conflicts, at the level of memory, and logical conflicts that actually violate application semantics. A TMsystem with open nesting can permit physical conflicts that are not logical conflicts, and thus increase concurrency among application threads. Here we present an implementation of open nested transactions in a Java-based software transactional memory (STM)system. We describe new language constructs to support open nesting in Java, and we discuss new abstract locking mechanisms that a programmer can use to prevent logical conflicts. We demonstrate how these constructs can be mapped efficiently to existing STM data structures. Finally, we evaluate our system on a set of Java applications and data structures, demonstrating how open nesting can enhance application scalability.},
	address = {New York, NY, USA},
	author = {Ni, Yang and Menon, Vijay S. and Adl-Tabatabai, Ali-Reza and Hosking, Antony L. and Hudson, Richard L. and Moss, J. Eliot B. and Saha, Bratin and Shpeisman, Tatiana},
	booktitle = {Proceedings of the 12th {ACM} {SIGPLAN} symposium on {Principles} and practice of parallel programming},
	doi = {10.1145/1229428.1229442},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/Z8BXS5PB/Ni et al. - 2007 - Open nesting in software transactional memory.pdf:application/pdf},
	isbn = {978-1-59593-602-8},
	keywords = {abstract locks, transactional memory, nested transactions, open nesting},
	month = mar,
	pages = {68--78},
	publisher = {Association for Computing Machinery},
	series = {{PPoPP} '07},
	title = {Open nesting in software transactional memory},
	url = {https://doi.org/10.1145/1229428.1229442},
	urldate = {2022-04-25},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1145/1229428.1229442}}

@inproceedings{gray_dsls_2008,
	abstract = {A resurging interest in domain-specific languages (DSLs) has identified the benefits to be realized from customized languages that provide a high-level of abstraction for specifying a problem concept in a particular domain. Although there has been much success and interest reported by industry practitioners and academic researchers, there is much more work that is needed to enable further adoption of DSLs.},
	address = {Nashville, TN, USA},
	author = {Gray, Jeff and Fisher, Kathleen and Consel, Charles and Karsai, Gabor and Mernik, Marjan and Tolvanen, Juha-Pekka},
	booktitle = {Companion to the 23rd {ACM} {SIGPLAN} conference on {Object} oriented programming systems languages and applications - {OOPSLA} {Companion} '08},
	doi = {10.1145/1449814.1449863},
	file = {Gray et al. - 2008 - DSLs the good, the bad, and the ugly.pdf:/Users/pandey/Zotero/storage/ZN4AS36D/Gray et al. - 2008 - DSLs the good, the bad, and the ugly.pdf:application/pdf},
	isbn = {978-1-60558-220-7},
	language = {en},
	pages = {791},
	publisher = {ACM Press},
	shorttitle = {{DSLs}},
	title = {{DSLs}: the good, the bad, and the ugly},
	url = {http://portal.acm.org/citation.cfm?doid=1449814.1449863},
	urldate = {2022-05-01},
	year = {2008},
	bdsk-url-1 = {http://portal.acm.org/citation.cfm?doid=1449814.1449863},
	bdsk-url-2 = {https://doi.org/10.1145/1449814.1449863}}

@incollection{shrestha_optimality_2020,
	abstract = {Synchronous consensus protocols, by definition, have a worst-case commit latency that depends on the bounded network delay. The notion of optimistic responsiveness was recently introduced to allow synchronous protocols to commit instantaneously when some optimistic conditions are met. In this work, we revisit this notion of optimistic responsiveness and present optimal latency results. We present a lower bound for Byzantine Broadcast that relates the latency of optimistic and synchronous commits when the designated sender is honest and while the optimistic commit can tolerate some faults. We then present two matching upper bounds for tolerating f faults out of \$n = 2f+1\$ parties. Our first upper bound result achieves optimal optimistic and synchronous commit latency when the designated sender is honest and the optimistic commit can tolerate at least one fault. We experimentally evaluate this protocol and show that it achieves throughput comparable to state-of-the-art synchronous and partially synchronous protocols and under optimistic conditions achieves latency better than the state-of-the-art. Our second upper bound result achieves optimal optimistic and synchronous commit latency when the designated sender is honest but the optimistic commit does not tolerate any faults. The presence of matching lower and upper bound results make both of the results tight for \$n = 2f+1\$. Our upper bound results are presented in a state machine replication setting with a steady-state leader who is replaced with a view-change protocol when they do not make progress. For this setting, we also present an optimistically responsive protocol where the view-change protocol is optimistically responsive too.},
	address = {New York, NY, USA},
	author = {Shrestha, Nibesh and Abraham, Ittai and Ren, Ling and Nayak, Kartik},
	booktitle = {Proceedings of the 2020 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	isbn = {978-1-4503-7089-9},
	keywords = {byzantine fault tolerance, distributed computing, optimistic responsiveness, synchrony},
	month = oct,
	pages = {839--857},
	publisher = {Association for Computing Machinery},
	title = {On the {Optimality} of {Optimistic} {Responsiveness}},
	url = {https://doi.org/10.1145/3372297.3417284},
	urldate = {2022-04-30},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3372297.3417284}}

@inproceedings{yang_dpcp-p_2020,
	abstract = {Real-time scheduling and locking protocols are fundamental facilities to construct time-critical systems. For parallel real-time tasks, predictable locking protocols are required when concurrent sub-jobs mutually exclusive access to shared resources. This paper for the first time studies the distributed synchronization framework of parallel real-time tasks, where both tasks and global resources are partitioned to designated processors, and requests to each global resource are conducted on the processor on which the resource is partitioned. We extend the Distributed Priority Ceiling Protocol (DPCP) for parallel tasks under federated scheduling, with which we proved that a request can be blocked by at most one lower-priority request. We develop task and resource partitioning heuristics and propose analysis techniques to safely bound the task response times. Numerical evaluation (with heavy tasks on 8-, 16-, and 32-core processors) indicates that the proposed methods improve the schedulability significantly compared to the state-of-the-art locking protocols under federated scheduling.},
	author = {Yang, Maolin and Chen, Zewei and Jiang, Xu and Guan, Nan and Lei, Hang},
	booktitle = {2020 57th {ACM}/{IEEE} {Design} {Automation} {Conference} ({DAC})},
	doi = {10.1109/DAC18072.2020.9218584},
	file = {Submitted Version:/Users/pandey/Zotero/storage/FAAUN36B/Yang et al. - 2020 - DPCP-p A Distributed Locking Protocol for Paralle.pdf:application/pdf},
	keywords = {Protocols, Program processors, Real-time systems, Task analysis, Time factors, locking protocols, parallel tasks, Processor scheduling, real-time scheduling, Synchronization},
	month = jul,
	note = {ISSN: 0738-100X},
	pages = {1--6},
	shorttitle = {{DPCP}-p},
	title = {{DPCP}-p: {A} {Distributed} {Locking} {Protocol} for {Parallel} {Real}-{Time} {Tasks}},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/DAC18072.2020.9218584}}

@article{brandenburg_blocking_nodate,
	author = {Brandenburg, Bj{\"o}rn B},
	file = {Brandenburg - Blocking Optimality in Distributed Real-Time Locki.pdf:/Users/pandey/Zotero/storage/3SWINXDF/Brandenburg - Blocking Optimality in Distributed Real-Time Locki.pdf:application/pdf},
	language = {en},
	pages = {22},
	title = {Blocking {Optimality} in {Distributed} {Real}-{Time} {Locking} {Protocols}}}

@inproceedings{kalikar_numlock_2018,
	abstract = {We study multi-granularity locking in hierarchies. Existing popular mechanisms for multi-granularity locking work on two extremes: fine-grained locking which maximizes concurrency, and coarsegrained locking which minimizes the locking cost. Between the two extremes, there exist several pareto-optimal options which provide a trade-off between the concurrency cost and the locking cost. These options are captured under multi-granularity locking (MGL) protocol, but there exists no methodical way to choose a good MGL option. In this work, we present a locking technique, NumLock, which chooses optimal locking combination to serve MGL requests. NumLock works in two phases: first, it generates few pareto-optimal MGL options and second, it quickly analyzes these options to report the optimal one. We propose a greedy algorithm to generate a subset of the pareto-optimal options. We further improve the concurrency by abstracting the non-planar hierarchy as logical planar sub-hierarchies using multiple intervals. Our study reveals that the lock manager must explore various MGL options for achieving the best parallel performance, and that the extreme options often used in practice are suboptimal. We validate our claim quantitatively using an XML database as well as STMBench7, and show that the intermediate MGL options perform better compared to the existing state-of-the-art methods. In particular, NumLock achieves 65\% reduction in execution time for the XML database, and 25\% throughput improvement on STMBench7.},
	address = {Eugene OR USA},
	author = {Kalikar, Saurabh and Nasre, Rupesh},
	booktitle = {Proceedings of the 47th {International} {Conference} on {Parallel} {Processing}},
	doi = {10.1145/3225058.3225141},
	file = {Kalikar and Nasre - 2018 - NumLock Towards Optimal Multi-Granularity Locking.pdf:/Users/pandey/Zotero/storage/UVH9MP5T/Kalikar and Nasre - 2018 - NumLock Towards Optimal Multi-Granularity Locking.pdf:application/pdf},
	isbn = {978-1-4503-6510-9},
	language = {en},
	month = aug,
	pages = {1--10},
	publisher = {ACM},
	shorttitle = {{NumLock}},
	title = {{NumLock}: {Towards} {Optimal} {Multi}-{Granularity} {Locking} in {Hierarchies}},
	url = {https://dl.acm.org/doi/10.1145/3225058.3225141},
	urldate = {2022-05-02},
	year = {2018},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3225058.3225141},
	bdsk-url-2 = {https://doi.org/10.1145/3225058.3225141}}

@article{kalikar_domlock_2017,
	abstract = {We present efficient locking mechanisms for hierarchical data structures. Several applications work on an abstract hierarchy of objects, and a parallel execution on this hierarchy necessitates synchronization across workers operating on different parts of the hierarchy. Existing synchronization mechanisms are too coarse, too inefficient, or too ad hoc, resulting in reduced or unpredictable amount of concurrency. We propose a new locking approach based on the structural properties of the underlying hierarchy. We show that the developed techniques are efficient even when the hierarchy is an arbitrary graph. Theoretically, we present our approach as a locking-cost-minimizing instance of a generic algebraic model of synchronization for hierarchies. Using STMBench7, we illustrate considerable reduction in the locking cost, resulting in an average throughput improvement of 42\%.},
	author = {Kalikar, Saurabh and Nasre, Rupesh},
	doi = {10.1145/3127584},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/UPGK79V6/Kalikar and Nasre - 2017 - DomLock A New Multi-Granularity Locking Technique.pdf:application/pdf},
	issn = {2329-4949},
	journal = {ACM Transactions on Parallel Computing},
	keywords = {dominators, graphs, Hierarchical data structure, locking, object graphs, synchronization, trees},
	month = sep,
	number = {2},
	pages = {7:1--7:29},
	shorttitle = {{DomLock}},
	title = {{DomLock}: {A} {New} {Multi}-{Granularity} {Locking} {Technique} for {Hierarchies}},
	url = {https://doi.org/10.1145/3127584},
	urldate = {2022-05-02},
	volume = {4},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3127584}}

@inproceedings{kalikar_toggle_2019,
	abstract = {Rooted hierarchies are efficiently operated on using hierarchical tasks. Effective synchronization for hierarchies therefore demands hierarchical locks. State-of-the-art approaches for hierarchical locking are unaware of how tasks are scheduled. We propose a lock-contention aware task scheduler which considers the locking request while assigning tasks to threads. We present the design and implementation of Toggle, which exploits nested intervals and work-stealing to maximize throughput. Using widely used STMBench7 benchmark, a real-world XML hierarchy, and a state-of-the-art hierarchical locking protocol, we illustrate that Toggle considerably improves the overall application throughput.},
	address = {Cham},
	author = {Kalikar, Saurabh and Nasre, Rupesh},
	booktitle = {Euro-{Par} 2019: {Parallel} {Processing}},
	doi = {10.1007/978-3-030-29400-7_11},
	editor = {Yahyapour, Ramin},
	file = {Kalikar and Nasre - 2019 - Toggle Contention-Aware Task Scheduler for Concur.pdf:/Users/pandey/Zotero/storage/95FD56LB/Kalikar and Nasre - 2019 - Toggle Contention-Aware Task Scheduler for Concur.pdf:application/pdf},
	isbn = {978-3-030-29400-7},
	language = {en},
	pages = {142--155},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	shorttitle = {Toggle},
	title = {Toggle: {Contention}-{Aware} {Task} {Scheduler} for {Concurrent} {Hierarchical} {Operations}},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-29400-7_11}}

@article{guerraoui_stmbench7_2007,
	abstract = {Software transactional memory (STM) is a promising technique for controlling concurrency in modern multi-processor architectures. STM aims to be more scalable than explicit coarse-grained locking and easier to use than fine-grained locking. However, STM implementations have yet to demonstrate that their runtime overheads are acceptable. To date, empiric evaluations of these implementations have suffered from the lack of realistic benchmarks. Measuring performance of an STM in an overly simplified setting can be at best uninformative and at worst misleading as it may steer researchers to try to optimize irrelevant aspects of their implementations. This paper presents STMBench7: a candidate benchmark for evaluating STM implementations. The underlying data structure consists of a set of graphs and indexes intended to be suggestive of many complex applications, e.g., CAD/CAM. A collection of operations is supported to model a wide range of workloads and concurrency patterns. Companion locking strategies serve as a baseline for STM performance comparisons. STMBench7 strives for simplicity. Users may choose a workload, number of threads, benchmark length, as well as the possibility of structure modification and the nature of traversals of shared data structures. We illustrate the use of STMBench7 with an evaluation of a well-known software transactional memory implementation.},
	author = {Guerraoui, Rachid and Kapalka, Michal and Vitek, Jan},
	doi = {10.1145/1272998.1273029},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/6EN23SD6/Guerraoui et al. - 2007 - STMBench7 a benchmark for software transactional .pdf:application/pdf},
	issn = {0163-5980},
	journal = {ACM SIGOPS Operating Systems Review},
	keywords = {benchmarks, software transactional memory},
	month = mar,
	number = {3},
	pages = {315--324},
	shorttitle = {{STMBench7}},
	title = {{STMBench7}: a benchmark for software transactional memory},
	url = {https://doi.org/10.1145/1272998.1273029},
	urldate = {2022-05-05},
	volume = {41},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1145/1272998.1273029}}

@article{swati_novel_2021,
	abstract = {We present an efficient locking scheme in a hierarchical data structure. The existing multi-granularity locking mechanism works on two extremes: fine-grained locking through which concurrency is being maximized, and coarse grained locking that is being applied to minimize the locking cost. Between the two extremes, there lies several pare to-optimal options that provide a trade-off between the concurrency that can be attained. In this work, we present a locking technique, Collaborative Granular Version Locking (CGVL) which selects an optimal locking combination to serve locking requests in a hierarchical structure. In CGVL a series of version is being maintained at each granular level which allows the simultaneous execution of read and write operation on the data item. Our study reveals that in order to achieve optimal performance the lock manager explore various locking options by converting certain non-supporting locking modes into supporting locking modes thereby improving the existing compatibility matrix of multiple granularity locking protocol. Our claim is being quantitatively validated by using a Java Sun JDK environment, which shows that our CGVL perform better compared to the state-of-the-art existing MGL methods. In particular, CGVL attains 20\% reduction in execution time for the locking operation that are being carried out by considering, the following parameters: i) The number of threads ii) The number of locked object iii) The duration of critical section (CPU Cycles) which significantly supports the achievement of enhanced concurrency\&nbsp; in terms of\&nbsp; the number of concurrent read accesses.},
	author = {Swati, Ms and Bajaj, Dr Shalini Bhaskar and Jaglan, Dr Vivek},
	copyright = {Copyright (c) 2021},
	doi = {10.17762/itii.v9i1.221},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/I7W7K4H2/Swati et al. - 2021 - A NOVEL MULTI GRANULARITY LOCKING SCHEME BASED ON .pdf:application/pdf},
	issn = {2203-1731},
	journal = {INFORMATION TECHNOLOGY IN INDUSTRY},
	language = {en},
	month = mar,
	note = {Number: 1},
	number = {1},
	pages = {932--947},
	title = {A {NOVEL} {MULTI} {GRANULARITY} {LOCKING} {SCHEME} {BASED} {ON} {CONCURRENT} {MULTI} -{VERSION} {HIERARCHICAL} {STRUCTURE}},
	url = {http://it-in-industry.org/index.php/itii/article/view/221},
	urldate = {2022-05-05},
	volume = {9},
	year = {2021},
	bdsk-url-1 = {http://it-in-industry.org/index.php/itii/article/view/221},
	bdsk-url-2 = {https://doi.org/10.17762/itii.v9i1.221}}

@inproceedings{preguica_commutative_2009,
	abstract = {A Commutative Replicated Data Type (CRDT) is one where all concurrent operations commute. The replicas of a CRDT converge automatically, without complex concurrency control. This paper describes Treedoc, a novel CRDT design for cooperative text editing. An essential property is that the identifiers of Treedoc atoms are selected from a dense space. We discuss practical alternatives for implementing the identifier space based on an extended binary tree. We also discuss storage alternatives for data and meta-data, and mechanisms for compacting the tree. In the best case, Treedoc incurs no overhead with respect to a linear text buffer. We validate the results with traces from existing edit histories.},
	address = {Montreal, Quebec, Canada},
	author = {Preguica, Nuno and Marques, Joan Manuel and Shapiro, Marc and Letia, Mihai},
	booktitle = {2009 29th {IEEE} {International} {Conference} on {Distributed} {Computing} {Systems}},
	doi = {10.1109/ICDCS.2009.20},
	file = {Preguica et al. - 2009 - A Commutative Replicated Data Type for Cooperative.pdf:/Users/pandey/Zotero/storage/NPDNNKVE/Preguica et al. - 2009 - A Commutative Replicated Data Type for Cooperative.pdf:application/pdf},
	language = {en},
	month = jun,
	pages = {395--403},
	publisher = {IEEE},
	title = {A {Commutative} {Replicated} {Data} {Type} for {Cooperative} {Editing}},
	url = {http://ieeexplore.ieee.org/document/5158449/},
	urldate = {2022-05-09},
	year = {2009},
	bdsk-url-1 = {http://ieeexplore.ieee.org/document/5158449/},
	bdsk-url-2 = {https://doi.org/10.1109/ICDCS.2009.20}}

@article{korman_proof_2010,
	abstract = {This paper addresses the problem of locally verifying global properties. Several natural questions are studied, such as ``how expensive is local verification?'' and more specifically ``how expensive is local verification compared to computation?'' A suitable model is introduced in which these questions are studied in terms of the number of bits a node needs to communicate. In addition, approaches are presented for the efficient construction of schemes, and upper and lower bounds are established on the cost of schemes for multiple basic problems. The paper also studies the role and cost of unique identities in terms of impossibility and complexity.},
	author = {Korman, Amos and Kutten, Shay and Peleg, David},
	doi = {10.1007/s00446-010-0095-3},
	file = {Korman et al. - 2010 - Proof labeling schemes.pdf:/Users/pandey/Zotero/storage/QH62FRAH/Korman et al. - 2010 - Proof labeling schemes.pdf:application/pdf},
	issn = {0178-2770, 1432-0452},
	journal = {Distributed Computing},
	language = {en},
	month = may,
	number = {4},
	pages = {215--233},
	title = {Proof labeling schemes},
	url = {http://link.springer.com/10.1007/s00446-010-0095-3},
	urldate = {2022-05-09},
	volume = {22},
	year = {2010},
	bdsk-url-1 = {http://link.springer.com/10.1007/s00446-010-0095-3},
	bdsk-url-2 = {https://doi.org/10.1007/s00446-010-0095-3}}

@article{anju_multi-interval_2022,
	abstract = {Locking has been a predominant technique depended upon for achieving thread synchronization and ensuring
correctness in multi-threaded applications. It has been established that the concurrent applications working
with hierarchical data witness significant benefits due to multi-granularity locking (MGL) techniques compared
to either fine or coarse-grained locking. The de facto MGL technique used in hierarchical databases is intention
locks, which uses a traversal-based protocol for hierarchical locking. A recent MGL implementation, DomLock,
exploits interval numbering to balance the locking cost and concurrency, and outperforms intention locks for
non-tree-structured hierarchies. We observe, however, that depending upon the hierarchy structure and the
interval numbering, DomLock pessimistically declares subhierarchies to be locked when in reality they are not.
This increases the waiting time of locks, and in turn, reduces concurrency. To address this issue, we present
MID (Multi-Interval DomLock), a new technique to improve the degree of concurrency of interval-based
hierarchical locking. By adding additional intervals for each node, MID helps in reducing the unnecessary lock
rejections due to false positive lock status of sub-hierarchies. Unleashing the hidden opportunities to exploit
more concurrency allows the parallel threads to finish their operations quickly, leading to notable performance
improvement. We also show that with sufficient number of intervals, MID can avoid all the lock rejections
due to false positive lock status of nodes. MID is general, and can be applied to any arbitrary hierarchy of
trees, DAGs and cycles. It also works with dynamic hierarchies wherein the hierarchical structure undergoes
updates.We illustrate the effectiveness of MID using STMBench7, and with extensive experimental evaluation,
show that it leads to significant throughput improvement (up to 141\%, average 106\%) over DomLock.},
	author = {Anju, M A and Nasre, Rupesh},
	file = {Anju and Nasre - 2022 - Multi-Interval DomLock (MID) Towards Improving Co.pdf:/Users/pandey/Zotero/storage/XIGMPW5E/Anju and Nasre - 2022 - Multi-Interval DomLock (MID) Towards Improving Co.pdf:application/pdf},
	journal = {ACM Transactions on Parallel Computing},
	month = apr,
	number = {1},
	title = {Multi-{Interval} {DomLock} ({MID}): {Towards} {Improving} {Concurrency} in {Hierarchies}},
	volume = {1},
	year = {2022}}

@phdthesis{najafzadeh_analysis_2016,
	abstract = {Distributed databases take advantage of replication to bring data close to the client, and to always be available. the primary challenge for such databases is to ensure consistency. recent research provide hybrid consistency models that allow the database supports asynchronous updates by default, but synchronisation is available upon request. to help programmers exploit the hybrid consistency model, we propose a set of useful patterns,proof rules, and tool for proving integrity invariants of applications. in the first part, we study a sound proof rule that enables programmers to check whether the operations of a given application semantics maintain the application invariants under a given amount of parallelism. we have developed a smt-based tool that automates this proof, and verified several example applications using the tool. in the second part, we apply the above methodology to the design of a replicated file system.the main invariant is that the directory structure forms a tree. we study three alternative semantics for the file system. each exposes a different amount of parallelism, and different anomalies. using our tool-assisted rules, we check whether a specific file system semantics maintains the tree invariant, and derive an appropriate consistency protocol. in the third part of this thesis, we present three classes of invariants: equivalence, partial order, and single-item generic. each places some constraints over the state. each of these classes maps to a different storage-layer consistency property: respectively, atomicity, causal ordering, or total ordering.},
	author = {Najafzadeh, Mahsa},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/SXD6UULT/Najafzadeh - 2016 - The analysis and co-design of weakly-consistent ap.pdf:application/pdf;Snapshot:/Users/pandey/Zotero/storage/3ZB8UMKP/tel-01351187.html:text/html},
	language = {en},
	month = apr,
	school = {Universit{\'e} Pierre et Marie Curie - Paris VI},
	title = {The analysis and co-design of weakly-consistent applications},
	type = {phdthesis},
	url = {https://tel.archives-ouvertes.fr/tel-01351187},
	urldate = {2022-05-12},
	year = {2016},
	bdsk-url-1 = {https://tel.archives-ouvertes.fr/tel-01351187}}

@article{bouajjani_verifying_2017,
	abstract = {Causal consistency is one of the most adopted consistency criteria for distributed implementations of data structures. It ensures that operations are executed at all sites according to their causal precedence. We address the issue of verifying automatically whether the executions of an implementation of a data structure are causally consistent. We consider two problems: (1) checking whether one single execution is causally consistent, which is relevant for developing testing and bug finding algorithms, and (2) verifying whether all the executions of an implementation are causally consistent. We show that the first problem is NP-complete. This holds even for the read-write memory abstraction, which is a building block of many modern distributed systems. Indeed, such systems often store data in key-value stores, which are instances of the read-write memory abstraction. Moreover, we prove that, surprisingly, the second problem is undecidable, and again this holds even for the read-write memory abstraction. However, we show that for the read-write memory abstraction, these negative results can be circumvented if the implementations are data independent, i.e., their behaviors do not depend on the data values that are written or read at each moment, which is a realistic assumption. We prove that for data independent implementations, the problem of checking the correctness of a single execution w.r.t. the read-write memory abstraction is polynomial time. Furthermore, we show that for such implementations the set of non-causally consistent executions can be represented by means of a finite number of register automata. Using these machines as observers (in parallel with the implementation) allows to reduce polynomially the problem of checking causal consistency to a state reachability problem. This reduction holds regardless of the class of programs used for the implementation, of the number of read-write variables, and of the used data domain. It allows leveraging existing techniques for assertion/reachability checking to causal consistency verification. Moreover, for a significant class of implementations, we derive from this reduction the decidability of verifying causal consistency w.r.t. the read-write memory abstraction.},
	author = {Bouajjani, Ahmed and Enea, Constantin and Guerraoui, Rachid and Hamza, Jad},
	doi = {10.1145/3093333.3009888},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/8J55DCU8/Bouajjani et al. - 2017 - On verifying causal consistency.pdf:application/pdf},
	issn = {0362-1340},
	journal = {ACM SIGPLAN Notices},
	keywords = {causal consistency, distributed systems, model checking, static program analysis},
	month = jan,
	number = {1},
	pages = {626--638},
	title = {On verifying causal consistency},
	url = {https://doi.org/10.1145/3093333.3009888},
	urldate = {2022-05-16},
	volume = {52},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3093333.3009888}}

@misc{noauthor_lowest_nodate,
	doi = {10.1016/j.jalgor.2005.08.001},
	file = {Snapshot:/Users/pandey/Zotero/storage/C58PDNKW/S0196677405000854.html:text/html;Submitted Version:/Users/pandey/Zotero/storage/YZ8ZUCXK/doi10.1016j.jalgor.2005.08.001 Elsevier Enhanc.pdf:application/pdf},
	language = {en},
	shorttitle = {doi},
	title = {Lowest common ancestors in trees and directed acyclic graphs},
	url = {https://reader.elsevier.com/reader/sd/pii/S0196677405000854?token=D3B7AC33BD5181CEBA59292427CF99F93BBB702A7A2BBBFDBB5DFDEFAE70E3CFAC7C0231D8E504781EE65083DD1F1567&originRegion=eu-west-1&originCreation=20220516125255},
	urldate = {2022-05-16},
	bdsk-url-1 = {https://reader.elsevier.com/reader/sd/pii/S0196677405000854?token=D3B7AC33BD5181CEBA59292427CF99F93BBB702A7A2BBBFDBB5DFDEFAE70E3CFAC7C0231D8E504781EE65083DD1F1567&originRegion=eu-west-1&originCreation=20220516125255},
	bdsk-url-2 = {https://doi.org/10.1016/j.jalgor.2005.08.001}}

@article{fischer_new_2010,
	abstract = {We derive a new generalization of lowest common ancestors (LCAs) in dags, called the lowest single common ancestor (LSCA). We show how to preprocess a static dag in linear time such that subsequent LSCA-queries can be answered in constant time. The size is linear in the number of nodes. We also consider a ``fuzzy'' variant of LSCA that allows to compute a node that is only an LSCA of a given percentage of the query nodes. The space and construction time of our scheme for fuzzy LSCAs is linear, whereas the query time has a sub-logarithmic slow-down. This ``fuzzy'' algorithm is also applicable to LCAs in trees, with the same complexities.},
	author = {Fischer, Johannes and Huson, Daniel H.},
	doi = {10.1016/j.ipl.2010.02.014},
	file = {ScienceDirect Full Text PDF:/Users/pandey/Zotero/storage/S3MIKZP6/Fischer and Huson - 2010 - New common ancestor problems in trees and directed.pdf:application/pdf},
	issn = {0020-0190},
	journal = {Information Processing Letters},
	keywords = {Algorithms, Computational biology, Data structures},
	language = {en},
	month = apr,
	number = {8},
	pages = {331--335},
	title = {New common ancestor problems in trees and directed acyclic graphs},
	url = {https://www.sciencedirect.com/science/article/pii/S0020019010000487},
	urldate = {2022-05-16},
	volume = {110},
	year = {2010},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020019010000487},
	bdsk-url-2 = {https://doi.org/10.1016/j.ipl.2010.02.014}}

@article{czumaj_faster_2007,
	abstract = {We present two new methods for finding a lowest common ancestor (LCA) for each pair of vertices of a directed acyclic graph (dag) on n vertices and m edges. The first method is surprisingly natural and solves the all-pairs LCA problem for the input dag on n vertices and m edges in time O(nm). The second method relies on a novel reduction of the all-pairs LCA problem to the problem of finding maximum witnesses for Boolean matrix product. We solve the latter problem (and hence also the all-pairs LCA problem) in time O(n2+λ), where λ satisfies the equation ω(1,λ,1)=1+2λ and ω(1,λ,1) is the exponent of the multiplication of an n×nλ matrix by an nλ×n matrix. By the currently best known bounds on ω(1,λ,1), the running time of our algorithm is O(n2.575). Our algorithm improves the previously known O(n2.688) time-bound for the general all-pairs LCA problem in dags by Bender et al. Our additional contribution is a faster algorithm for solving the all-pairs lowest common ancestor problem in dags of small depth, where the depth of a dag is defined as the length of the longest path in the dag. For all dags of depth at most h≤nα, where α≈0.294, our algorithm runs in a time that is asymptotically the same as that required for multiplying two n×n matrices, that is, O(nω); we also prove that this running time is optimal even for dags of depth 1. For dags with depth h{\textgreater}nα, the running time of our algorithm is at most O(nω⋅h0.468). This algorithm is faster than our algorithm for arbitrary dags for all values of h≤n0.42.},
	author = {Czumaj, Artur and Kowaluk, Miros{\l}aw and Lingas, Andrzej},
	doi = {10.1016/j.tcs.2007.02.053},
	file = {ScienceDirect Full Text PDF:/Users/pandey/Zotero/storage/YBUH8Q6M/Czumaj et al. - 2007 - Faster algorithms for finding lowest common ancest.pdf:application/pdf},
	issn = {0304-3975},
	journal = {Theoretical Computer Science},
	keywords = {Time complexity, Directed acyclic graphs, Lowest common ancestors, Matrix multiplication},
	language = {en},
	month = jun,
	number = {1},
	pages = {37--46},
	series = {Automata, {Languages} and {Programming}},
	title = {Faster algorithms for finding lowest common ancestors in directed acyclic graphs},
	url = {https://www.sciencedirect.com/science/article/pii/S0304397507001569},
	urldate = {2022-05-16},
	volume = {380},
	year = {2007},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0304397507001569},
	bdsk-url-2 = {https://doi.org/10.1016/j.tcs.2007.02.053}}

@article{alstrup_nearest_2004,
	abstract = {Several papers describe linear time algorithms to preprocess a tree, in order to answer subsequent nearest common ancestor queries in constant time. Here, we survey these algorithms and related results. Whereas previous algorithms produce a linear space data structure, in this paper we address the problem of distributing the data structure into short labels associated with the nodes. Localized data structures received much attention recently as they play an important role for distributed applications such as routing. We conclude our survey with a new simple algorithm that labels in O(n) time all the nodes of an n-node rooted tree such that from the labels of any two nodes alone one can compute in constant time the label of their nearest common ancestor. The labels assigned by our algorithm are of size O(log n) bits.},
	author = {Alstrup, Stephen and Gavoille, Cyril and Kaplan, Haim and Rauhe, Theis},
	doi = {10.1007/s00224-004-1155-5},
	file = {Alstrup et al. - 2004 - Nearest Common Ancestors A Survey and a New Algor.pdf:/Users/pandey/Zotero/storage/DGZQS79N/Alstrup et al. - 2004 - Nearest Common Ancestors A Survey and a New Algor.pdf:application/pdf},
	issn = {1433-0490},
	journal = {Theory of Computing Systems},
	keywords = {Balance Binary Tree, Complete Binary Tree, Heavy Label, Label Scheme, Lower Common Ancestor},
	language = {en},
	month = may,
	number = {3},
	pages = {441--456},
	shorttitle = {Nearest {Common} {Ancestors}},
	title = {Nearest {Common} {Ancestors}: {A} {Survey} and a {New} {Algorithm} for a {Distributed} {Environment}},
	url = {https://doi.org/10.1007/s00224-004-1155-5},
	urldate = {2022-05-16},
	volume = {37},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1007/s00224-004-1155-5}}

@inproceedings{caminiti_engineering_2008,
	abstract = {We address the problem of labeling the nodes of a tree such that one can determine the identifier of the least common ancestor of any two nodes by looking only at their labels. This problem has application in routing and in distributed computing in peer-to-peer networks. A labeling scheme using Θ(log2n)-bit labels has been previously presented by Peleg. By engineering this scheme, we obtain a variety of data structures with the same asymptotic performances. We conduct a thorough experimental evaluation of all these data structures. Our results clearly show which variants achieve the best performances in terms of space usage, construction time, and query time.},
	address = {Berlin, Heidelberg},
	author = {Caminiti, Saverio and Finocchi, Irene and Petreschi, Rossella},
	booktitle = {Algorithms - {ESA} 2008},
	doi = {10.1007/978-3-540-87744-8_20},
	editor = {Halperin, Dan and Mehlhorn, Kurt},
	file = {Caminiti et al. - 2008 - Engineering Tree Labeling Schemes A Case Study on.pdf:/Users/pandey/Zotero/storage/8FDXDWYU/Caminiti et al. - 2008 - Engineering Tree Labeling Schemes A Case Study on.pdf:application/pdf},
	isbn = {978-3-540-87744-8},
	language = {en},
	pages = {234--245},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	shorttitle = {Engineering {Tree} {Labeling} {Schemes}},
	title = {Engineering {Tree} {Labeling} {Schemes}: {A} {Case} {Study} on {Least} {Common} {Ancestors}},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-540-87744-8_20}}

@inproceedings{peleg_informative_2000,
	abstract = {This paper introduces and studies the notion of informative labeling schemes for arbitrary graphs. Let f(W) be a function on subsets of vertices W. An f labeling scheme labels the vertices of a weighted graph G in such a way that f(W) can be inferred efficiently for any vertex subset W of G by merely inspecting the labels of the vertices of W, without having to use any additional information sources.},
	address = {Berlin, Heidelberg},
	author = {Peleg, David},
	booktitle = {Mathematical {Foundations} of {Computer} {Science} 2000},
	doi = {10.1007/3-540-44612-5_53},
	editor = {Nielsen, Mogens and Rovan, Branislav},
	file = {Peleg - 2005 - Informative labeling schemes for graphs.pdf:/Users/pandey/Zotero/storage/N2E26XPK/Peleg - 2005 - Informative labeling schemes for graphs.pdf:application/pdf},
	isbn = {978-3-540-44612-5},
	keywords = {Label Scheme, Distance Label, Graph Class, Steiner Tree, Weighted Graph},
	language = {en},
	pages = {579--588},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Informative {Labeling} {Schemes} for {Graphs}},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1007/3-540-44612-5_53}}

@book{nielsen_mathematical_2000,
	address = {Berlin Heidelberg},
	editor = {Nielsen, Mogens},
	file = {Nielsen - 2000 - Mathematical foundations of computer science 2000.pdf:/Users/pandey/Zotero/storage/4UJZS5VE/Nielsen - 2000 - Mathematical foundations of computer science 2000.pdf:application/pdf},
	isbn = {978-3-540-67901-1},
	language = {en},
	note = {Meeting Name: MFCS},
	number = {1893},
	publisher = {Springer},
	series = {Lecture notes in computer science},
	shorttitle = {Mathematical foundations of computer science 2000},
	title = {Mathematical foundations of computer science 2000: 25th international symposium ; proceedings},
	year = {2000}}

@inproceedings{wang_dual_2006,
	abstract = {Graph reachability is fundamental to a wide range of applications, including XML indexing, geographic navigation, Internet routing, ontology queries based on RDF/OWL, etc. Many applications involve huge graphs and require fast answering of reachability queries. Several reachability labeling methods have been proposed for this purpose. They assign labels to the vertices, such that the reachability between any two vertices may be decided using their labels only. For sparse graphs, 2-hop based reachability labeling schemes answer reachability queries efficiently using relatively small label space. However, the labeling process itself is often too time consuming to be practical for large graphs. In this paper, we propose a novel labeling scheme for sparse graphs. Our scheme ensures that graph reachability queries can be answered in constant time. Furthermore, for sparse graphs, the complexity of the labeling process is almost linear, which makes our algorithm applicable to massive datasets. Analytical and experimental results show that our approach is much more efficient than stateof- the-art approaches. Furthermore, our labeling method also provides an alternative scheme to tradeoff query time for label space, which further benefits applications that use tree-like graphs.},
	author = {Wang, Haixun and He, Hao and Yang, Jun and Yu, P.S. and Yu, J.X.},
	booktitle = {22nd {International} {Conference} on {Data} {Engineering} ({ICDE}'06)},
	doi = {10.1109/ICDE.2006.53},
	keywords = {Data analysis, Indexing, Internet, Labeling, Navigation, Ontologies, OWL, Resource description framework, Routing, XML},
	month = apr,
	note = {ISSN: 2375-026X},
	pages = {75--75},
	shorttitle = {Dual {Labeling}},
	title = {Dual {Labeling}: {Answering} {Graph} {Reachability} {Queries} in {Constant} {Time}},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/ICDE.2006.53}}

@inproceedings{gawrychowski_labeling_2018,
	abstract = {It is argued that the existing schemes can be reformulated as such, and it allows us to obtain clean and good bounds on the length of the labels, and shows that the size of any minor-universal tree for trees on \$n\$ nodes is \${\textbackslash}Omega(n{\textasciicircum}\{2.174\})\$. A labeling scheme for nearest common ancestors assigns a distinct binary string, called the label, to every node of a tree, so that given the labels of two nodes (and no further information about the topology of the tree) we can compute the label of their nearest common ancestor. The goal is to make the labels as short as possible. Alstrup, Gavoille, Kaplan, and Rauhe [Theor. Comput. Syst. 37(3):441-456 2004] showed that \$O({\textbackslash}log n)\$-bit labels are enough. More recently, Alstrup, Halvorsen, and Larsen [SODA 2014] refined this to only \$2.772{\textbackslash}log n\$, and provided a lower bound of \$1.008{\textbackslash}log n\$. 
We connect designing a labeling scheme for nearest common ancestors to the existence of a tree, called a minor-universal tree, that contains every tree on \$n\$ nodes as a topological minor. Even though it is not clear if a labeling scheme must be based on such a notion, we argue that the existing schemes can be reformulated as such, and it allows us to obtain clean and good bounds on the length of the labels. As the main upper bound, we show that \$2.318{\textbackslash}log n\$-bit labels are enough. Surprisingly, the notion of a minor-universal tree for binary trees on \$n\$ nodes has been already used in a different context by Hrubes et al. [CCC 2010], and Young, Chu, and Wong [J. ACM 46(3):416-435, 1999] introduced a closely related notion of a universal tree. On the lower bound side, we show that the size of any minor-universal tree for trees on \$n\$ nodes is \${\textbackslash}Omega(n{\textasciicircum}\{2.174\})\$. This highlights a natural limitation for all approaches based on such trees. Our lower bound technique also implies that the size of any universal tree in the sense of Young et al. is \${\textbackslash}Omega(n{\textasciicircum}\{2.185\})\$, thus dramatically improves their lower bound of \${\textbackslash}Omega(n{\textbackslash}log n)\$. We complement the existential results with a generic transformation that decreases the query time to constant in any scheme based on a minor-universal tree.},
	author = {Gawrychowski, Pawel and Kuhn, F. and Lopuszanski, J. and Panagiotou, K. and Su, Pascal},
	booktitle = {{SODA}},
	doi = {10.1137/1.9781611975031.166},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/94VSTE5K/Gawrychowski et al. - 2018 - Labeling Schemes for Nearest Common Ancestors thro.pdf:application/pdf},
	title = {Labeling {Schemes} for {Nearest} {Common} {Ancestors} through {Minor}-{Universal} {Trees}},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1137/1.9781611975031.166}}

@article{harel_fast_1984,
	abstract = {An algorithm for a random access machine with uniform cost measure (and a bound of \${\textbackslash}Omega ({\textbackslash}log n)\$ on the number of bits per word) that requires time per query and preprocessing time is presented, assuming that the collection of trees is static. We consider the following problem: Given a collection of rooted trees, answer on-line queries of the form, ``What is the nearest common ancester of vertices x and y?'' We show that any pointer machine that solves this problem requires \${\textbackslash}Omega ({\textbackslash}log {\textbackslash}log n)\$ time per query in the worst case, where n is the total number of vertices in the trees. On the other hand, we present an algorithm for a random access machine with uniform cost measure (and a bound of \${\textbackslash}Omega ({\textbackslash}log n)\$ on the number of bits per word) that requires \$O(1)\$ time per query and \$O(n)\$ preprocessing time, assuming that the collection of trees is static. For a version of the problem in which the trees can change between queries, we obtain an almost-linear-time (and linear-space) algorithm.},
	author = {Harel, D. and Tarjan, R.},
	doi = {10.1137/0213024},
	journal = {SIAM J. Comput.},
	title = {Fast {Algorithms} for {Finding} {Nearest} {Common} {Ancestors}},
	year = {1984},
	bdsk-url-1 = {https://doi.org/10.1137/0213024}}

@article{tian_contention-aware_2018,
	abstract = {Lock managers are among the most studied components in concurrency control and transactional systems. However, one question seems to have been generally overlooked: ``When there are multiple lock requests on the same object, which one(s) should be granted first?'' Nearly all existing systems rely on a FIFO (first in, first out) strategy to decide which transaction(s) to grant the lock to. In this paper, however, we show that the lock scheduling choices have significant ramifications on the overall performance of a transactional system. Despite the large body of research on job scheduling outside the database context, lock scheduling presents subtle but challenging requirements that render existing results on scheduling inapt for a transactional database. By carefully studying this problem, we present the concept of contention-aware scheduling, show the hardness of the problem, and propose novel lock scheduling algorithms (LDSF and bLDSF), which guarantee a constant factor approximation of the best scheduling. We conduct extensive experiments using a popular database on both TPC-C and a microbenchmark. Compared to FIFO---the default scheduler in most database systems---our bLDSF algorithm yields up to 300x speedup in overall transaction latency. Alternatively, our LDSF algorithm, which is simpler and achieves comparable performance to bLDSF, has already been adopted by open-source community, and was chosen as the default scheduling strategy in MySQL 8.0.3+.},
	author = {Tian, Boyu and Huang, Jiamin and Mozafari, Barzan and Schoenebeck, Grant},
	doi = {10.1145/3187009.3177740},
	file = {Tian et al. - 2018 - Contention-aware lock scheduling for transactional.pdf:/Users/pandey/Zotero/storage/ABLBAKLY/Tian et al. - 2018 - Contention-aware lock scheduling for transactional.pdf:application/pdf},
	issn = {21508097},
	journal = {Proceedings of the VLDB Endowment},
	language = {en},
	month = jan,
	number = {5},
	pages = {648--662},
	title = {Contention-aware lock scheduling for transactional databases},
	url = {http://dl.acm.org/citation.cfm?doid=3187009.3177740},
	urldate = {2022-05-30},
	volume = {11},
	year = {2018},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=3187009.3177740},
	bdsk-url-2 = {https://doi.org/10.1145/3187009.3177740}}

@article{bender_lowest_2005,
	abstract = {We study the problem of finding lowest common ancestors (LCA) in trees and directed acyclic graphs (DAGs). Specifically, we extend the LCA problem to DAGs and study the LCA variants that arise in this general setting. We begin with a clear exposition of Berkman and Vishkin's simple optimal algorithm for LCA in trees. Their ideas lay the foundation for our work on LCA problems in DAGs. We present an algorithm that finds all-pairs-representative LCA in DAGs in O˜(n2.688) operations, provide a transitive-closure lower bound for the all-pairs-representative-LCA problem, and develop an LCA-existence algorithm that preprocesses the DAG in transitive-closure time. We also present a suboptimal but practical O(n3) algorithm for all-pairs-representative LCA in DAGs that uses ideas from the optimal algorithms in trees and DAGs. Our results reveal a close relationship between the LCA, all-pairs-shortest-path, and transitive-closure problems. We conclude the paper with a short experimental study of LCA algorithms in trees and DAGs. Our experiments and source code demonstrate the elegance of the preprocessing-query algorithms for LCA in trees. We show that for most trees the suboptimal Θ(nlogn)-preprocessing Θ(1)-query algorithm should be preferred, and demonstrate that our proposed O(n3) algorithm for all-pairs-representative LCA in DAGs performs well in both low and high density DAGs.},
	author = {Bender, Michael A. and Farach-Colton, Mart{\'\i}n and Pemmasani, Giridhar and Skiena, Steven and Sumazin, Pavel},
	doi = {10.1016/j.jalgor.2005.08.001},
	file = {ScienceDirect Snapshot:/Users/pandey/Zotero/storage/BVKET4EA/S0196677405000854.html:text/html;Submitted Version:/Users/pandey/Zotero/storage/2DLU4H3E/Bender et al. - 2005 - Lowest common ancestors in trees and directed acyc.pdf:application/pdf},
	issn = {0196-6774},
	journal = {Journal of Algorithms},
	keywords = {Cartesian tree, Directed cyclic graph (DAG), Lowest common ancestor (LCA), Range minimum query (RMQ), Shortest path},
	language = {en},
	month = nov,
	number = {2},
	pages = {75--94},
	title = {Lowest common ancestors in trees and directed acyclic graphs},
	url = {https://www.sciencedirect.com/science/article/pii/S0196677405000854},
	urldate = {2022-05-30},
	volume = {57},
	year = {2005},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0196677405000854},
	bdsk-url-2 = {https://doi.org/10.1016/j.jalgor.2005.08.001}}

@article{silvano_iota_2020,
	abstract = {The emergence of distributed ledger technologies (DLT) and design limitations of Blockchain systems for some types of applications led to the development of cryptocurrency alternatives for various purposes. Iota is a cryptocurrency with a new architecture called Tangle, which promises high scalability, no fees, and near-instant transfers, focused on the Internet-of-Things (IoT) solutions. This paper aims to present a systematic community's visions of this new technology and to provide minimum background to understand the Iota Tangle and ecosystem generated by this distributed Ledger. The first parts of this article describe the ecosystem behind Iota, theoretical mathematical foundation, and its challenges and solutions for implementation. In the second part, we presented systematic research about Iota Tangle in academic databases: IEEE, ScienceDirect, Scopus, and Research Gate. We select the articles those of high impact which can be filtered with the H5-index indicator. This criterion aims to guarantee that the papers analyzed underwent a careful selection process, evaluated by peers. The methodology used helped have a global vision of Iota, including that this innovation is not only understood as a cryptocurrency but can be considered as a ``distributed communication protocol'', absence of fees, low latency, and low computational cost for sending transactions. However, there exist several challenges in the vanguard of development of this ledger. It could also be identified that this technology enables many possibilities, however, it is fundamental to understand the potentials and limitations of this ecosystem to generate the best use cases.},
	author = {Silvano, Wellington Fernandes and Marcelino, Roderval},
	doi = {10.1016/j.future.2020.05.047},
	file = {ScienceDirect Snapshot:/Users/pandey/Zotero/storage/ZKUPHMYZ/S0167739X19329048.html:text/html},
	issn = {0167-739X},
	journal = {Future Generation Computer Systems},
	keywords = {Distributed computing, DLT, IoT, Iota, Iota Tangle},
	language = {en},
	month = nov,
	pages = {307--319},
	shorttitle = {Iota {Tangle}},
	title = {Iota {Tangle}: {A} cryptocurrency to communicate {Internet}-of-{Things} data},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X19329048},
	urldate = {2022-06-01},
	volume = {112},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167739X19329048},
	bdsk-url-2 = {https://doi.org/10.1016/j.future.2020.05.047}}

@inproceedings{ezhilchelvan_design_2020,
	abstract = {A new concurrency control protocol for distributed graph databases is described. It avoids the introduction of certain types of inconsistencies by aborting vulnerable transactions. An approximate model that allows the computation of performance measures, including the fraction of aborted transactions, is developed. The accuracy of the approximations is assessed by comparing them with simulations, for a variety of parameter settings.},
	address = {Cham},
	author = {Ezhilchelvan, Paul and Mitrani, Isi and Waudby, Jack and Webber, Jim},
	booktitle = {Computer {Performance} {Engineering}},
	doi = {10.1007/978-3-030-44411-2_4},
	editor = {Gribaudo, Marco and Iacono, Mauro and Phung-Duc, Tuan and Razumchik, Rostislav},
	isbn = {978-3-030-44411-2},
	keywords = {Arbitration, Edge-order consistency, Graph databases, Reciprocal consistency, Simulation, Stochastic modelling},
	language = {en},
	pages = {50--64},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Design and {Evaluation} of an {Edge} {Concurrency} {Control} {Protocol} for {Distributed} {Graph} {Databases}},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-44411-2_4}}

@inproceedings{koloniari_transaction_2016,
	abstract = {Many graph databases, both open and proprietary, have been recently developed to efficiently store and manage graph structured data. As the volume of such data grows, graph databases most often offer distributed solutions implemented in a cloud infrastructure. In this paper, we focus on transaction management for such cloud-based graph databases. In particular, we use various graph databases as case studies to survey the different levels of transaction support and concurrency control protocols offered. We also study data distribution issues and replication protocols. Finally, we highlight open issues that need to be addressed in the future.},
	address = {Cham},
	author = {Koloniari, Georgia and Pitoura, Evaggelia},
	booktitle = {Algorithmic {Aspects} of {Cloud} {Computing}},
	doi = {10.1007/978-3-319-29919-8_8},
	editor = {Karydis, Ioannis and Sioutas, Spyros and Triantafillou, Peter and Tsoumakos, Dimitrios},
	isbn = {978-3-319-29919-8},
	keywords = {Consistency, Cloud computing, Graph database},
	language = {en},
	pages = {99--113},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Transaction {Management} for {Cloud}-{Based} {Graph} {Databases}},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-319-29919-8_8}}

@article{sestak_integrity_nodate,
	abstract = {Graph databases are becoming more and more popular as they represent a good alternative to relational databases for some problem scenarios. Searching a graph is sometimes very convenient, unlike writing complex SQL queries that require a table to be joined to itself several times. However, graph databases do not support all the constraints that are familiar and used in relational databases. In this paper, we discuss integrity constraints in graph databases and technical implementation issues that prevent these constraints from being specified.},
	author = {{\v S}estak, Martina and Rabuzin, Kornelije and Novak, Matija},
	file = {{\v S}estak et al. - Integrity constraints in graph databases -- impleme.pdf:/Users/pandey/Zotero/storage/WDY3WI2U/{\v S}estak et al. - Integrity constraints in graph databases -- impleme.pdf:application/pdf},
	language = {en},
	pages = {9},
	title = {Integrity constraints in graph databases -- implementation challenges}}

@misc{noauthor_concurrency_nodate,
	file = {Concurrency control in graph protocols by using edge locks | Proceedings of the 3rd ACM SIGACT-SIGMOD symposium on Principles of database systems:/Users/pandey/Zotero/storage/75SCWR6S/588011.html:text/html},
	title = {Concurrency control in graph protocols by using edge locks {\textbar} {Proceedings} of the 3rd {ACM} {SIGACT}-{SIGMOD} symposium on {Principles} of database systems},
	url = {https://dl.acm.org/doi/abs/10.1145/588011.588019},
	urldate = {2022-06-01},
	bdsk-url-1 = {https://dl.acm.org/doi/abs/10.1145/588011.588019}}

@techreport{nair_proving_2020,
	abstract = {To provide high availability in distributed systems, object replicas allow concurrent updates. Although replicas eventually converge, they may diverge temporarily, for instance when the network fails. This makes it difficult for the developer to reason about the object's properties , and in particular, to prove invariants over its state. For the sub-class of state-based distributed systems, we propose a proof methodology for establishing that a given object maintains a given invariant, taking into account any concurrency control. Our approach allows reasoning about individual operations separately. We demonstrate that our rules are sound, and we illustrate their use with some representative examples. We automate the rule using Boogie, an SMT-based tool.},
	author = {Nair, Sreeja S and Petri, Gustavo and Shapiro, Marc},
	file = {HAL PDF Full Text:/Users/pandey/Zotero/storage/RJM5CQ5V/Nair et al. - 2020 - Proving the safety of highly-available distributed.pdf:application/pdf},
	institution = {LIP6, Sorbonne Universit{\'e}, Inria, Paris, France ; Arm Research, Cambridge, UK},
	keywords = {Automatic verification, Consistency, Distributed application design, Tool support, Replicated objects},
	month = feb,
	title = {Proving the safety of highly-available distributed objects ({Extended} version)},
	type = {Other},
	url = {https://hal.archives-ouvertes.fr/hal-02492599},
	urldate = {2022-06-09},
	year = {2020},
	bdsk-url-1 = {https://hal.archives-ouvertes.fr/hal-02492599}}

@inproceedings{golan-gueta_automatic_2011,
	abstract = {We present a technique for automatically adding fine-grain locking to an abstract data type that is implemented using a dynamic forest -i.e., the data structures may be mutated, even to the point of violating forestness temporarily during the execution of a method of the ADT. Our automatic technique is based on Domination Locking, a novel locking protocol. Domination locking is designed specifically for software concurrency control, and in particular is designed for object-oriented software with destructive pointer updates. Domination locking is a strict generalization of existing locking protocols for dynamically changing graphs. We show our technique can successfully add fine-grain locking to libraries where manually performing locking is extremely challenging. We show that automatic fine-grain locking is more efficient than coarse-grain locking, and obtains similar performance to hand-crafted fine-grain locking.},
	address = {New York, NY, USA},
	author = {Golan-Gueta, Guy and Bronson, Nathan and Aiken, Alex and Ramalingam, G. and Sagiv, Mooly and Yahav, Eran},
	booktitle = {Proceedings of the 2011 {ACM} international conference on {Object} oriented programming systems languages and applications},
	doi = {10.1145/2048066.2048086},
	file = {Golan-Gueta et al. - Automatic Fine-Grain Locking using Shape Propertie.pdf:/Users/pandey/Zotero/storage/9BZYR7EJ/Golan-Gueta et al. - Automatic Fine-Grain Locking using Shape Propertie.pdf:application/pdf},
	isbn = {978-1-4503-0940-0},
	keywords = {locking protocol, atomicity, concurrency, reduction, serializability, synthesis},
	month = oct,
	pages = {225--242},
	publisher = {Association for Computing Machinery},
	series = {{OOPSLA} '11},
	title = {Automatic fine-grain locking using shape properties},
	url = {https://doi.org/10.1145/2048066.2048086},
	urldate = {2022-06-15},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1145/2048066.2048086}}

@article{noauthor_granularity_nodate,
	file = {Granularity of Locks and Degrees of Consistency in.pdf:/Users/pandey/Zotero/storage/XJB2PQVA/Granularity of Locks and Degrees of Consistency in.pdf:application/pdf},
	language = {en},
	pages = {30},
	title = {Granularity of {Locks} and {Degrees} of {Consistency} in a {Shared} {Data} {Base}}}

@incollection{hutchison_reasoning_2012,
	abstract = {A lock placement describes, for each heap location, which lock guards the location, and under what circumstances. We formalize methods for reasoning about lock placements, making precise the interacting obligations between the program, the organization of the heap, and the placement of locks. Our methods capture realistic and subtle situations, such as the placement and correct use of speculative locks and lock assignments that change dynamically with updates to the heap. We present results for flat heaps with no structure, tree-structured heaps and a language of DAG-shaped heaps with bounded in-degree.},
	address = {Berlin, Heidelberg},
	author = {Hawkins, Peter and Aiken, Alex and Fisher, Kathleen and Rinard, Martin and Sagiv, Mooly},
	booktitle = {Programming {Languages} and {Systems}},
	doi = {10.1007/978-3-642-28869-2_17},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Seidl, Helmut},
	file = {Hawkins et al. - 2012 - Reasoning about Lock Placements.pdf:/Users/pandey/Zotero/storage/4PKEGYS7/Hawkins et al. - 2012 - Reasoning about Lock Placements.pdf:application/pdf},
	isbn = {978-3-642-28868-5 978-3-642-28869-2},
	language = {en},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {336--356},
	publisher = {Springer Berlin Heidelberg},
	title = {Reasoning about {Lock} {Placements}},
	url = {http://link.springer.com/10.1007/978-3-642-28869-2_17},
	urldate = {2022-06-20},
	volume = {7211},
	year = {2012},
	bdsk-url-1 = {http://link.springer.com/10.1007/978-3-642-28869-2_17},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-642-28869-2_17}}

@misc{sang_scalable_nodate,
	abstract = {The basic actor model of computation supports highly concurrent applications in that actors can execute in individual threads and interact only via asynchronous message passing, which can be easily used to implement distributed applications. However, this makes it hard for programmers to reason about combinations of messages as opposed to individual messages, which is essential in many scenarios. This paper presents a variant of the actor model in which messages can be composed into atomic units, which are executed in a strictly serializable fashion, whilst still retaining a high degree of concurrency. In short, our model is based on an orchestration of actors along a directed acyclic graph which supports decentralized synchronization among actors based on their actual interaction. We demonstrate our model through examples, and present a formal semantics which faithfully characterizes the implementation of our model in the AEON language. We prove serializability and the absence of deadlocks in this semantics, and demonstrate the scalability of our model empirically.},
	author = {SANG, BO SANG and Petri, Guatavo and Saeida Ardekani, Masoud and Ravi, Srivatsan and Eugster, Patrick},
	file = {_.pdf:/Users/pandey/Zotero/storage/AVQ8S5NZ/_.pdf:application/pdf},
	title = {Scalable {Serializable} {Multi}-{Actor} {Programming}}}

@article{sang_scalable_2020,
	abstract = {BO SANG, Purdue University and Ant Group, USA PATRICK EUGSTER, USI Lugano, Switzerland, Purdue University, USA, and TU Darmstadt, Germany GUSTAVO PETRI, ARM Research Cambridge, United Kingdom SRIVATSAN RAVI, University of Southern California, USA PIERRE-LOUIS ROMAN, USI Lugano, Switzerland A major challenge in writing applications that execute across hosts, such as distributed online services, is to reconcile (a) parallelism (i.e., allowing components to execute independently on disjoint tasks), and (b) cooperation (i.e., allowing components to work together on common tasks). A good compromise between the two is vital to scalability, a core concern in distributed networked applications. The actor model of computation is a widely promoted programming model for distributed applications, as actors can execute in individual threads (parallelism) across different hosts and interact via asynchronous message passing (collaboration). However, this makes it hard for programmers to reason about combinations of messages as opposed to individual messages, which is essential in many scenarios. This paper presents a pragmatic variant of the actor model in which messages can be grouped into units that are executed in a serializable manner, whilst still retaining a high degree of parallelism. In short, our model is based on an orchestration of actors along a directed acyclic graph that supports efficient decentralized synchronization among actors based on their actual interaction. We present the implementation of this model, based on a dynamic DAG-inducing referencing discipline, in the actor-based programming language AEON. We argue serializability and the absence of deadlocks in our model, and demonstrate its scalability and usability through extensive evaluation and case studies of wide-ranging applications. CCS Concepts: · Computing methodologies → Distributed programming languages.},
	author = {Sang, Bo and Eugster, Patrick and Petri, Gustavo and Ravi, Srivatsan and Roman, Pierre-Louis},
	doi = {10.1145/3428266},
	file = {Sang et al. - 2020 - Scalable and serializable networked multi-actor pr.pdf:/Users/pandey/Zotero/storage/EK6HJAEX/Sang et al. - 2020 - Scalable and serializable networked multi-actor pr.pdf:application/pdf},
	issn = {2475-1421},
	journal = {Proceedings of the ACM on Programming Languages},
	language = {en},
	month = nov,
	number = {OOPSLA},
	pages = {1--30},
	title = {Scalable and serializable networked multi-actor programming},
	url = {https://dl.acm.org/doi/10.1145/3428266},
	urldate = {2022-06-25},
	volume = {4},
	year = {2020},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3428266},
	bdsk-url-2 = {https://doi.org/10.1145/3428266}}

@article{strecker_interactive_2018,
	abstract = {This article explores methods to provide computer support for reasoning about graph transformations. We first define a general framework for representing graphs, graph morphisms and single graph rewriting steps. This setup allows for interactively reasoning about graph transformations. In order to achieve a higher degree of automation, we identify fragments of the graph description language in which we can reduce reasoning about global graph properties to reasoning about local properties, involving only a bounded number of nodes, which can be decided by Boolean satisfiability solving or even by deterministic computation of low complexity.},
	author = {Strecker, Martin},
	doi = {10.1017/S096012951800021X},
	file = {Strecker - 2018 - Interactive and automated proofs for graph transfo.pdf:/Users/pandey/Zotero/storage/LUEJJYUU/Strecker - 2018 - Interactive and automated proofs for graph transfo.pdf:application/pdf},
	issn = {0960-1295, 1469-8072},
	journal = {Mathematical Structures in Computer Science},
	language = {en},
	month = sep,
	number = {8},
	pages = {1333--1362},
	title = {Interactive and automated proofs for graph transformations},
	url = {https://www.cambridge.org/core/product/identifier/S096012951800021X/type/journal_article},
	urldate = {2022-06-27},
	volume = {28},
	year = {2018},
	bdsk-url-1 = {https://www.cambridge.org/core/product/identifier/S096012951800021X/type/journal_article},
	bdsk-url-2 = {https://doi.org/10.1017/S096012951800021X}}

@inproceedings{swalens_chocola_2018,
	abstract = {Developers often combine different concurrency models in a single program, in each part of the program using the model that fits best. Many programming languages, such as Clojure, Scala, and Haskell, cater to this need by supporting different concurrency models. However, they are often combined in an ad hoc way and the semantics of the combination is not always well defined. This paper studies the combination of three concurrency models: futures, actors, and transactions. We show that a naive combination of these models invalidates the guarantees they normally provide, thereby breaking the assumptions of developers. Hence, we present Chocola: a unified framework of futures, actors, and transactions that maintains the guarantees of all models wherever possible, even when they are combined. We present the semantics of this model and its implementation in Clojure, and have evaluated its performance and expressivity using three benchmark applications.},
	address = {New York, NY, USA},
	author = {Swalens, Janwillem and De Koster, Joeri and De Meuter, Wolfgang},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN} {International} {Workshop} on {Programming} {Based} on {Actors}, {Agents}, and {Decentralized} {Control}},
	doi = {10.1145/3281366.3281373},
	file = {Submitted Version:/Users/pandey/Zotero/storage/4ZMTIE29/Swalens et al. - 2018 - Chocola integrating futures, actors, and transact.pdf:application/pdf},
	isbn = {978-1-4503-6066-1},
	keywords = {transactions, concurrency, actors, futures, parallelism, Software Transactional Memory},
	month = nov,
	pages = {33--43},
	publisher = {Association for Computing Machinery},
	series = {{AGERE} 2018},
	shorttitle = {Chocola},
	title = {Chocola: integrating futures, actors, and transactions},
	url = {https://doi.org/10.1145/3281366.3281373},
	urldate = {2022-07-02},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3281366.3281373}}

@misc{rusu_-depth_2019,
	abstract = {In this study, we present the first results of a complete implementation of the LDBC SNB benchmark -- interactive short, interactive complex, and business intelligence -- in two native graph database systems---Neo4j and TigerGraph. In addition to thoroughly evaluating the performance of all of the 46 queries in the benchmark on four scale factors -- SF-1, SF-10, SF-100, and SF-1000 -- and three computing architectures -- on premise and in the cloud -- we also measure the bulk loading time and storage size. Our results show that TigerGraph is consistently outperforming Neo4j on the majority of the queries---by two or more orders of magnitude (100X factor) on certain interactive complex and business intelligence queries. The gap increases with the size of the data since only TigerGraph is able to scale to SF-1000---Neo4j finishes only 12 of the 25 business intelligence queries in reasonable time. Nonetheless, Neo4j is generally faster at bulk loading graph data up to SF-100. A key to our study is the active involvement of the vendors in the tuning of their platforms. In order to encourage reproducibility, we make all the code, scripts, and configuration parameters publicly available online.},
	author = {Rusu, Florin and Huang, Zhiyi},
	doi = {10.48550/arXiv.1907.07405},
	file = {arXiv Fulltext PDF:/Users/pandey/Zotero/storage/IW5X3A4K/Rusu and Huang - 2019 - In-Depth Benchmarking of Graph Database Systems wi.pdf:application/pdf;arXiv.org Snapshot:/Users/pandey/Zotero/storage/64S42S9R/1907.html:text/html},
	keywords = {Computer Science - Databases},
	month = jul,
	note = {Number: arXiv:1907.07405 arXiv:1907.07405 [cs]},
	publisher = {arXiv},
	title = {In-{Depth} {Benchmarking} of {Graph} {Database} {Systems} with the {Linked} {Data} {Benchmark} {Council} ({LDBC}) {Social} {Network} {Benchmark} ({SNB})},
	url = {http://arxiv.org/abs/1907.07405},
	urldate = {2022-07-02},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1907.07405},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.1907.07405}}

@inproceedings{kolomicenko_experimental_2013,
	abstract = {In the recent years a new type of NoSQL databases, called graph databases (GDBs), has gained significant popularity due to the increasing need of processing and storing data in the form of a graph. The objective of this paper is a research on possibilities and limitations of GDBs and conducting an experimental comparison of selected GDB implementations. For this purpose the requirements of a universal GDB benchmark have been formulated and an extensible benchmarking tool, called BlueBench, has been developed.},
	address = {New York, NY, USA},
	author = {Kolomi{\v c}enko, Vojt{\v e}ch and Svoboda, Martin and Ml{\'y}nkov{\'a}, Irena Holubov{\'a}},
	booktitle = {Proceedings of {International} {Conference} on {Information} {Integration} and {Web}-based {Applications} \& {Services}},
	doi = {10.1145/2539150.2539155},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/UYPC2A2V/Kolomi{\v c}enko et al. - 2013 - Experimental Comparison of Graph Databases.pdf:application/pdf},
	isbn = {978-1-4503-2113-6},
	keywords = {benchmarking, experimental comparison, graph databases, NoSQL databases},
	month = dec,
	pages = {115--124},
	publisher = {Association for Computing Machinery},
	series = {{IIWAS} '13},
	title = {Experimental {Comparison} of {Graph} {Databases}},
	url = {https://doi.org/10.1145/2539150.2539155},
	urldate = {2022-08-08},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1145/2539150.2539155}}

@inproceedings{szekeres_making_2018,
	abstract = {Ordering guarantees are often defined using abstract execution models [2, 8-11, 19, 22]. Unfortunately, these models are complex and make different assumptions about system semantics. As a result, researchers find it impossible to compare the ordering guarantees of coherence, consistency and isolation. This paper presents a simple, unified model for defining ordering guarantees that is sufficiently general to model a wide range of systems, including processor memory, distributed storage, and databases. We define a new single constraint relationship, result visibility, which formalizes the "appears to execute before" relationship between operations. Using only result visibility, we define more than 20 ordering guarantees from different research areas, including PRAM [17], snapshot isolation [6] and eventual consistency session guarantees [21]. To our knowledge, these definitions form the broadest survey of ordering guarantees using a single constraint in the current literature.},
	address = {New York, NY, USA},
	author = {Szekeres, Adriana and Zhang, Irene},
	booktitle = {Proceedings of the 5th {Workshop} on the {Principles} and {Practice} of {Consistency} for {Distributed} {Data}},
	doi = {10.1145/3194261.3194268},
	isbn = {978-1-4503-5655-8},
	month = apr,
	pages = {1--8},
	publisher = {Association for Computing Machinery},
	series = {{PaPoC} '18},
	shorttitle = {Making consistency more consistent},
	title = {Making consistency more consistent: a unified model for coherence, consistency and isolation},
	url = {https://doi.org/10.1145/3194261.3194268},
	urldate = {2022-08-26},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3194261.3194268}}

@inproceedings{vaikuntam_evaluation_2014,
	abstract = {Graph databases attempt to make the modelling and processing of highly interconnected data, easier and more efficient by representing a system as a graph-like structure of nodes and edges. The fundamental premise of Graph Databases, unlike relational, is explicit and distinct definition of relationships and direct, non-index based access of related nodes from a given node. Growth of graph databases happened in large part with a need for complex processing of interconnected documents in the WWW and the surge in social networking. What was initially proprietary research has now transformed into a plethora of commercial and open source products, increasingly being adopted outside the internet services industry. This paper attempts to evaluate several such contemporary Graph Databases from a subjective feature-based and empirical performance-based perspective.},
	address = {New York, NY, USA},
	author = {Vaikuntam, Aparna and Perumal, Vinodh Kumar},
	booktitle = {Proceedings of the 7th {ACM} {India} {Computing} {Conference}},
	doi = {10.1145/2675744.2675752},
	isbn = {978-1-60558-814-8},
	keywords = {graph databases, databases, empirical, evaluation, feature based, open source, performance based, subjective, survey},
	month = oct,
	pages = {1--10},
	publisher = {Association for Computing Machinery},
	series = {{COMPUTE} '14},
	title = {Evaluation of contemporary graph databases},
	url = {https://doi.org/10.1145/2675744.2675752},
	urldate = {2022-08-26},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1145/2675744.2675752}}

@inproceedings{waudby_preserving_2020,
	abstract = {Our earlier work identifies reciprocal consistency as an important property that must be preserved in distributed graph databases. It also demonstrates that a failure to do so seriously undermines the integrity of the database itself in the long term. Reciprocal consistency can be maintained as a part of enforcing any known isolation guarantee and such an enforcement is also known to lead to reduction in performance. Therefore, in practice, distributed graph databases are often built atop BASE databases with no isolation guarantees, benefiting from good performance but leaving them susceptible to corruption due to violations of reciprocal consistency. This paper designs and presents a lightweight, locking-free protocol and then evaluates the protocol's abilities to preserve reciprocal consistency and also offer good throughput. Our evaluations establish that the protocol can offer both integrity guarantees and sound performance when the value of its parameter is chosen appropriately.},
	address = {New York, NY, USA},
	author = {Waudby, Jack and Ezhilchelvan, Paul and Webber, Jim and Mitrani, Isi},
	booktitle = {Proceedings of the 7th {Workshop} on {Principles} and {Practice} of {Consistency} for {Distributed} {Data}},
	doi = {10.1145/3380787.3393675},
	file = {Submitted Version:/Users/pandey/Zotero/storage/HFLGRW32/Waudby et al. - 2020 - Preserving reciprocal consistency in distributed g.pdf:application/pdf},
	isbn = {978-1-4503-7524-5},
	keywords = {graph databases, concurrency control, reciprocal consistency},
	month = apr,
	pages = {1--7},
	publisher = {Association for Computing Machinery},
	series = {{PaPoC} '20},
	title = {Preserving reciprocal consistency in distributed graph databases},
	url = {https://doi.org/10.1145/3380787.3393675},
	urldate = {2022-08-26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3380787.3393675}}

@inproceedings{jouili_empirical_2013,
	abstract = {In recent years, more and more companies provide services that can not be anymore achieved efficiently using relational databases. As such, these companies are forced to use alternative database models such as XML databases, object-oriented databases, document-oriented databases and, more recently graph databases. Graph databases only exist for a few years. Although there have been some comparison attempts, they are mostly focused on certain aspects only. In this paper, we present a distributed graph database comparison framework and the results we obtained by comparing four important players in the graph databases market: Neo4j, Orient DB, Titan and DEX.},
	author = {Jouili, Salim and Vansteenberghe, Valentin},
	booktitle = {2013 {International} {Conference} on {Social} {Computing}},
	doi = {10.1109/SocialCom.2013.106},
	keywords = {Benchmark testing, Distributed databases, Servers, Biological system modeling, Database Benchmark, graph, graph database, graph traversal, Loading, Time measurement},
	month = sep,
	pages = {708--715},
	title = {An {Empirical} {Comparison} of {Graph} {Databases}},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/SocialCom.2013.106}}

@inproceedings{jain_overview_2013,
	abstract = {The change in technology and the form of data being used has led to massive changes in database technologies. Most applications today, represent data that are intensely associative i.e. structured graphs, texts, hypertexts, wikis, RDF and social networking. The traditional relational database model (RDBMS) has been proved to be ineffective while handling such associated data with problems regarding horizontal scalability and dynamic data handling. Graph database is one such database technology that is capable of handling such highly heterogeneous and dynamic data. In this paper we will be summarizing popular graph databases that are currently in use in many prestigious organisations.},
	author = {Jain, Ravinsingh and Iyengar, Srikant and Arora, Ananyaa},
	booktitle = {2013 {Fourth} {International} {Conference} on {Computing}, {Communications} and {Networking} {Technologies} ({ICCCNT})},
	doi = {10.1109/ICCCNT.2013.6850236},
	keywords = {Data models, Distributed databases, Graph database, Availability, Bigtable, Cassandra, Dynamo, Generators, Google, Object oriented modeling, Voldemort},
	month = jul,
	pages = {1--6},
	title = {Overview of popular graph databases},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/ICCCNT.2013.6850236}}

@inproceedings{reina_modeling_2020,
	abstract = {The enormous volume and high variety of information that is constantly produced by computing systems requires storage technologies able to provide high processing velocity and data quality. The suitability for modeling complex data and for delivering performance are characteristics that are making graph databases become very popular. However, existing limitations still prevent database management systems that adopt the graph model to fully ensure data consistency, given that the means for ensuring data consistency are usually nonexistent or at most very simple. This work intends to overcome this limitation by extending the support for defining and enforcing integrity constraints on graph databases, in order to prevent the graph to reach an inconsistent state and compromise the correctness of applications. The proposed integrity constraints are implemented on OrientDB. Experimental results show that the prototype implementation can improve the performance in comparison to verification of constraints on a client application.},
	address = {Cham},
	author = {Reina, F{\'a}bio and Huf, Alexis and Presser, Daniel and Siqueira, Frank},
	booktitle = {Database and {Expert} {Systems} {Applications}},
	doi = {10.1007/978-3-030-59003-1_18},
	editor = {Hartmann, Sven and K{\"u}ng, Josef and Kotsis, Gabriele and Tjoa, A. Min and Khalil, Ismail},
	isbn = {978-3-030-59003-1},
	keywords = {Data integrity, Graph databases, Data consistency, Integrity constraints, OrientDB},
	language = {en},
	pages = {269--284},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Modeling and {Enforcing} {Integrity} {Constraints} on {Graph} {Databases}},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-59003-1_18}}

@article{sasaki_graph_nodate,
	author = {Sasaki, Bryce Merkl and Chao, Joy and Howard, Rachel},
	file = {Sasaki et al. - Graph Databases for Beginners.pdf:/Users/pandey/Zotero/storage/SVI9PQR9/Sasaki et al. - Graph Databases for Beginners.pdf:application/pdf},
	language = {en},
	pages = {46},
	title = {Graph {Databases} for {Beginners}}}

@inproceedings{pokorny_conceptual_2016,
	abstract = {Comparing graph databases with traditional, e.g., relational databases, some important database features are often missing there. Particularly, a graph database schema including integrity constraints is not explicitly defined, also a conceptual modelling is not used at all. It is hard to check a consistency of the graph database, because almost no integrity constraints are defined. In the paper, we discuss these issues and present current possibilities and challenges in graph database modelling. Also a conceptual level of a graph database design is considered. We propose a sufficient conceptual model and show its relationship to a graph database model. We focus also on integrity constraints modelling functional dependencies between entity types, which reminds modelling functional dependencies known from relational databases and extend them to conditional functional dependencies.},
	address = {New York, NY, USA},
	author = {Pokorn{\'y}, Jaroslav},
	booktitle = {Proceedings of the 20th {International} {Database} {Engineering} \& {Applications} {Symposium}},
	doi = {10.1145/2938503.2938547},
	isbn = {978-1-4503-4118-9},
	keywords = {graph database, conditional functional dependencies, graph conceptual database model, graph conceptual schema, graph database model, graph database schema, integrity constraints},
	month = jul,
	pages = {370--377},
	publisher = {Association for Computing Machinery},
	series = {{IDEAS} '16},
	title = {Conceptual and {Database} {Modelling} of {Graph} {Databases}},
	url = {https://doi.org/10.1145/2938503.2938547},
	urldate = {2022-08-26},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1145/2938503.2938547}}

@article{thakker_supporting_nodate,
	author = {Thakker, Harsh Vrajeshkumar},
	file = {Thakker - On Supporting Interoperability between RDF and Pro.pdf:/Users/pandey/Zotero/storage/CEWE9PKE/Thakker - On Supporting Interoperability between RDF and Pro.pdf:application/pdf},
	language = {de},
	pages = {211},
	title = {On {Supporting} {Interoperability} between {RDF} and {Property} {Graph} {Databases}}}

@inproceedings{pacaci_we_2017,
	abstract = {With the advent of online social networks, there is an increasing demand for storage and processing of graph-structured data. Social networking applications pose new challenges to data management systems due to demand for real-time querying and manipulation of the graph structure. Recently, several systems specialized systems for graph-structured data have been introduced. However, whether we should abandon mature RDBMS technology for graph databases remains an ongoing discussion. In this paper we present an graph database benchmarking architecture built on the existing LDBC Social Network Benchmark. Our proposed architecture stresses the systems with an interactive transactional workload to better simulate the real-time nature of social networking applications. Using this improved architecture, we evaluated a selection of specialized graph databases, RDF stores, and RDBMSes adapted for graphs. We do not find that specialized graph databases provide definitively better performance.},
	address = {New York, NY, USA},
	author = {Pacaci, Anil and Zhou, Alice and Lin, Jimmy and {\"O}zsu, M. Tamer},
	booktitle = {Proceedings of the {Fifth} {International} {Workshop} on {Graph} {Data}-management {Experiences} \& {Systems}},
	doi = {10.1145/3078447.3078459},
	isbn = {978-1-4503-5038-9},
	month = may,
	pages = {1--7},
	publisher = {Association for Computing Machinery},
	series = {{GRADES}'17},
	shorttitle = {Do {We} {Need} {Specialized} {Graph} {Databases}?},
	title = {Do {We} {Need} {Specialized} {Graph} {Databases}? {Benchmarking} {Real}-{Time} {Social} {Networking} {Applications}},
	url = {https://doi.org/10.1145/3078447.3078459},
	urldate = {2022-08-26},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3078447.3078459}}

@inproceedings{pokorny_graph_2015,
	abstract = {Real world data offers a lot of possibilities to be represented as graphs. As a result we obtain undirected or directed graphs, multigraphs and hypergraphs, labelled or weighted graphs and their variants. A development of graph modelling brings also new approaches, e.g., considering constraints. Processing graphs in a database way can be done in many different ways. Some graphs can be represented as JSON or XML structures and processed by their native database tools. More generally, a graph database is specified as any storage system that provides index-free adjacency, i.e. an explicit graph structure. Graph database technology contains some technological features inherent to traditional databases, e.g. ACID properties and availability. Use cases of graph databases like Neo4j, OrientDB, InfiniteGraph, FlockDB, AllegroGraph, and others, document that graph databases are becoming a common means for any connected data. In Big Data era, important questions are connected with scalability for large graphs as well as scaling for read/write operations. For example, scaling graph data by distributing it in a network is much more difficult than scaling simpler data models and is still a work in progress. Still a challenge is pattern matching in graphs providing, in principle, an arbitrarily complex identity function. Mining complete frequent patterns from graph databases is also challenging since supporting operations are computationally costly. In this paper, we discuss recent advances and limitations in these areas as well as future directions.},
	address = {Cham},
	author = {Pokorn{\'y}, Jaroslav},
	booktitle = {Computer {Information} {Systems} and {Industrial} {Management}},
	doi = {10.1007/978-3-319-24369-6_5},
	editor = {Saeed, Khalid and Homenda, Wladyslaw},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/UI5JHIFB/Pokorn{\'y} - 2015 - Graph Databases Their Power and Limitations.pdf:application/pdf},
	isbn = {978-3-319-24369-6},
	keywords = {Graph database, Big graphs, Graph querying, Graph scalability, Graph storage},
	language = {en},
	pages = {58--69},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	shorttitle = {Graph {Databases}},
	title = {Graph {Databases}: {Their} {Power} and {Limitations}},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-319-24369-6_5}}

@phdthesis{gubichev_query_2015,
	abstract = {Graph data management has received a lot of attention in 
the last decade, fueled by rapid development of two vertical domains,
Linked Data and Social Media. This thesis deals with the database aspects of 
graph processing problems in these two domains. We present both the new query processing 
techniques (e.g., efficient shortest path estimation) and the novel query optimization
methods (in particular, for join ordering and cardinality estimation) for a 
broad range of graph databases.},
	author = {Gubichev, Andrey},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/4S8NYFVF/Gubichev - 2015 - Query Processing and Optimization in Graph Databas.pdf:application/pdf;Snapshot:/Users/pandey/Zotero/storage/UCZP7IDC/1238730.html:text/html},
	school = {Technische Universit{\"a}t M{\"u}nchen},
	title = {Query {Processing} and {Optimization} in {Graph} {Databases}},
	url = {https://mediatum.ub.tum.de/1238730},
	urldate = {2022-08-26},
	year = {2015},
	bdsk-url-1 = {https://mediatum.ub.tum.de/1238730}}

@inproceedings{kumar_kaliyar_graph_2015,
	abstract = {In the era of big data, data analytics, business intelligence database management plays a vital role from technical business management and research point of view. Over many decades, database management has been a topic of active research. There are different type of database management system have been proposed over a period of time but Relational Database Management System (RDBMS) is the one which has been most popularly used in academic research as well as industrial setup[1]. In recent years, graph databases regained interest among the researchers for certain obvious reasons. One of the most important reasons for such an interest in a graph database is because of the inherent property of graphs as a graph structure. Graphs are present everywhere in the data structure, which represents the strong connectivity within the data. Most of the graph database models are defined in which data-structure for schema and instances are modeled as graph or generalization of a graph. In such graph database models, data manipulations are expressed by graph-oriented operations and type constructors [9]. Now days, most of the real world applications can be modeled as a graph and one of the best real world examples is social or biological network. This paper gives an overview of the different type of graph databases, applications, and comparison between their models based on some properties.},
	author = {kumar Kaliyar, Rohit},
	booktitle = {Communication \& {Automation} {International} {Conference} on {Computing}},
	doi = {10.1109/CCAA.2015.7148480},
	file = {IEEE Xplore Abstract Record:/Users/pandey/Zotero/storage/65R5TSMG/7148480.html:text/html},
	keywords = {Data models, Databases, Data structures, Graph database, Biological system modeling, Object oriented modeling, big data analytics, Computational modeling, Java, RDBMS, type constructors},
	month = may,
	pages = {785--790},
	shorttitle = {Graph databases},
	title = {Graph databases: {A} survey},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/CCAA.2015.7148480}}

@misc{chen_formal_2018,
	abstract = {Comparing provers on a formalization of the same problem is always a valuable exercise. In this paper, we present the formal proof of correctness of a non-trivial algorithm from graph theory that was carried out in three proof assistants: Why3, Coq, and Isabelle.},
	author = {Chen, Ran and Cohen, Cyril and Levy, Jean-Jacques and Merz, Stephan and Thery, Laurent},
	doi = {10.48550/arXiv.1810.11979},
	file = {arXiv Fulltext PDF:/Users/pandey/Zotero/storage/QH6YSV2R/Chen et al. - 2018 - Formal Proofs of Tarjan's Algorithm in Why3, Coq, .pdf:application/pdf;arXiv.org Snapshot:/Users/pandey/Zotero/storage/QYHK2EWR/1810.html:text/html},
	keywords = {Computer Science - Logic in Computer Science},
	month = oct,
	note = {arXiv:1810.11979 [cs]},
	publisher = {arXiv},
	title = {Formal {Proofs} of {Tarjan}'s {Algorithm} in {Why3}, {Coq}, and {Isabelle}},
	url = {http://arxiv.org/abs/1810.11979},
	urldate = {2022-09-01},
	year = {2018},
	bdsk-url-1 = {http://arxiv.org/abs/1810.11979},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.1810.11979}}

@article{lissandrini_beyond_2018,
	abstract = {Despite the increasing interest in graph databases their requirements and specifications are not yet fully understood by everyone, leading to a great deal of variation in the supported functionalities and the achieved performances. In this work, we provide a comprehensive study of the existing graph database systems. We introduce a novel microbenchmarking framework that provides insights on their performance that go beyond what macro-benchmarks can offer. The framework includes the largest set of queries and operators so far considered. The graph database systems are evaluated on synthetic and real data, from different domains, and at scales much larger than any previous work. The framework is materialized as an open-source suite and is easily extended to new datasets, systems, and queries1.},
	author = {Lissandrini, Matteo and Brugnara, Martin and Velegrakis, Yannis},
	doi = {10.14778/3297753.3297759},
	file = {Lissandrini et al. - 2018 - Beyond macrobenchmarks microbenchmark-based graph.pdf:/Users/pandey/Zotero/storage/XSHVF725/Lissandrini et al. - 2018 - Beyond macrobenchmarks microbenchmark-based graph.pdf:application/pdf;lissandrini-techreport-gdb.pdf:/Users/pandey/Zotero/storage/ZLEJNZYU/lissandrini-techreport-gdb.pdf:application/pdf},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	language = {en},
	month = dec,
	number = {4},
	pages = {390--403},
	shorttitle = {Beyond macrobenchmarks},
	title = {Beyond macrobenchmarks: microbenchmark-based graph database evaluation},
	url = {https://dl.acm.org/doi/10.14778/3297753.3297759},
	urldate = {2022-09-19},
	volume = {12},
	year = {2018},
	bdsk-url-1 = {https://dl.acm.org/doi/10.14778/3297753.3297759},
	bdsk-url-2 = {https://doi.org/10.14778/3297753.3297759}}

@misc{dauterman_reflections_2022,
	abstract = {Many systems today distribute trust across multiple parties such that the system provides certain security properties if a subset of the parties are honest. In the past few years, we have seen an explosion of academic and industrial cryptographic systems built on distributed trust, including secure multi-party computation applications (e.g., private analytics, secure learning, and private key recovery) and blockchains. These systems have great potential for improving security and privacy, but face a significant hurdle on the path to deployment. We initiate study of the following problem: a single organization is, by definition, a single party, and so how can a single organization build a distributed-trust system where corruptions are independent? We instead consider an alternative formulation of the problem: rather than ensuring that a distributed-trust system is set up correctly by design, what if instead, users can audit a distributed-trust deployment? We propose a framework that enables a developer to efficiently and cheaply set up any distributed-trust system in a publicly auditable way. To do this, we identify two application-independent building blocks that we can use to bootstrap arbitrary distributed-trust applications: secure hardware and an append-only log. We show how to leverage existing implementations of these building blocks to deploy distributed-trust systems, and we give recommendations for infrastructure changes that would make it easier to deploy distributed-trust systems in the future.},
	author = {Dauterman, Emma and Fang, Vivian and Crooks, Natacha and Popa, Raluca Ada},
	doi = {10.1145/3563766.3564089},
	file = {arXiv Fulltext PDF:/Users/pandey/Zotero/storage/WDYRC64W/Dauterman et al. - 2022 - Reflections on trusting distributed trust.pdf:application/pdf;arXiv.org Snapshot:/Users/pandey/Zotero/storage/RCNPHVD2/2210.html:text/html},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Cryptography and Security},
	month = oct,
	note = {arXiv:2210.08127 [cs]},
	title = {Reflections on trusting distributed trust},
	url = {http://arxiv.org/abs/2210.08127},
	urldate = {2022-10-27},
	year = {2022},
	bdsk-url-1 = {http://arxiv.org/abs/2210.08127},
	bdsk-url-2 = {https://doi.org/10.1145/3563766.3564089}}

@article{carey_007_1993,
	abstract = {The OO7 Benchmark represents a comprehensive test of OODBMS performance. In this paper we describe the benchmark and present performance results from its implementation in three OODBMS systems. It is our hope that the OO7 Benchmark will provide useful insight for end-users evaluating the performance of OODBMS systems; we also hope that the research community will find that OO7 provides a database schema, instance, and workload that is useful for evaluating new techniques and algorithms for OODBMS implementation.},
	author = {Carey, Michael J. and DeWitt, David J. and Naughton, Jeffrey F.},
	doi = {10.1145/170036.170041},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/PRLMPRIT/Carey et al. - 1993 - The 007 Benchmark.pdf:application/pdf},
	issn = {0163-5808},
	journal = {ACM SIGMOD Record},
	month = jun,
	number = {2},
	pages = {12--21},
	title = {The 007 {Benchmark}},
	url = {https://doi.org/10.1145/170036.170041},
	urldate = {2022-11-01},
	volume = {22},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1145/170036.170041}}

@inproceedings{erling_ldbc_2015,
	abstract = {The Linked Data Benchmark Council (LDBC) is now two years underway and has gathered strong industrial participation for its mission to establish benchmarks, and benchmarking practices for evaluating graph data management systems. The LDBC introduced a new choke-point driven methodology for developing benchmark workloads, which combines user input with input from expert systems architects, which we outline. This paper describes the LDBC Social Network Benchmark (SNB), and presents database benchmarking innovation in terms of graph query functionality tested, correlated graph generation techniques, as well as a scalable benchmark driver on a workload with complex graph dependencies. SNB has three query workloads under development: Interactive, Business Intelligence, and Graph Algorithms. We describe the SNB Interactive Workload in detail and illustrate the workload with some early results, as well as the goals for the two other workloads.},
	address = {New York, NY, USA},
	author = {Erling, Orri and Averbuch, Alex and Larriba-Pey, Josep and Chafi, Hassan and Gubichev, Andrey and Prat, Arnau and Pham, Minh-Duc and Boncz, Peter},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	doi = {10.1145/2723372.2742786},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/WCC7ACG2/Erling et al. - 2015 - The LDBC Social Network Benchmark Interactive Wor.pdf:application/pdf},
	isbn = {978-1-4503-2758-9},
	keywords = {benchmarking, graph databases, rdf databases},
	month = may,
	pages = {619--630},
	publisher = {Association for Computing Machinery},
	series = {{SIGMOD} '15},
	shorttitle = {The {LDBC} {Social} {Network} {Benchmark}},
	title = {The {LDBC} {Social} {Network} {Benchmark}: {Interactive} {Workload}},
	url = {https://doi.org/10.1145/2723372.2742786},
	urldate = {2022-12-05},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1145/2723372.2742786}}

@inproceedings{liu_unleashing_2014,
	abstract = {To implement the atomicity in accessing the irregular data structure, developers often use the coarse-grained locking because the hierarchical nature of the data structure makes the reasoning of fine-grained locking difficult and error-prone for the update of an ancestor field in the data structure may affect its descendants. The coarse-grained locking disallows the concurrent accesses to the entire data structure and leads to a low degree of concurrency. We propose an approach, built upon the Multiple Granularity Lock (MGL), that replaces the coarse-grained locks to unleash more concurrency for irregular data structures. Our approach is widely applicable and does not require the data structures to have special shapes. We produce the MGL locks through reasoning about the hierarchy of the data structure and the accesses to it. According to the evaluation results on widely used applications, our optimization brings the significant speedup, e.g., at least 7\%-20\% speedup and up to 2X speedup.},
	address = {Hyderabad India},
	author = {Liu, Peng and Zhang, Charles},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	doi = {10.1145/2568225.2568277},
	file = {Liu and Zhang - 2014 - Unleashing concurrency for irregular data structur.pdf:/Users/pandey/Zotero/storage/7PC5QYTS/Liu and Zhang - 2014 - Unleashing concurrency for irregular data structur.pdf:application/pdf},
	isbn = {978-1-4503-2756-5},
	language = {en},
	month = may,
	pages = {480--490},
	publisher = {ACM},
	title = {Unleashing concurrency for irregular data structures},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568277},
	urldate = {2022-12-19},
	year = {2014},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/2568225.2568277},
	bdsk-url-2 = {https://doi.org/10.1145/2568225.2568277}}

@inproceedings{gray_granularity_1975,
	abstract = {This paper proposes a locking protocol which associates locks with sets of resources. This protocol allows simultaneous locking at various granularities by different transactions. It is based on the introduction of additional lock modes besides the conventional share mode and exclusive mode. The protocol is generalized from simple hierarchies of locks to directed acyclic graphs of locks and to dynamic graphs of locks. The issues of scheduling and granting conflicting requests for the same resource are then discussed. Lastly, these ideas are compared with the lock mechanisms provided by existing data management systems.},
	address = {New York, NY, USA},
	author = {Gray, J. N. and Lorie, R. A. and Putzolu, G. R.},
	booktitle = {Proceedings of the 1st {International} {Conference} on {Very} {Large} {Data} {Bases}},
	doi = {10.1145/1282480.1282513},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/KIYB463C/Gray et al. - 1975 - Granularity of locks in a shared data base.pdf:application/pdf},
	isbn = {978-1-4503-3920-9},
	month = sep,
	pages = {428--451},
	publisher = {Association for Computing Machinery},
	series = {{VLDB} '75},
	title = {Granularity of locks in a shared data base},
	url = {https://doi.org/10.1145/1282480.1282513},
	urldate = {2023-01-06},
	year = {1975},
	bdsk-url-1 = {https://doi.org/10.1145/1282480.1282513}}

@inproceedings{carey_granularity_1983,
	abstract = {This paper shows that granularity hierarchies may be used with many types of concurrency control algorithms. Hierarchical versions of a validation algorithm, a timestamp algorithm, and a multiversion algorithm are given, and hierarchical algorithm issues relating to request escalation and distributed databases are discussed as well. It is argued that these hierarchical algorithms should offer improved performance for certain transaction mixes.},
	address = {New York, NY, USA},
	author = {Carey, Michael J.},
	booktitle = {Proceedings of the 2nd {ACM} {SIGACT}-{SIGMOD} symposium on {Principles} of database systems},
	doi = {10.1145/588058.588079},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/WII4YWSM/Carey - 1983 - Granularity hierarchies in concurrency control.pdf:application/pdf},
	isbn = {978-0-89791-097-2},
	month = mar,
	pages = {156--165},
	publisher = {Association for Computing Machinery},
	series = {{PODS} '83},
	title = {Granularity hierarchies in concurrency control},
	url = {https://doi.org/10.1145/588058.588079},
	urldate = {2023-02-02},
	year = {1983},
	bdsk-url-1 = {https://doi.org/10.1145/588058.588079}}

@inproceedings{devadas_scalable_2020,
	abstract = {Given continually increasing core counts, multiprocessor software scaling is critical. One set of applications that is especially difficult to parallelize efficiently are those that operate on hierarchical data. In such applications, correct execution relies on all threads coordinating their accesses within the hierarchy. At the same time, high-performance execution requires that this coordination be efficient and that it maximize parallelism. In this paper, we identify two key scalability bottlenecks in the coordination of hierarchical parallelism by studying the hierarchical data partitioning framework within the NetApp{\textregistered} WAFL{\textregistered} file system. We first observe that the global synchronization required to enforce the hierarchical constraints limits performance on increased core counts. We thus propose a distributed architecture, called Scheduler Pools, that divides the hierarchy into disjoint subhierarchies that can be managed independently in the common case, thereby reducing coordination overhead. We next observe that periodically draining all in-flight operations in order to facilitate the execution of coarse-grained operations in the hierarchy results in an excess of idle CPU cycles. To address this issue, we propose a new scheme, called Hierarchy-Aware Draining, that minimizes wasted CPU cycles by draining only those regions of the hierarchy that are required to execute the desired operation. When implemented together in the context of WAFL, Scheduler Pools and Hierarchy-Aware Draining overcome the observed scalability bottlenecks. Our evaluation with a range of benchmarks on high-end storage systems shows throughput gains of up to 33\% and reductions in latency of up to 64\%.},
	address = {New York, NY, USA},
	author = {Devadas, Vinay and Curtis-Maury, Matthew},
	booktitle = {Proceedings of the 49th {International} {Conference} on {Parallel} {Processing}},
	doi = {10.1145/3404397.3404398},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/FWE6RPY5/Devadas and Curtis-Maury - 2020 - Scalable Coordination of Hierarchical Parallelism.pdf:application/pdf},
	isbn = {978-1-4503-8816-0},
	month = aug,
	pages = {1--11},
	publisher = {Association for Computing Machinery},
	series = {{ICPP} '20},
	title = {Scalable {Coordination} of {Hierarchical} {Parallelism}},
	url = {https://doi.org/10.1145/3404397.3404398},
	urldate = {2023-02-02},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3404397.3404398}}

@phdthesis{nair_designing_2021,
	abstract = {Designing distributed applications involves a fundamental trade-off between safety and performance as described by CAP theorem. We focus on the cases where safety is the top requirement.For the subclass of state-based distributed systems, we propose a proof methodology for establishing that a given application maintains a given invariant. Our approach allows reasoning about individual operations separately. We demonstrate that our rules are sound, and with a mechanized proof engine, we illustrate their use with some representative examples. For conflicting operations, the developer can choose between conflict resolution or coordination. We present a novel replicated tree data structure that supports coordination-free concurrent atomic moves, and arguably maintains the tree invariant. Our analysis identifies cases where concurrent moves are inherently safe. For the remaining cases we devise a conflict resolution algorithm. The trade-off is that in some cases a move operation "loses". Given the coordination required by some application for safety, it can be implemented in many different ways. Even restricting to locks, they can use various configurations, differing by lock granularity, type, and placement. The performance of each configuration depends on workload. We study the "coordination lattice", i.e., design space of lock configurations, and define a set of metrics to systematically navigate them.},
	author = {Nair, Sreeja Sasidharan},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/C8L7M5YH/Nair - 2021 - Designing safe and highly available distributed ap.pdf:application/pdf},
	language = {en},
	month = jul,
	school = {Sorbonne Universit{\'e}},
	title = {Designing safe and highly available distributed applications},
	type = {phdthesis},
	url = {https://theses.hal.science/tel-03339393},
	urldate = {2023-02-02},
	year = {2021},
	bdsk-url-1 = {https://theses.hal.science/tel-03339393}}

@misc{viotti_consistency_2016,
	abstract = {Over the years, different meanings have been associated to the word consistency in the distributed systems community. While in the '80s ``consistency'' typically meant strong consistency, later defined also as linearizability, in recent years, with the advent of highly available and scalable systems, the notion of ``consistency'' has been at the same time both weakened and blurred.},
	author = {Viotti, Paolo and Vukoli{\'c}, Marko},
	file = {Viotti and Vukoli{\'c} - 2016 - Consistency in Non-Transactional Distributed Stora.pdf:/Users/pandey/Zotero/storage/GMASKGD8/Viotti and Vukoli{\'c} - 2016 - Consistency in Non-Transactional Distributed Stora.pdf:application/pdf},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, H.3.4},
	language = {en},
	month = apr,
	note = {arXiv:1512.00168 [cs]},
	publisher = {arXiv},
	title = {Consistency in {Non}-{Transactional} {Distributed} {Storage} {Systems}},
	url = {http://arxiv.org/abs/1512.00168},
	urldate = {2023-02-27},
	year = {2016},
	bdsk-url-1 = {http://arxiv.org/abs/1512.00168}}

@inproceedings{filliatre_why3_2013,
	abstract = {We present Why3, a tool for deductive program verification, and WhyML, its programming and specification language. WhyML is a first-order language with polymorphic types, pattern matching, and inductive predicates. Programs can make use of record types with mutable fields, type invariants, and ghost code. Verification conditions are discharged by Why3 with the help of various exist- ing automated and interactive theorem provers. To keep verification conditions tractable and comprehensible, WhyML imposes a static control of aliases that ob- viates the use of a memory model. A user can write WhyML programs directly and get correct-by-construction OCaml programs via an automated extraction mech- anism. WhyML is also used as an intermediate language for the verification of C, Java, or Ada programs. We demonstrate the benefits of Why3 and WhyML on non- trivial examples of program verification.},
	author = {Filli{\^a}tre, Jean-Christophe and Paskevich, Andrei},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/GMIAP582/Filli{\^a}tre and Paskevich - 2013 - Why3 -- Where Programs Meet Provers.pdf:application/pdf},
	language = {en},
	month = mar,
	publisher = {Springer},
	title = {Why3 -- {Where} {Programs} {Meet} {Provers}},
	url = {https://hal.inria.fr/hal-00789533},
	urldate = {2023-03-04},
	volume = {7792},
	year = {2013},
	bdsk-url-1 = {https://hal.inria.fr/hal-00789533}}

@misc{meirim_cise3_2020,
	abstract = {In this paper we present a tool for the formal analysis of applications built on top of replicated databases, where data integrity can be at stake. To address this issue, one can introduce synchronization in the system. Introducing synchronization in too many places can hurt the system's availability but if introduced in too few places, then data integrity can be compromised. The goal of our tool is to aid the programmer reason about the correct balance of synchronization in the system. Our tool analyses a sequential specification and deduces which operations require synchronization in order for the program to safely execute in a distributed environment. Our prototype is built on top of the deductive verification platform Why3, which provides a friendly and integrated user experience. Several case studies have been successfully verified using our tool.},
	author = {Meirim, Filipe and Pereira, M{\'a}rio and Ferreira, Carla},
	doi = {10.48550/arXiv.2010.06622},
	file = {arXiv Fulltext PDF:/Users/pandey/Zotero/storage/SYB7BEZM/Meirim et al. - 2020 - CISE3 Verifying Weakly Consistent Applications wi.pdf:application/pdf;arXiv.org Snapshot:/Users/pandey/Zotero/storage/75KPYINH/2010.html:text/html},
	keywords = {Computer Science - Programming Languages},
	month = oct,
	note = {arXiv:2010.06622 [cs]},
	publisher = {arXiv},
	shorttitle = {{CISE3}},
	title = {{CISE3}: {Verifying} {Weakly} {Consistent} {Applications} with {Why3}},
	url = {http://arxiv.org/abs/2010.06622},
	urldate = {2023-03-04},
	year = {2020},
	bdsk-url-1 = {http://arxiv.org/abs/2010.06622},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2010.06622}}

@article{farhan_fast_2021,
	abstract = {Finding the shortest-path distance between an arbitrary pair of vertices is a fundamental problem in graph theory. A tremendous amount of research has explored this problem, most of which is limited to static graphs. Due to the dynamic nature of real-world networks, such as social networks or web graphs in which a link between two entities may fail or become alive at any time, there is a pressing need to address this problem for dynamic networks. Existing work can only accommodate distance queries over moderately large dynamic networks due to high space cost and long pre-processing time required for constructing distance labelling, and even on such moderately large dynamic networks, distance labelling can hardly be updated efficiently. In this article, we propose a fully dynamic labelling method to efficiently update distance labelling so as to answer distance queries over large dynamic graphs. At its core, our proposed method incorporates two building blocks: (i) incremental algorithm for handling incremental update operations, i.e. edge insertions, and (ii) decremental algorithm for handling decremental update operations, i.e. edge deletions. These building blocks are built in a highly scalable framework of distance query answering. We theoretically prove the correctness of our fully dynamic labelling method and its preservation of the minimality of labelling. We have also evaluated on 13 real-world large complex networks to empirically verify the efficiency, scalability and robustness of our method.},
	author = {Farhan, Muhammad and Wang, Qing and Lin, Yu and McKay, Brendan},
	doi = {10.1007/s00778-021-00707-z},
	issn = {1066-8888},
	journal = {The VLDB Journal --- The International Journal on Very Large Data Bases},
	keywords = {Distance labelling, Dynamic graphs, Graph algorithms, Query processing},
	month = oct,
	number = {3},
	pages = {483--506},
	title = {Fast fully dynamic labelling for distance queries},
	url = {https://doi.org/10.1007/s00778-021-00707-z},
	urldate = {2023-03-08},
	volume = {31},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/s00778-021-00707-z}}

@inproceedings{angles_comparison_2012,
	abstract = {The limitations of traditional databases, in particular the relational model, to cover the requirements of current applications has lead the development of new database technologies. Among them, the Graph Databases are calling the attention of the database community because in trendy projects where a database is needed, the extraction of worthy information relies on processing the graph-like structure of the data. In this paper we present a systematic comparison of current graph database models. Our review includes general features (for data storing and querying), data modeling features (i.e., data structures, query languages, and integrity constraints), and the support for essential graph queries.},
	author = {Angles, Renzo},
	booktitle = {2012 {IEEE} 28th {International} {Conference} on {Data} {Engineering} {Workshops}},
	doi = {10.1109/ICDEW.2012.31},
	file = {Angles - 2012 - A Comparison of Current Graph Database Models.pdf:/Users/pandey/Zotero/storage/NDE6GPBR/Angles - 2012 - A Comparison of Current Graph Database Models.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/pandey/Zotero/storage/LFDQJYII/6313676.html:text/html},
	keywords = {Data models, Distributed databases, Data structures, Resource description framework, Database languages, Standards},
	month = apr,
	pages = {171--177},
	title = {A {Comparison} of {Current} {Graph} {Database} {Models}},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/ICDEW.2012.31}}

@article{tian_world_2023,
	abstract = {Rapidly growing social networks and other graph data have created a high demand for graph technologies in the market. A plethora of graph databases, systems, and solutions have emerged, as a result. On the other hand, graph has long been a well studied area in the database research community. Despite the numerous surveys on various graph research topics, there is a lack of survey on graph technologies from an industry perspective. The purpose of this paper is to provide the research community with an industrial perspective on the graph database landscape, so that graph researcher can better understand the industry trend and the challenges that the industry is facing, and work on solutions to help address these problems.},
	author = {Tian, Yuanyuan},
	doi = {10.1145/3582302.3582320},
	file = {Tian - 2023 - The World of Graph Databases from An Industry Pers.pdf:/Users/pandey/Zotero/storage/GHXCD964/Tian - 2023 - The World of Graph Databases from An Industry Pers.pdf:application/pdf},
	issn = {0163-5808},
	journal = {ACM SIGMOD Record},
	language = {en},
	month = jan,
	number = {4},
	pages = {60--67},
	title = {The {World} of {Graph} {Databases} from {An} {Industry} {Perspective}},
	url = {https://dl.acm.org/doi/10.1145/3582302.3582320},
	urldate = {2023-03-16},
	volume = {51},
	year = {2023},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3582302.3582320},
	bdsk-url-2 = {https://doi.org/10.1145/3582302.3582320}}

@misc{ozsu_graph_nodate,
	author = {{\"O}zsu, M. Tamer},
	file = {VLDB19-keynote.pdf:/Users/pandey/Zotero/storage/D4G62RXW/VLDB19-keynote.pdf:application/pdf},
	title = {Graph {Processing}: {A} {Panoramic} {View} and {Some} {Open} {Problems}},
	url = {https://cs.uwaterloo.ca/~tozsu/presentations/VLDB19-keynote.pdf},
	urldate = {2023-03-16},
	bdsk-url-1 = {https://cs.uwaterloo.ca/~tozsu/presentations/VLDB19-keynote.pdf}}

@article{sahu_ubiquity_2020,
	abstract = {Graph processing is becoming increasingly prevalent across many application domains. In spite of this prevalence, there is little research about how graphs are actually used in practice. We performed an extensive study that consisted of an online survey of 89 users, a review of the mailing lists, source repositories, and whitepapers of a large suite of graph software products, and in-person interviews with 6 users and 2 developers of these products. Our online survey aimed at understanding: (i) the types of graphs users have; (ii) the graph computations users run; (iii) the types of graph software users use; and (iv) the major challenges users face when processing their graphs. We describe the participants' responses to our questions highlighting common patterns and challenges. Based on our interviews and survey of the rest of our sources, we were able to answer some new questions that were raised by participants' responses to our online survey and understand the specific applications that use graph data and software. Our study revealed surprising facts about graph processing in practice. In particular, real-world graphs represent a very diverse range of entities and are often very large, scalability and visualization are undeniably the most pressing challenges faced by participants, and data integration, recommendations, and fraud detection are very popular applications supported by existing graph software. We hope these findings can guide future research.},
	author = {Sahu, Siddhartha and Mhedhbi, Amine and Salihoglu, Semih and Lin, Jimmy and {\"O}zsu, M. Tamer},
	doi = {10.1007/s00778-019-00548-x},
	file = {arXiv Fulltext PDF:/Users/pandey/Zotero/storage/D7YXA9N7/Sahu et al. - 2020 - The Ubiquity of Large Graphs and Surprising Challe.pdf:application/pdf;arXiv.org Snapshot:/Users/pandey/Zotero/storage/3EVW6CC5/1709.html:text/html},
	issn = {1066-8888, 0949-877X},
	journal = {The VLDB Journal},
	keywords = {Computer Science - Databases},
	month = may,
	note = {arXiv:1709.03188 [cs]},
	number = {2-3},
	pages = {595--618},
	shorttitle = {The {Ubiquity} of {Large} {Graphs} and {Surprising} {Challenges} of {Graph} {Processing}},
	title = {The {Ubiquity} of {Large} {Graphs} and {Surprising} {Challenges} of {Graph} {Processing}: {Extended} {Survey}},
	url = {http://arxiv.org/abs/1709.03188},
	urldate = {2023-03-16},
	volume = {29},
	year = {2020},
	bdsk-url-1 = {http://arxiv.org/abs/1709.03188},
	bdsk-url-2 = {https://doi.org/10.1007/s00778-019-00548-x}}

@article{boncz_sorry_nodate,
	author = {Boncz, Peter},
	file = {Boncz - The (sorry) State of Graph Database Systems.pdf:/Users/pandey/Zotero/storage/Y2PGFDYL/Boncz - The (sorry) State of Graph Database Systems.pdf:application/pdf},
	language = {en},
	title = {The (sorry) {State} of {Graph} {Database} {Systems}}}

@misc{angles_foundations_2017,
	abstract = {We survey foundational features underlying modern graph query languages. We first discuss two popular graph data models: edge-labelled graphs, where nodes are connected by directed, labelled edges; and property graphs, where nodes and edges can further have attributes. Next we discuss the two most fundamental graph querying functionalities: graph patterns and navigational expressions. We start with graph patterns, in which a graph-structured query is matched against the data. Thereafter we discuss navigational expressions, in which patterns can be matched recursively against the graph to navigate paths of arbitrary length; we give an overview of what kinds of expressions have been proposed, and how they can be combined with graph patterns. We also discuss several semantics under which queries using the previous features can be evaluated, what effects the selection of features and semantics has on complexity, and offer examples of such features in three modern languages that are used to query graphs: SPARQL, Cypher and Gremlin. We conclude by discussing the importance of formalisation for graph query languages; a summary of what is known about SPARQL, Cypher and Gremlin in terms of expressivity and complexity; and an outline of possible future directions for the area.},
	author = {Angles, Renzo and Arenas, Marcelo and Barcelo, Pablo and Hogan, Aidan and Reutter, Juan and Vrgoc, Domagoj},
	doi = {10.48550/arXiv.1610.06264},
	file = {arXiv Fulltext PDF:/Users/pandey/Zotero/storage/HAKPC849/Angles et al. - 2017 - Foundations of Modern Query Languages for Graph Da.pdf:application/pdf;arXiv.org Snapshot:/Users/pandey/Zotero/storage/5GT9C4P5/1610.html:text/html},
	keywords = {Computer Science - Databases},
	month = jun,
	note = {arXiv:1610.06264 [cs]},
	publisher = {arXiv},
	title = {Foundations of {Modern} {Query} {Languages} for {Graph} {Databases}},
	url = {http://arxiv.org/abs/1610.06264},
	urldate = {2023-03-16},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1610.06264},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.1610.06264}}

@inproceedings{waudby_towards_2021,
	abstract = {Verifying ACID compliance is an essential part of database benchmarking, because the integrity of performance results can be undermined as the performance benefits of operating with weaker safety guarantees (at the potential cost of correctness) are well known. Traditionally, benchmarks have specified a number of tests to validate ACID compliance. However, these tests have been formulated in the context of relational database systems and SQL, whereas our scope of benchmarking are systems for graph data, many of which are non-relational. This paper presents a set of data model-agnostic ACID compliance tests for the LDBC (Linked Data Benchmark Council) Social Network Benchmark suite's Interactive (SNB-I) workload, a transaction processing benchmark for graph databases. We test all ACID properties with a particular emphasis on isolation, covering 10 transaction anomalies in total. We present results from implementing the test suite on 5 database systems.},
	address = {Cham},
	author = {Waudby, Jack and Steer, Benjamin A. and Karimov, Karim and Marton, J{\'o}zsef and Boncz, Peter and Sz{\'a}rnyas, G{\'a}bor},
	booktitle = {Performance {Evaluation} and {Benchmarking}},
	doi = {10.1007/978-3-030-84924-5_1},
	editor = {Nambiar, Raghunath and Poess, Meikel},
	isbn = {978-3-030-84924-5},
	language = {en},
	pages = {1--17},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Towards {Testing} {ACID} {Compliance} in the {LDBC} {Social} {Network} {Benchmark}},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-84924-5_1}}

@incollection{nambiar_towards_2021,
	abstract = {Verifying ACID compliance is an essential part of database benchmarking, because the integrity of performance results can be undermined as the performance benefits of operating with weaker safety guarantees (at the potential cost of correctness) are well known. Traditionally, benchmarks have specified a number of tests to validate ACID compliance. However, these tests have been formulated in the context of relational database systems and SQL, whereas our scope of benchmarking are systems for graph data, many of which are non-relational. This paper presents a set of data model-agnostic ACID compliance tests for the LDBC (Linked Data Benchmark Council) Social Network Benchmark suite's Interactive (SNB-I) workload, a transaction processing benchmark for graph databases. We test all ACID properties with a particular emphasis on isolation, covering 10 transaction anomalies in total. We present results from implementing the test suite on 5 database systems.},
	address = {Cham},
	author = {Waudby, Jack and Steer, Benjamin A. and Karimov, Karim and Marton, J{\'o}zsef and Boncz, Peter and Sz{\'a}rnyas, G{\'a}bor},
	booktitle = {Performance {Evaluation} and {Benchmarking}},
	doi = {10.1007/978-3-030-84924-5_1},
	editor = {Nambiar, Raghunath and Poess, Meikel},
	file = {Waudby et al. - 2021 - Towards Testing ACID Compliance in the LDBC Social.pdf:/Users/pandey/Zotero/storage/R4PGH7N7/Waudby et al. - 2021 - Towards Testing ACID Compliance in the LDBC Social.pdf:application/pdf},
	isbn = {978-3-030-84923-8 978-3-030-84924-5},
	language = {en},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {1--17},
	publisher = {Springer International Publishing},
	title = {Towards {Testing} {ACID} {Compliance} in the {LDBC} {Social} {Network} {Benchmark}},
	url = {https://link.springer.com/10.1007/978-3-030-84924-5_1},
	urldate = {2023-03-16},
	volume = {12752},
	year = {2021},
	bdsk-url-1 = {https://link.springer.com/10.1007/978-3-030-84924-5_1},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-84924-5_1}}

@inproceedings{schiefer_merge_2022,
	abstract = {In a local-first architecture that prioritizes availability in the presence of network partitions, there is a tension between two goals: merging concurrent changes without user intervention and maintaining data integrity constraints. We propose a synchronization model called forking histories which satisfies both goals in an unconventional way. In the case of conflicting writes, the model exposes multiple event histories that users can see and edit rather than converging to a single state. This allows integrity constraints to be maintained within each history while giving users flexibility in deciding when to manually reconcile conflicts. We describe a class of applications for which these integrity constraints are particularly important and propose a design for a system that implements this model.},
	address = {New York, NY, USA},
	author = {Schiefer, Nicholas and Litt, Geoffrey and Jackson, Daniel},
	booktitle = {Proceedings of the 9th {Workshop} on {Principles} and {Practice} of {Consistency} for {Distributed} {Data}},
	doi = {10.1145/3517209.3524041},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/XIXE5NXZ/Schiefer et al. - 2022 - Merge what you can, fork what you can't managing .pdf:application/pdf},
	isbn = {978-1-4503-9256-3},
	keywords = {CRDTs, optimistic concurrency control},
	month = apr,
	pages = {24--32},
	publisher = {Association for Computing Machinery},
	series = {{PaPoC} '22},
	shorttitle = {Merge what you can, fork what you can't},
	title = {Merge what you can, fork what you can't: managing data integrity in local-first software},
	url = {https://dl.acm.org/doi/10.1145/3517209.3524041},
	urldate = {2023-03-23},
	year = {2022},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3517209.3524041},
	bdsk-url-2 = {https://doi.org/10.1145/3517209.3524041}}

@phdthesis{toumlilt_colony_2021,
	abstract = {Immediate response, autonomy and availability is brought to edge applications, such as gaming, cooperative engineering, or in-the-field information sharing, by distributing and replicating data at the edge. However, application developers and users demand the highest possible consistency guarantees, and specific support for group collaboration. To address this challenge, COLONY guarantees Transactional Causal Plus Consistency (TCC+) globally, dovetailing with Snapshot Isolation within edge groups. To help with scalability, fault tolerance and security, its logical communication topology is tree-like, with replicated roots in the core cloud, but with the flexibility to migrate a node or a group. Despite this hybrid approach, applications enjoy the same semantics everywhere in the topology. Our experiments show that local caching and peer groups improve throughput and response time significantly, performance is not affected in offline mode, and that migration is seamless.},
	author = {Toumlilt, Ilyas},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/PFP7R7H2/Toumlilt - 2021 - Colony a Hybrid Consistency System for Highly-Av.pdf:application/pdf},
	language = {en},
	month = dec,
	school = {Sorbonne Universit{\'e}},
	shorttitle = {Colony},
	title = {Colony : a {Hybrid} {Consistency} {System} for {Highly}-{Available} {Collaborative} {Edge} {Computing}},
	type = {phdthesis},
	url = {https://theses.hal.science/tel-03727724},
	urldate = {2023-03-23},
	year = {2021},
	bdsk-url-1 = {https://theses.hal.science/tel-03727724}}

@inproceedings{toumlilt_highly-available_2021,
	abstract = {Edge applications, such as gaming, cooperative engineering, or in-the-field information sharing, enjoy immediate response, autonomy and availability by distributing and replicating data at the edge. However, application developers and users demand the highest possible consistency guarantees, and specific support for group collaboration. To address this challenge, Colony guarantees Transactional Causal Plus Consistency (TCC+) globally, strengthened to Snapshot Isolation within edge groups. To help with scalability, fault tolerance and security, its logical communication topology is forest-like, with replicated roots in the core cloud, but with the flexibility to migrate a node or a group. Despite this hybrid approach, applications enjoy the same semantics everywhere in the topology. Our experiments show that local caching and peer groups improve throughput and response time significantly, performance is not affected in offline mode, and that migration is seamless.},
	address = {New York, NY, USA},
	author = {Toumlilt, Ilyas and Sutra, Pierre and Shapiro, Marc},
	booktitle = {Proceedings of the 22nd {International} {Middleware} {Conference}},
	doi = {10.1145/3464298.3493405},
	file = {Submitted Version:/Users/pandey/Zotero/storage/KI9KQD8B/Toumlilt et al. - 2021 - Highly-available and consistent group collaboratio.pdf:application/pdf},
	isbn = {978-1-4503-8534-3},
	keywords = {causal consistency, peer-to-peer systems, CRDTs, collaborative computing, edge computing},
	month = dec,
	pages = {336--351},
	publisher = {Association for Computing Machinery},
	series = {Middleware '21},
	title = {Highly-available and consistent group collaboration at the edge with colony},
	url = {https://doi.org/10.1145/3464298.3493405},
	urldate = {2023-03-23},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3464298.3493405}}

@inproceedings{litt_cambria_2021,
	abstract = {Schema evolution is a pervasive challenge in distributed systems. Developers often resort to ad hoc solutions that mix compatibility concerns with application code. We propose a principled replacement: an isolated software layer that uses bidirectional lenses to translate data between schemas on demand to maintain backward and forward compatibility. We have implemented these ideas in a TypeScript library called Cambria, and integrated it with the Automerge CRDT to support peer-to-peer editing. We share reflections from building a decentralized issue tracker application and successfully using Cambria to maintain compatibility across versions. We also present our vision for a "Cambrian Era" for software, where better schema evolution tools enable people to customize their software more freely while preserving the ability to collaborate.},
	address = {New York, NY, USA},
	author = {Litt, Geoffrey and Hardenberg, Peter van and Henry, Orion},
	doi = {10.1145/3447865.3457963},
	file = {Litt et al. - 2021 - Cambria Schema Evolution in Distributed Systems with Edit Lenses.pdf:/Users/pandey/Zotero/storage/NN5PW3F8/Litt et al. - 2021 - Cambria Schema Evolution in Distributed Systems with Edit Lenses.pdf:application/pdf},
	isbn = {978-1-4503-8338-7},
	keywords = {CRDTs, bidirectional lenses, schema evolution},
	month = apr,
	pages = {1--9},
	publisher = {Association for Computing Machinery},
	series = {{PaPoC} '21},
	shorttitle = {Cambria},
	title = {Cambria: {Schema} {Evolution} in {Distributed} {Systems} with {Edit} {Lenses}},
	url = {https://dl.acm.org/doi/10.1145/3447865.3457963},
	urldate = {2023-03-25},
	year = {2021},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3447865.3457963},
	bdsk-url-2 = {https://doi.org/10.1145/3447865.3457963}}

@inproceedings{kleppmann_local-first_2019,
	abstract = {Cloud apps like Google Docs and Trello are popular because they enable real-time collaboration with colleagues, and they make it easy for us to access our work from all of our devices. However, by centralizing data storage on servers, cloud apps also take away ownership and agency from users. If a service shuts down, the software stops functioning, and data created with that software is lost. In this article we propose local-first software, a set of principles for software that enables both collaboration and ownership for users. Local-first ideals include the ability to work offline and collaborate across multiple devices, while also improving the security, privacy, long-term preservation, and user control of data. We survey existing approaches to data storage and sharing, ranging from email attachments to web apps to Firebase-backed mobile apps, and we examine the trade-offs of each. We look at Conflict-free Replicated Data Types (CRDTs): data structures that are multi-user from the ground up while also being fundamentally local and private. CRDTs have the potential to be a foundational technology for realizing local-first software. We share some of our findings from developing local-first software prototypes at the Ink \& Switch research lab over the course of several years. These experiments test the viability of CRDTs in practice, and explore the user interface challenges for this new data model. Lastly, we suggest some next steps for moving towards local-first software: for researchers, for app developers, and a startup opportunity for entrepreneurs.},
	address = {New York, NY, USA},
	author = {Kleppmann, Martin and Wiggins, Adam and van Hardenberg, Peter and McGranaghan, Mark},
	booktitle = {Proceedings of the 2019 {ACM} {SIGPLAN} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software}},
	doi = {10.1145/3359591.3359737},
	file = {Submitted Version:/Users/pandey/Zotero/storage/UPIHKSP6/Kleppmann et al. - 2019 - Local-first software you own your data, in spite .pdf:application/pdf},
	isbn = {978-1-4503-6995-4},
	keywords = {CRDTs, collaboration software, data ownership, mobile computing, peer-to-peer communication},
	month = oct,
	pages = {154--178},
	publisher = {Association for Computing Machinery},
	series = {Onward! 2019},
	shorttitle = {Local-first software},
	title = {Local-first software: you own your data, in spite of the cloud},
	url = {https://doi.org/10.1145/3359591.3359737},
	urldate = {2023-03-27},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3359591.3359737}}

@inproceedings{zawirski_write_2015,
	abstract = {Client-side apps (e.g., mobile or in-browser) need cloud data to be available in a local cache, for both reads and updates. For optimal user experience and developer support, the cache should be consistent and fault-tolerant. In order to scale to high numbers of unreliable and resource-poor clients, and large database, the system needs to use resources sparingly. The SwiftCloud distributed object database is the first to provide fast reads and writes via a causally-consistent client-side local cache backed by the cloud. It is thrifty in resources and scales well, thanks to consistent versioning provided by the cloud, using small and bounded metadata. It remains available during faults, switching to a different data centre when the current one is not responsive, while maintaining its consistency guarantees. This paper presents the SwiftCloud algorithms, design, and experimental evaluation. It shows that client-side apps enjoy the high performance and availability, under the same guarantees as a remote cloud data store, at a small cost.},
	address = {New York, NY, USA},
	author = {Zawirski, Marek and Pregui{\c c}a, Nuno and Duarte, S{\'e}rgio and Bieniusa, Annette and Balegas, Valter and Shapiro, Marc},
	booktitle = {Proceedings of the 16th {Annual} {Middleware} {Conference}},
	doi = {10.1145/2814576.2814733},
	file = {Submitted Version:/Users/pandey/Zotero/storage/4AQGUL97/Zawirski et al. - 2015 - Write Fast, Read in the Past Causal Consistency f.pdf:application/pdf},
	isbn = {978-1-4503-3618-5},
	keywords = {Causal Consistency, Client-side Storage, Eventual Consistency, Fault Tolerance, Geo-replication},
	month = nov,
	pages = {75--87},
	publisher = {Association for Computing Machinery},
	series = {Middleware '15},
	shorttitle = {Write {Fast}, {Read} in the {Past}},
	title = {Write {Fast}, {Read} in the {Past}: {Causal} {Consistency} for {Client}-{Side} {Applications}},
	url = {https://doi.org/10.1145/2814576.2814733},
	urldate = {2023-03-27},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1145/2814576.2814733}}

@inproceedings{akkoorath_cure_2016,
	abstract = {Developers of cloud-scale applications face a difficult decision of which kind of storage to use, summarised by the CAP theorem. Currently, the choice is between classical CP databases, which provide strong guarantees but are slow, expensive, and unavailable under partition; and NoSQL-style AP databases, which are fast and available, but too hard to program against. We present an alternative: Cure provides the highest level of guarantees that remains compatible with availability. These guarantees include: causal consistency (no ordering anomalies), atomicity (consistent multi-key updates), and high-level data types (developer friendly) with safe resolution of concurrent updates (guaranteeing convergence). These guarantees minimise the anomalies caused by parallelism and distribution, and facilitate the development of applications. This report presents the protocols for highly available transactions, and an experimental evaluation showing that Cure is able to achieve performance similar to eventually-consistent NoSQL databases, while providing stronger guarantees.},
	address = {Nara, Japan},
	author = {Akkoorath, Deepthi Devaki and Tomsic, Alejandro Z. and Bravo, Manuel and Li, Zhongmiao and Crain, Tyler and Bieniusa, Annette and Preguica, Nuno and Shapiro, Marc},
	booktitle = {2016 {IEEE} 36th {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	doi = {10.1109/ICDCS.2016.98},
	file = {Akkoorath et al. - 2016 - Cure Strong Semantics Meets High Availability and.pdf:/Users/pandey/Zotero/storage/6TXHZHA6/Akkoorath et al. - 2016 - Cure Strong Semantics Meets High Availability and.pdf:application/pdf},
	isbn = {978-1-5090-1483-5},
	language = {en},
	month = jun,
	pages = {405--414},
	publisher = {IEEE},
	shorttitle = {Cure},
	title = {Cure: {Strong} {Semantics} {Meets} {High} {Availability} and {Low} {Latency}},
	url = {http://ieeexplore.ieee.org/document/7536539/},
	urldate = {2023-03-27},
	year = {2016},
	bdsk-url-1 = {http://ieeexplore.ieee.org/document/7536539/},
	bdsk-url-2 = {https://doi.org/10.1109/ICDCS.2016.98}}

@inproceedings{du_clock-si_2013,
	abstract = {Clock-SI is a fully distributed protocol that implements snapshot isolation (SI) for partitioned data stores. It derives snapshot and commit timestamps from loosely synchronized clocks, rather than from a centralized timestamp authority as used in current systems. A transaction obtains its snapshot timestamp by reading the clock at its originating partition and Clock-SI provides the corresponding consistent snapshot across all the partitions. In contrast to using a centralized timestamp authority, Clock-SI has availability and performance benefits: It avoids a single point of failure and a potential performance bottleneck, and improves transaction latency and throughput. We develop an analytical model to study the trade-offs introduced by Clock-SI among snapshot age, delay probabilities of transactions, and abort rates of update transactions. We verify the model predictions using a system implementation. Furthermore, we demonstrate the performance benefits of Clock-SI experimentally using a micro-benchmark and an application-level benchmark on a partitioned key-value store. For short read-only transactions, Clock-SI improves latency and throughput by 50\% by avoiding communications with a centralized timestamp authority. With a geographically partitioned data store, Clock-SI reduces transaction latency by more than 100 milliseconds. Moreover, the performance benefits of Clock-SI come with higher availability.},
	author = {Du, Jiaqing and Elnikety, Sameh and Zwaenepoel, Willy},
	booktitle = {2013 {IEEE} 32nd {International} {Symposium} on {Reliable} {Distributed} {Systems}},
	doi = {10.1109/SRDS.2013.26},
	file = {IEEE Xplore Abstract Record:/Users/pandey/Zotero/storage/PM4SNAI7/6656273.html:text/html;Submitted Version:/Users/pandey/Zotero/storage/HBNMKMYI/Du et al. - 2013 - Clock-SI Snapshot Isolation for Partitioned Data .pdf:application/pdf},
	keywords = {Clocks, Distributed databases, Protocols, Silicon, Synchronization, Delays, distributed transactions, loosely synchronized clocks, partitioned data, snapshot isolation},
	month = sep,
	note = {ISSN: 1060-9857},
	pages = {173--184},
	shorttitle = {Clock-{SI}},
	title = {Clock-{SI}: {Snapshot} {Isolation} for {Partitioned} {Data} {Stores} {Using} {Loosely} {Synchronized} {Clocks}},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/SRDS.2013.26}}

@misc{shapiro_database_2018,
	abstract = {A data store allows application processes to put and get data from a shared memory. In general, a data store cannot be modelled as a strictly sequential process. Applications observe non-sequential behaviours, called anomalies. The set of pos- sible behaviours, and conversely of possible anomalies, constitutes the consistency model of the data store.},
	author = {Shapiro, Marc and Sutra, Pierre},
	doi = {10.1007/978-3-319-63962-8\_203-1},
	file = {Shapiro and Sutra - 2018 - Database Consistency Models.pdf:/Users/pandey/Zotero/storage/RKPX5CXV/Shapiro and Sutra - 2018 - Database Consistency Models.pdf:application/pdf},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing},
	language = {en},
	month = apr,
	note = {arXiv:1804.00914 [cs]},
	title = {Database {Consistency} {Models}},
	url = {http://arxiv.org/abs/1804.00914},
	urldate = {2023-03-27},
	year = {2018},
	bdsk-url-1 = {http://arxiv.org/abs/1804.00914},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-63962-8%5C_203-1}}

@inproceedings{bornholt_using_2021,
	abstract = {This paper reports our experience applying lightweight formal methods to validate the correctness of ShardStore, a new key-value storage node implementation for the Amazon S3 cloud object storage service. By ``lightweight formal methods{\v z} we mean a pragmatic approach to verifying the correctness of a production storage node that is under ongoing feature development by a full-time engineering team. We do not aim to achieve full formal verification, but instead emphasize automation, usability, and the ability to continually ensure correctness as both software and its specification evolve over time. Our approach decomposes correctness into independent properties, each checked by the most appropriate tool, and develops executable reference models as specifications to be checked against the implementation. Our work has prevented 16 issues from reaching production, including subtle crash consistency and concurrency problems, and has been extended by non-formal-methods experts to check new features and properties as ShardStore has evolved.},
	address = {Virtual Event Germany},
	author = {Bornholt, James and Joshi, Rajeev and Astrauskas, Vytautas and Cully, Brendan and Kragl, Bernhard and Markle, Seth and Sauri, Kyle and Schleit, Drew and Slatton, Grant and Tasiran, Serdar and Van Geffen, Jacob and Warfield, Andrew},
	booktitle = {Proceedings of the {ACM} {SIGOPS} 28th {Symposium} on {Operating} {Systems} {Principles} {CD}-{ROM}},
	doi = {10.1145/3477132.3483540},
	file = {Bornholt et al. - 2021 - Using Lightweight Formal Methods to Validate a Key.pdf:/Users/pandey/Zotero/storage/I8P832E6/Bornholt et al. - 2021 - Using Lightweight Formal Methods to Validate a Key.pdf:application/pdf},
	isbn = {978-1-4503-8709-5},
	language = {en},
	month = oct,
	pages = {836--850},
	publisher = {ACM},
	title = {Using {Lightweight} {Formal} {Methods} to {Validate} a {Key}-{Value} {Storage} {Node} in {Amazon} {S3}},
	url = {https://dl.acm.org/doi/10.1145/3477132.3483540},
	urldate = {2023-03-27},
	year = {2021},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3477132.3483540},
	bdsk-url-2 = {https://doi.org/10.1145/3477132.3483540}}

@misc{nath_consistency_2021,
	abstract = {Unrelated and Unusual Note: I am devastated seeing India's helpless COVID-19 second wave situation due to severe Oxygen, medical equipment{\ldots}},
	author = {Nath, Kousik},
	file = {Snapshot:/Users/pandey/Zotero/storage/33Y5NRJ4/consistency-guarantees-in-distributed-systems-explained-simply-720caa034116.html:text/html},
	journal = {Medium},
	language = {en},
	month = may,
	title = {Consistency {Guarantees} in {Distributed} {Systems} {Explained} {Simply}},
	url = {https://kousiknath.medium.com/consistency-guarantees-in-distributed-systems-explained-simply-720caa034116},
	urldate = {2023-03-27},
	year = {2021},
	bdsk-url-1 = {https://kousiknath.medium.com/consistency-guarantees-in-distributed-systems-explained-simply-720caa034116}}

@article{khelaifa_comparative_2019,
	abstract = {NoSQL storage systems are used extensively by web applications and provide an attractive alternative to conventional databases due to their high security and availability with a low cost. High data availability is achieved by replicating data in different servers in order to reduce access time lag, network bandwidth consumption and system unreliability. Hence, the data consistency is a major challenge in distributed systems. In this context, strong consistency guarantees data freshness but affects directly the performance and availability of the system. In contrast, weaker consistency enhances availability and performance but increases data staleness. Therefore, an adaptive consistency strategy is needed to tune, during runtime, the consistency level depending on the criticality of the requests or data items. Although there is a rich literature on adaptive consistency approaches in cloud storage, there is a need to classify as well as regroup the approaches based on their strategies. This paper will establish a set of comparative criteria and then make a comparative analysis of existing adaptive consistency approaches. A survey of this kind not only provides the user/researcher with a comparative performance analysis of the approaches but also clarifies the suitability of these for candidate cloud systems.},
	author = {Khelaifa, Abdennacer and Benharzallah, Saber and Kahloul, Laid and Euler, Reinhardt and Laouid, Abdelkader and Bounceur, Ahc{\`e}ne},
	doi = {10.1016/j.jpdc.2019.03.006},
	file = {Khelaifa et al. - 2019 - A comparative analysis of adaptive consistency app.pdf:/Users/pandey/Zotero/storage/97W2ACIM/Khelaifa et al. - 2019 - A comparative analysis of adaptive consistency app.pdf:application/pdf},
	issn = {07437315},
	journal = {Journal of Parallel and Distributed Computing},
	language = {en},
	month = jul,
	pages = {36--49},
	title = {A comparative analysis of adaptive consistency approaches in cloud storage},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0743731518301795},
	urldate = {2023-03-28},
	volume = {129},
	year = {2019},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0743731518301795},
	bdsk-url-2 = {https://doi.org/10.1016/j.jpdc.2019.03.006}}

@inproceedings{huang_incremental_2022,
	abstract = {Causal consistency is one of the strongest consistency models that can be implemented to ensure availability under network partition in distributed systems. The problem of causal consistency checking asks whether a given history of some system is causally consistent. Recently Bouajjani et al. showed that for read-write memory histories in which writes assign unique values to each variable, this can be solved in polynomial time. The algorithm searches for bad patterns of causal consistency, which are defined using various relations derived from history. However, the high time complexity of the algorithm makes it not so practical. In this paper, we show how to improve this checking algorithm by incrementally computing the relations underlying the bad patterns. We also demonstrate its efficiency by conducting experiments on both random histories and those generated by MongoDB.},
	address = {New York, NY, USA},
	author = {Huang, Yi and Wei, Hengfeng},
	booktitle = {Proceedings of the 13th {Asia}-{Pacific} {Symposium} on {Internetware}},
	doi = {10.1145/3545258.3545262},
	isbn = {978-1-4503-9780-3},
	keywords = {Causal Consistency, Consistency Checking, Consistency Models},
	month = sep,
	pages = {181--191},
	publisher = {Association for Computing Machinery},
	series = {Internetware '22},
	title = {Incremental {Causal} {Consistency} {Checking} for {Read}-{Write} {Memory} {Histories}},
	url = {https://doi.org/10.1145/3545258.3545262},
	urldate = {2023-03-27},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3545258.3545262}}

@article{mehdi_i_nodate,
	abstract = {We describe the design, implementation, and evaluation of Occult (Observable Causal Consistency Using Lossy Timestamps), the first scalable, geo-replicated data store that provides causal consistency to its clients without exposing the system to the possibility of slowdown cascades, a key obstacle to the deployment of causal consistency at scale. Occult supports read/write transactions under PC-PSI, a variant of Parallel Snapshot Isolation that contributes to Occult's immunity to slowdown cascades by weakening how PSI replicates transactions committed at the same replica. While PSI insists that they all be totally ordered, PC-PSI simply requires total order Per Client session. Nonetheless, Occult guarantees that all transactions read from a causally consistent snapshot of the datastore without requiring any coordination in how transactions are asynchronously replicated.},
	author = {Mehdi, Syed Akbar and Littley, Cody and Crooks, Natacha and Alvisi, Lorenzo and Bronson, Nathan and Lloyd, Wyatt},
	file = {Mehdi et al. - I Can't Believe It's Not Causal! Scalable Causal C.pdf:/Users/pandey/Zotero/storage/Q25RL7MP/Mehdi et al. - I Can't Believe It's Not Causal! Scalable Causal C.pdf:application/pdf},
	language = {en},
	title = {I {Can}'t {Believe} {It}'s {Not} {Causal}! {Scalable} {Causal} {Consistency} with {No} {Slowdown} {Cascades}}}

@inproceedings{kokocinski_mixing_2019,
	abstract = {In this paper we study the properties of eventually consistent distributed systems that feature arbitrarily complex semantics and mix eventual and strong consistency. These systems execute requests in a highly-available, weakly-consistent fashion, but also enable stronger guarantees through additional inter-replica synchronization mechanisms that require the ability to solve distributed consensus. We use the seminal Bayou system as a case study, and then generalize our findings to a whole class of systems. We show dubious and unintuitive behaviour exhibited by those systems and provide a theoretical framework for reasoning about their correctness. We also state an impossibility result that formally proves the inherent limitation of such systems, namely temporary operation reordering, which admits interim disagreement between replicas on the relative order in which the client requests were executed.},
	address = {Toronto ON Canada},
	author = {Kokoci{\'n}ski, Maciej and Kobus, Tadeusz and Wojciechowski, Pawe{\l} T.},
	booktitle = {Proceedings of the 2019 {ACM} {Symposium} on {Principles} of {Distributed} {Computing}},
	doi = {10.1145/3293611.3331583},
	file = {Kokoci{\'n}ski et al. - 2019 - On Mixing Eventual and Strong Consistency Bayou R.pdf:/Users/pandey/Zotero/storage/8QEGSF2R/Kokoci{\'n}ski et al. - 2019 - On Mixing Eventual and Strong Consistency Bayou R.pdf:application/pdf},
	isbn = {978-1-4503-6217-7},
	language = {en},
	month = jul,
	pages = {458--460},
	publisher = {ACM},
	shorttitle = {On {Mixing} {Eventual} and {Strong} {Consistency}},
	title = {On {Mixing} {Eventual} and {Strong} {Consistency}: {Bayou} {Revisited}},
	url = {https://dl.acm.org/doi/10.1145/3293611.3331583},
	urldate = {2023-03-30},
	year = {2019},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3293611.3331583},
	bdsk-url-2 = {https://doi.org/10.1145/3293611.3331583}}

@phdthesis{nair_designing_2021-1,
	abstract = {Designing distributed applications involves a fundamental trade-off between safety and performance as described by CAP theorem. We focus on the cases where safety is the top requirement.For the subclass of state-based distributed systems, we propose a proof methodology for establishing that a given application maintains a given invariant. Our approach allows reasoning about individual operations separately. We demonstrate that our rules are sound, and with a mechanized proof engine, we illustrate their use with some representative examples. For conflicting operations, the developer can choose between conflict resolution or coordination. We present a novel replicated tree data structure that supports coordination-free concurrent atomic moves, and arguably maintains the tree invariant. Our analysis identifies cases where concurrent moves are inherently safe. For the remaining cases we devise a conflict resolution algorithm. The trade-off is that in some cases a move operation "loses". Given the coordination required by some application for safety, it can be implemented in many different ways. Even restricting to locks, they can use various configurations, differing by lock granularity, type, and placement. The performance of each configuration depends on workload. We study the "coordination lattice", i.e., design space of lock configurations, and define a set of metrics to systematically navigate them.},
	author = {Nair, Sreeja Sasidharan},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/UVEN3MI5/Nair - 2021 - Designing safe and highly available distributed ap.pdf:application/pdf},
	language = {en},
	month = jul,
	school = {Sorbonne Universit{\'e}},
	title = {Designing safe and highly available distributed applications},
	type = {phdthesis},
	url = {https://theses.hal.science/tel-03339393},
	urldate = {2023-03-30},
	year = {2021},
	bdsk-url-1 = {https://theses.hal.science/tel-03339393}}

@article{blum_linux_nodate,
	author = {Blum, Richard},
	file = {Blum - Linux{\textregistered} Command Line and Shell Scripting Bible.pdf:/Users/pandey/Downloads/Blum - Linux{\textregistered} Command Line and Shell Scripting Bible.pdf:application/pdf},
	language = {en},
	title = {Linux{\textregistered} {Command} {Line} and {Shell} {Scripting} {Bible}}}

@article{tang_adaptive_nodate,
	abstract = {Use of transactional multicore main-memory databases is growing due to dramatic increases in memory size and CPU cores available for a single machine. To leverage these resources, recent concurrency control protocols have been proposed for main-memory databases, but are largely optimized for specific workloads. Due to shifting and unknown access patterns, workloads may change and one specific algorithm cannot dynamically fit all varied workloads. Thus, it is desirable to choose the right concurrency control protocol for a given workload. To address this issue we present adaptive concurrency control (ACC), that dynamically clusters data and chooses the optimal concurrency control protocol for each cluster. ACC addresses three key challenges: i) how to cluster data to minimize cross-cluster access and maintain load-balancing, ii) how to model workloads and perform protocol selection accordingly, and iii) how to support mixed concurrency control protocols running simultaneously. In this paper, we outline these challenges and present preliminary results.},
	author = {Tang, Dixin and Jiang, Hao and Elmore, Aaron J},
	file = {Tang et al. - Adaptive Concurrency Control Despite the Looking .pdf:/Users/pandey/Zotero/storage/YGSRRGIV/Tang et al. - Adaptive Concurrency Control Despite the Looking .pdf:application/pdf},
	language = {en},
	title = {Adaptive {Concurrency} {Control}: {Despite} the {Looking} {Glass}, {One} {Concurrency} {Control} {Does} {Not} {Fit} {All}}}

@article{tang_toward_nodate,
	abstract = {Recent studies show that mixing concurrency control protocols within a single database can significantly outperform a single protocol. However, prior projects to mix concurrency control either are limited to specific pairs of protocols (e.g mixing two-phase locking (2PL) and optimistic concurrency control (OCC)) or introduce extra concurrency control overhead to guarantee their general applicability, which can be a performance bottleneck. In addition, due to unknown and shifting access patterns within a workload, candidate protocols should be chosen dynamically in response to workload changes. This requires changing candidate protocols online without having to stop the whole system, which prior work does not fully address. To resolve these two issues, we present CormCC, a general mixed concurrency control framework with no coordination overhead across candidate protocols while supporting the ability to change a protocol online with minimal overhead. Based on this framework, we build a prototype main-memory multicore database to dynamically mix three popular protocols. Our experiments show CormCC has significantly higher throughput compared with single protocols and state-of-the-art mixed concurrency control approaches.},
	author = {Tang, Dixin and Elmore, Aaron J},
	file = {Tang and Elmore - Toward Coordination-free and Reconfigurable Mixed C.pdf:/Users/pandey/Zotero/storage/EIZCJRJM/Tang and Elmore - Toward Coordination-free and Reconfigurable Mixed C.pdf:application/pdf},
	language = {en},
	title = {Toward {Coordination}-free and {Reconfigurable} {Mixed} {Concurrency} {Control}}}

@article{diniz_dynamic_nodate,
	abstract = {This paper presents dynamic feedback, a technique that enables computations to adapt dynamically to different execution environments. A compiler that uses dynamic feedback produces several different versions of the same source code; each version uses a different optimization policy. The generated code alternately performs sampling phases and production phases. Each sampling phase measures the overhead of each version in the current environment. Each production phase uses the version with the least overhead in the previous sampling phase. The computation periodically resamples to adjust dynamically to changes in the environment.},
	author = {Diniz, Pedro and Rinard, Martin},
	file = {Diniz and Rinard - Dynamic Feedback An Effective Technique for Adapt.pdf:/Users/pandey/Zotero/storage/CJ3UFE5N/Diniz and Rinard - Dynamic Feedback An Effective Technique for Adapt.pdf:application/pdf},
	language = {en},
	title = {Dynamic {Feedback}: {An} {Effective} {Technique} for {Adaptive} {Computing}}}

@article{letia_consistency_2010,
	abstract = {Replicas of a commutative replicated data type (CRDT) eventually converge without any complex concurrency control. We validate the design of a non-trivial CRDT, a replicated sequence, with performance measurements in the context of Wikipedia. Furthermore, we discuss how to eliminate a remaining scalability bottleneck: Whereas garbage collection previously required a system-wide consensus, here we propose a flexible two-tier architecture and a protocol for migrating between tiers. We also discuss how the CRDT concept can be generalised, and its limitations.},
	author = {Letia, Mihai and Pregui{\c c}a, Nuno and Shapiro, Marc},
	doi = {10.1145/1773912.1773921},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/SENZYK6Q/Letia et al. - 2010 - Consistency without concurrency control in large, .pdf:application/pdf},
	issn = {0163-5980},
	journal = {ACM SIGOPS Operating Systems Review},
	month = apr,
	number = {2},
	pages = {29--34},
	title = {Consistency without concurrency control in large, dynamic systems},
	url = {https://dl.acm.org/doi/10.1145/1773912.1773921},
	urldate = {2023-04-03},
	volume = {44},
	year = {2010},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/1773912.1773921},
	bdsk-url-2 = {https://doi.org/10.1145/1773912.1773921}}

@misc{angles_ldbc_2022,
	abstract = {The Linked Data Benchmark Council's Social Network Benchmark (LDBC SNB) is an effort intended to test various functionalities of systems used for graph-like data management. For this, LDBC SNB uses the recognizable scenario of operating a social network, characterized by its graph-shaped data. LDBC SNB consists of two workloads that focus on different functionalities: the Interactive workload (interactive transactional queries) and the Business Intelligence workload (analytical queries). This document contains the definition of both workloads. This includes a detailed explanation of the data used in the LDBC SNB, a detailed description for all queries, and instructions on how to generate the data and run the benchmark with the provided software.},
	author = {Angles, Renzo and Antal, J{\'a}nos Benjamin and Averbuch, Alex and Birler, Altan and Boncz, Peter and B{\'u}r, M{\'a}rton and Erling, Orri and Gubichev, Andrey and Haprian, Vlad and Kaufmann, Moritz and Pey, Josep Llu{\'\i}s Larriba and Mart{\'\i}nez, Norbert and Marton, J{\'o}zsef and Paradies, Marcus and Pham, Minh-Duc and Prat-P{\'e}rez, Arnau and Spasi{\'c}, Mirko and Steer, Benjamin A. and Szak{\'a}llas, D{\'a}vid and Sz{\'a}rnyas, G{\'a}bor and Waudby, Jack and Wu, Mingxi and Zhang, Yuchen},
	file = {Angles et al. - 2022 - The LDBC Social Network Benchmark.pdf:/Users/pandey/Zotero/storage/9689IQW6/Angles et al. - 2022 - The LDBC Social Network Benchmark.pdf:application/pdf},
	keywords = {Computer Science - Databases, Computer Science - Performance, Computer Science - Social and Information Networks, H.2.4},
	language = {en},
	month = nov,
	note = {arXiv:2001.02299 [cs]},
	publisher = {arXiv},
	title = {The {LDBC} {Social} {Network} {Benchmark}},
	url = {http://arxiv.org/abs/2001.02299},
	urldate = {2023-04-14},
	year = {2022},
	bdsk-url-1 = {http://arxiv.org/abs/2001.02299}}

@article{idreos_periodic_nodate,
	abstract = {We describe the vision of being able to reason about the design space of data structures. We break this down into two questions: 1) Can we know all data structures that is possible to design? 2) Can we compute the performance of arbitrary designs on a given hardware and workload without having to implement the design or even access the target hardware? If those challenges are possible, then an array of exciting opportunities would become feasible such as interactive what-if design to improve the productivity of data systems researchers and engineers, and informed decision making in industrial settings with regards to critical hardware/workload/data structure design issues. Then, even fully automated discovery of new data structure designs becomes possible. Furthermore, the structure of the design space itself provides numerous insights and opportunities such as the existence of design continuums that can lead to data systems with deep adaptivity, and a new understanding of the possible performance tradeoffs. Given the universal presence of data structures at the very core of any data-driven field across all sciences and industries, reasoning about their design can have significant benefits, making it more feasible (easier, faster and cheaper) to adopt tailored state-of-the-art storage solutions. And this effect is going to become increasingly more critical as data keeps growing, hardware keeps changing and more applications/fields realize the transformative power and potential of data analytics. This paper presents this vision and surveys first steps that demonstrate its feasibility.},
	author = {Idreos, Stratos and Athanassoulis, Kostas Zoumpatianos Manos and Dayan, Niv and Hentschel, Brian},
	file = {Idreos et al. - The Periodic Table of Data Structures.pdf:/Users/pandey/Zotero/storage/6WNV398A/Idreos et al. - The Periodic Table of Data Structures.pdf:application/pdf},
	language = {en},
	title = {The {Periodic} {Table} of {Data} {Structures}}}

@misc{webber_strong_2020,
	abstract = {Thoughts on the design of distributed graph databases that use strongly consistent methods for the data stored on their shards, but pay no regard to ordering of concurrent updates.},
	author = {Webber, Dr Jim},
	file = {Snapshot:/Users/pandey/Zotero/storage/LYVBBACK/2020-12-weak-stong-consistency.html:text/html},
	journal = {World Wide Webber},
	language = {en-us},
	month = dec,
	title = {Strong {Consitency} {Claims} in {Distributed} {Graph} {Databases}},
	url = {https://jimwebber.org/post/2020-12-weak-stong-consistency/},
	urldate = {2023-04-20},
	year = {2020},
	bdsk-url-1 = {https://jimwebber.org/post/2020-12-weak-stong-consistency/}}

@article{dubey_weaver_2016,
	abstract = {Graph databases have become a common infrastructure component. Yet existing systems either operate on offline snapshots, provide weak consistency guarantees, or use expensive concurrency control techniques that limit performance. In this paper, we introduce a new distributed graph database, called Weaver, which enables efficient, transactional graph analyses as well as strictly serializable ACID transactions on dynamic graphs. The key insight that allows Weaver to combine strict serializability with horizontal scalability and high performance is a novel request ordering mechanism called refinable timestamps. This technique couples coarse-grained vector timestamps with a fine-grained timeline oracle to pay the overhead of strong consistency only when needed. Experiments show that Weaver enables a Bitcoin blockchain explorer that is 8× faster than Blockchain.info, and achieves 10.9× higher throughput than the Titan graph database on social network workloads and 4× lower latency than GraphLab on offline graph traversal workloads.},
	author = {Dubey, Ayush and Hill, Greg D. and Escriva, Robert and Sirer, Emin G{\"u}n},
	doi = {10.14778/2983200.2983202},
	file = {Dubey et al. - 2016 - Weaver A High-Performance, Transactional Graph Da.pdf:/Users/pandey/Zotero/storage/AIZ7K4BQ/Dubey et al. - 2016 - Weaver A High-Performance, Transactional Graph Da.pdf:application/pdf},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing},
	language = {en},
	month = jul,
	note = {arXiv:1509.08443 [cs]},
	number = {11},
	pages = {852--863},
	shorttitle = {Weaver},
	title = {Weaver: {A} {High}-{Performance}, {Transactional} {Graph} {Database} {Based} on {Refinable} {Timestamps}},
	url = {http://arxiv.org/abs/1509.08443},
	urldate = {2023-04-22},
	volume = {9},
	year = {2016},
	bdsk-url-1 = {http://arxiv.org/abs/1509.08443},
	bdsk-url-2 = {https://doi.org/10.14778/2983200.2983202}}

@inproceedings{khandelwal_zipg_2017,
	abstract = {We present ZipG, a distributed memory-efficient graph store for serving interactive graph queries. ZipG achieves memory efficiency by storing the input graph data using a compressed representation. What differentiates ZipG from other graph stores is its ability to execute a wide range of graph queries directly on this compressed representation. ZipG can thus execute a larger fraction of queries in main memory, achieving query interactivity. ZipG exposes a minimal API that is functionally rich enough to implement published functionalities from several industrial graph stores. We demonstrate this by implementing and evaluating graph queries from Facebook TAO, LinkBench, Graph Search and several other workloads on top of ZipG. On a single server with 244GB memory, ZipG executes tens of thousands of queries from these workloads for raw graph data over half a TB; this leads to an order of magnitude (sometimes as much as 23×) higher throughput than Neo4j and Titan. We get similar gains in distributed settings compared to Titan.},
	address = {New York, NY, USA},
	author = {Khandelwal, Anurag and Yang, Zongheng and Ye, Evan and Agarwal, Rachit and Stoica, Ion},
	booktitle = {Proceedings of the 2017 {ACM} {International} {Conference} on {Management} of {Data}},
	doi = {10.1145/3035918.3064012},
	file = {Khandelwal et al. - 2017 - ZipG A Memory-efficient Graph Store for Interacti.pdf:/Users/pandey/Zotero/storage/FA3WPV3S/Khandelwal et al. - 2017 - ZipG A Memory-efficient Graph Store for Interacti.pdf:application/pdf},
	isbn = {978-1-4503-4197-4},
	keywords = {graph compression, graph store, interactive graph queries},
	month = may,
	pages = {1149--1164},
	publisher = {Association for Computing Machinery},
	series = {{SIGMOD} '17},
	shorttitle = {{ZipG}},
	title = {{ZipG}: {A} {Memory}-efficient {Graph} {Store} for {Interactive} {Queries}},
	url = {https://doi.org/10.1145/3035918.3064012},
	urldate = {2023-04-26},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3035918.3064012}}

@inproceedings{milano_mixt_2018,
	abstract = {Programming concurrent, distributed systems is hard---especially when these systems mutate shared, persistent state replicated at geographic scale. To enable high availability and scalability, a new class of weakly consistent data stores has become popular. However, some data needs strong consistency. To manipulate both weakly and strongly consistent data in a single transaction, we introduce a new abstraction: mixed-consistency transactions, embodied in a new embedded language, MixT. Programmers explicitly associate consistency models with remote storage sites; each atomic, isolated transaction can access a mixture of data with different consistency models. Compile-time information-flow checking, applied to consistency models, ensures that these models are mixed safely and enables the compiler to automatically partition transactions. New run-time mechanisms ensure that consistency models can also be mixed safely, even when the data used by a transaction resides on separate, mutually unaware stores. Performance measurements show that despite their stronger guarantees, mixed-consistency transactions retain much of the speed of weak consistency, significantly outperforming traditional serializable transactions.},
	address = {New York, NY, USA},
	author = {Milano, Mae and Myers, Andrew C.},
	booktitle = {Proceedings of the 39th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	doi = {10.1145/3192366.3192375},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/YLF9AGDD/Milano and Myers - 2018 - MixT a language for mixing consistency in geodist.pdf:application/pdf},
	isbn = {978-1-4503-5698-5},
	keywords = {Consistency, Information Flow, Transactions},
	month = jun,
	pages = {226--241},
	publisher = {Association for Computing Machinery},
	series = {{PLDI} 2018},
	shorttitle = {{MixT}},
	title = {{MixT}: a language for mixing consistency in geodistributed transactions},
	url = {https://dl.acm.org/doi/10.1145/3192366.3192375},
	urldate = {2023-04-26},
	year = {2018},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3192366.3192375},
	bdsk-url-2 = {https://doi.org/10.1145/3192366.3192375}}

@inproceedings{pandey_terrace_2021,
	abstract = {Various applications model problems as streaming graphs, which need to quickly apply a stream of updates and run algorithms on the updated graph. Furthermore, many dynamic real-world graphs, such as social networks, follow a skewed distribution of vertex degrees, where there are a few high-degree vertices and many low-degree vertices. Existing static graph-processing systems optimized for graph skewness achieve high performance and low space usage by preprocessing a cache-efficient graph partitioning based on vertex degree. In the streaming setting, the whole graph is not available upfront, however, so finding an optimal partitioning is not feasible in the presence of updates. As a result, existing streaming graph-processing systems take a "one-size-fits-all" approach, leaving performance on the table. We present Terrace, a system for streaming graphs that uses a hierarchical data structure design to store a vertex's neighbors in different data structures depending on the degree of the vertex. This multi-level structure enables Terrace to dynamically partition vertices based on their degrees and adapt to skewness in the underlying graph. Our experiments show that Terrace supports faster batch insertions for batch sizes up to 1M when compared to Aspen, a state-of-the-art graph streaming system. On graph query algorithms, Terrace is between 1.7X--2.6X faster than Aspen and between 0.5X--1.3X as fast as Ligra, a state-of-the-art static graph-processing system.},
	address = {New York, NY, USA},
	author = {Pandey, Prashant and Wheatman, Brian and Xu, Helen and Buluc, Aydin},
	booktitle = {Proceedings of the 2021 {International} {Conference} on {Management} of {Data}},
	doi = {10.1145/3448016.3457313},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/WRS8W3CB/Pandey et al. - 2021 - Terrace A Hierarchical Graph Container for Skewed.pdf:application/pdf},
	isbn = {978-1-4503-8343-1},
	keywords = {graph data structures, indexing, streaming},
	month = jun,
	pages = {1372--1385},
	publisher = {Association for Computing Machinery},
	series = {{SIGMOD} '21},
	shorttitle = {Terrace},
	title = {Terrace: {A} {Hierarchical} {Graph} {Container} for {Skewed} {Dynamic} {Graphs}},
	url = {https://dl.acm.org/doi/10.1145/3448016.3457313},
	urldate = {2023-04-26},
	year = {2021},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3448016.3457313},
	bdsk-url-2 = {https://doi.org/10.1145/3448016.3457313}}

@article{pedone_byzantine_2012,
	abstract = {Replication is a well-established approach to increasing database availability. Many database replication protocols have been proposed for the crash-stop failure model, in which servers fail silently. Fewer database replication protocols have been proposed for the byzantine failure model, in which servers may fail arbitrarily. This paper considers deferred update replication, a popular database replication technique, under byzantine failures. The paper makes three contributions. First, it shows that making deferred update replication tolerate byzantine failures is quite simple. Second, the paper presents a byzantine-tolerant mechanism to execute read-only transactions at a single server. Third, we consider byzantine client attacks against deferred update replication and discuss effective countermeasures against these attacks.},
	author = {Pedone, Fernando and Schiper, Nicolas},
	doi = {10.1007/s13173-012-0060-z},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/Z9MNF6WY/Pedone and Schiper - 2012 - Byzantine fault-tolerant deferred update replicati.pdf:application/pdf},
	issn = {1678-4804},
	journal = {Journal of the Brazilian Computer Society},
	keywords = {Byzantine fault-tolerance, Database replication, Dependable systems},
	language = {en},
	month = mar,
	number = {1},
	pages = {3--18},
	title = {Byzantine fault-tolerant deferred update replication},
	url = {https://doi.org/10.1007/s13173-012-0060-z},
	urldate = {2023-05-10},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1007/s13173-012-0060-z}}

@inproceedings{buragohain_a1_2020,
	abstract = {A1 is an in-memory distributed database used by the Bing search engine to support complex queries over structured data. The key enablers for A1 are availability of cheap DRAM and high speed RDMA (Remote Direct Memory Access) networking in commodity hardware. A1 uses FaRM [11,12] as its underlying storage layer and builds the graph abstraction and query engine on top. The combination of in-memory storage and RDMA access requires rethinking how data is allocated, organized and queried in a large distributed system. A single A1 cluster can store tens of billions of vertices and edges and support a throughput of 350+ million of vertex reads per second with end to end query latency in single digit milliseconds. In this paper we describe the A1 data model, RDMA optimized data structures and query execution.},
	address = {New York, NY, USA},
	author = {Buragohain, Chiranjeeb and Risvik, Knut Magne and Brett, Paul and Castro, Miguel and Cho, Wonhee and Cowhig, Joshua and Gloy, Nikolas and Kalyanaraman, Karthik and Khanna, Richendra and Pao, John and Renzelmann, Matthew and Shamis, Alex and Tan, Timothy and Zheng, Shuheng},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	doi = {10.1145/3318464.3386135},
	file = {Submitted Version:/Users/pandey/Zotero/storage/ICVGRDF4/Buragohain et al. - 2020 - A1 A Distributed In-Memory Graph Database.pdf:application/pdf},
	isbn = {978-1-4503-6735-6},
	keywords = {graph database, distributed database, graph query processing, in-memory database, RDMA},
	month = may,
	pages = {329--344},
	publisher = {Association for Computing Machinery},
	series = {{SIGMOD} '20},
	shorttitle = {A1},
	title = {A1: {A} {Distributed} {In}-{Memory} {Graph} {Database}},
	url = {https://doi.org/10.1145/3318464.3386135},
	urldate = {2023-05-10},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3318464.3386135}}

@inproceedings{neiheiser_fireplug_2018,
	abstract = {The paper describes and evaluates Fireplug, a flexible architecture to build robust geo-replicated graph databases. Fireplug can be configured to tolerate from crash to Byzantine faults, both within and across different datacenters. Furthermore, Fireplug is robust to bugs in existing graph database implementations, as it allows to combine multiple graph databases instances in a cohesive manner. Thus, Fireplug can support many different deployments, according to the performance/robustness tradeoffs imposed by the target application. Our evaluation shows that Fireplug can implement Byzantine fault tolerance in geo-replicated scenarios and still outperform the built-in replication mechanism of Neo4j, which only supports crash faults.},
	author = {Neiheiser, Ray and Presser, Daniel and Rech, Luciana and Bravo, Manuel and Rodrigues, Lu{\'\i}s and Correia, Miguel},
	booktitle = {2018 {International} {Conference} on {Information} {Networking} ({ICOIN})},
	doi = {10.1109/ICOIN.2018.8343095},
	file = {IEEE Xplore Abstract Record:/Users/pandey/Zotero/storage/S9RILVE9/8343095.html:text/html},
	keywords = {Protocols, Databases, Fault tolerance, Fault tolerant systems, Graph databases, Geo-replication, Byzantine faults, Computer crashes, N-version programming, Programming, Software},
	month = jan,
	pages = {110--115},
	shorttitle = {Fireplug},
	title = {Fireplug: {Flexible} and robust {N}-version geo-replication of graph databases},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/ICOIN.2018.8343095}}

@article{homatas_building_nodate,
	abstract = {Formally verifying the correctness of algorithms has become extremely important due to the impact even a small error can have. While many algorithms can be considered proven on paper (i.e. informally) they do not consider implementation factors such as data structures and memory. Many algorithms operate on graphs which is a relatively simple concept but more complicated when implemented as a data structure. There is not much in literature on formal verification of graph algorithms and in particular coloring algorithms. Graph coloring is the problem of assigning a color to each vertex in a graph such that no two adjacent vertices have the same color. While the problem of coloring a 3-colorable graph may seem simple, it is actually NP-hard, so there has been an effort to find polynomial-time algorithms that color a 3-colorable graph with as few colors as possible. One of the first major advancements in this problem was√made by Avi Wigderson in which he provided a polynomial-time algorithm to color a graph with n vertices using O( n) colors [7]. Since then, there have been many more advancements in improving this, and now the 1 best known algorithm uses o(n 5 ) colors [6]. We will f√ormalize the correctness of Wigderson's algorithm to prove t√hat it indeed correctly colors a 3-colorable graph with O( n) colors. In particular, we will attain a bound of exactly 3 n. We will also sho√w that the algorithm is robust i.e. if the graph is not 3-colorable, the graph will either be correctly colored with O( n) colors or will identify that the graph is not 3-colorable. We will prove the desired statements in a faithful way, encoding the graphs as a Coq data structure. Wigderson's algorithm has been a useful foundation for this problem, so verifying its correctness may be a valuable resource for extending this to other coloring algorithms or graph algorithms in general. Our code can be found at github.com/siraben/coq-wigderson.},
	author = {Homatas, Jamison},
	file = {Homatas - Building Graph Theory Using Coq Verification of W.pdf:/Users/pandey/Zotero/storage/CSVFTZT8/Homatas - Building Graph Theory Using Coq Verification of W.pdf:application/pdf},
	language = {en},
	title = {Building {Graph} {Theory} {Using} {Coq}: {Verification} of {Wigderson}'s {Graph} {Coloring} {Algorithm}}}

@inproceedings{roy_x-stream_2013,
	abstract = {X-Stream is a system for processing both in-memory and out-of-core graphs on a single shared-memory machine. While retaining the scatter-gather programming model with state stored in the vertices, X-Stream is novel in (i) using an edge-centric rather than a vertex-centric implementation of this model, and (ii) streaming completely unordered edge lists rather than performing random access. This design is motivated by the fact that sequential bandwidth for all storage media (main memory, SSD, and magnetic disk) is substantially larger than random access bandwidth. We demonstrate that a large number of graph algorithms can be expressed using the edge-centric scatter-gather model. The resulting implementations scale well in terms of number of cores, in terms of number of I/O devices, and across different storage media. X-Stream competes favorably with existing systems for graph processing. Besides sequential access, we identify as one of the main contributors to better performance the fact that X-Stream does not need to sort edge lists during preprocessing.},
	address = {New York, NY, USA},
	author = {Roy, Amitabha and Mihailovic, Ivo and Zwaenepoel, Willy},
	booktitle = {Proceedings of the {Twenty}-{Fourth} {ACM} {Symposium} on {Operating} {Systems} {Principles}},
	doi = {10.1145/2517349.2522740},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/W6DLUDWC/Roy et al. - 2013 - X-Stream edge-centric graph processing using stre.pdf:application/pdf},
	isbn = {978-1-4503-2388-8},
	month = nov,
	pages = {472--488},
	publisher = {Association for Computing Machinery},
	series = {{SOSP} '13},
	shorttitle = {X-{Stream}},
	title = {X-{Stream}: edge-centric graph processing using streaming partitions},
	url = {https://dl.acm.org/doi/10.1145/2517349.2522740},
	urldate = {2023-05-12},
	year = {2013},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/2517349.2522740},
	bdsk-url-2 = {https://doi.org/10.1145/2517349.2522740}}

@article{li_making_nodate,
	abstract = {Online services distribute and replicate state across geographically diverse data centers and direct user requests to the closest or least loaded site. While effectively ensuring low latency responses, this approach is at odds with maintaining cross-site consistency. We make three contributions to address this tension. First, we propose RedBlue consistency, which enables blue operations to be fast (and eventually consistent) while the remaining red operations are strongly consistent (and slow). Second, to make use of fast operation whenever possible and only resort to strong consistency when needed, we identify conditions delineating when operations can be blue and must be red. Third, we introduce a method that increases the space of potential blue operations by breaking them into separate generator and shadow phases. We built a coordination infrastructure called Gemini that offers RedBlue consistency, and we report on our experience modifying the TPC-W and RUBiS benchmarks and an online social network to use Gemini. Our experimental results show that RedBlue consistency provides substantial performance gains without sacrificing consistency.},
	author = {Li, Cheng and Porto, Daniel and Clement, Allen and Gehrke, Johannes and Preguica, Nuno and Rodrigues, Rodrigo},
	file = {Li et al. - Making Geo-Replicated Systems Fast as Possible, Co.pdf:/Users/pandey/Zotero/storage/ANHUKQII/Li et al. - Making Geo-Replicated Systems Fast as Possible, Co.pdf:application/pdf},
	language = {en},
	title = {Making {Geo}-{Replicated} {Systems} {Fast} as {Possible}, {Consistent} when {Necessary}}}

@article{bravo_unistore_nodate,
	abstract = {Modern online services rely on data stores that replicate their data across geographically distributed data centers. Providing strong consistency in such data stores results in high latencies and makes the system vulnerable to network partitions. The alternative of relaxing consistency violates crucial correctness properties. A compromise is to allow multiple consistency levels to coexist in the data store. In this paper we present UNISTORE, the first fault-tolerant and scalable data store that combines causal and strong consistency. The key challenge we address in UNISTORE is to maintain liveness despite data center failures: this could be compromised if a strong transaction takes a dependency on a causal transaction that is later lost because of a failure. UNISTORE ensures that such situations do not arise while paying the cost of durability for causal transactions only when necessary. We evaluate UNISTORE on Amazon EC2 using both microbenchmarks and a sample application. Our results show that UNISTORE effectively and scalably combines causal and strong consistency.},
	author = {Bravo, Manuel and Gotsman, Alexey and de R{\'e}gil, Borja and Wei, Hengfeng},
	file = {Bravo et al. - UNISTORE A fault-tolerant marriage of causal and .pdf:/Users/pandey/Zotero/storage/LNHNLX8M/Bravo et al. - UNISTORE A fault-tolerant marriage of causal and .pdf:application/pdf},
	language = {en},
	title = {{UNISTORE}: {A} fault-tolerant marriage of causal and strong consistency}}

@misc{baquero_pure_2017,
	abstract = {Distributed systems designed to serve clients across the world often make use of geo-replication to attain low latency and high availability. Conflict-free Replicated Data Types (CRDTs) allow the design of predictable multi-master replication and support eventual consistency of replicas that are allowed to transiently diverge. CRDTs come in two flavors: state-based, where a state is changed locally and shipped and merged into other replicas; operation-based, where operations are issued locally and reliably causal broadcast to all other replicas. However, the standard definition of op-based CRDTs is very encompassing, allowing even sending the full-state, and thus imposing storage and dissemination overheads as well as blurring the distinction from state-based CRDTs. We introduce pure op-based CRDTs, that can only send operations to other replicas, drawing a clear distinction from state-based ones. Data types with commutative operations can be trivially implemented as pure op-based CRDTs using standard reliable causal delivery; whereas data types having non-commutative operations are implemented using a PO-Log, a partially ordered log of operations, and making use of an extended API, i.e., a Tagged Causal Stable Broadcast (TCSB), that provides extra causality information upon delivery and later informs when delivered messages become causally stable, allowing further PO-Log compaction. The framework is illustrated by a catalog of pure op-based specifications for classic CRDTs, including counters, multi-value registers, add-wins and remove-wins sets.},
	author = {Baquero, Carlos and Almeida, Paulo Sergio and Shoker, Ali},
	doi = {10.48550/arXiv.1710.04469},
	file = {arXiv Fulltext PDF:/Users/pandey/Zotero/storage/9S39535A/Baquero et al. - 2017 - Pure Operation-Based Replicated Data Types.pdf:application/pdf;arXiv.org Snapshot:/Users/pandey/Zotero/storage/A22LEY9U/1710.html:text/html},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Data Structures and Algorithms},
	month = oct,
	note = {arXiv:1710.04469 [cs]},
	publisher = {arXiv},
	title = {Pure {Operation}-{Based} {Replicated} {Data} {Types}},
	url = {http://arxiv.org/abs/1710.04469},
	urldate = {2023-05-17},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1710.04469},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.1710.04469}}

@inproceedings{sutra_fast_2011,
	abstract = {Replication is a fundamental technique to build fault-tolerant computer systems. This technique relies on consensus, a powerful group communication primitive. A well-known solution to consensus is Fast Paxos. In a recent paper, Leslie Lamport enhances Fast Paxos by leveraging the commutativity of concurrent commands. The new primitive, called Generalized Paxos, reduces the collision rate, and thus the latency of Fast Paxos. However if a collision occurs, the latency of Generalized Paxos equals six message delays, which is higher than Fast Paxos. In this paper we present a solution to reduce recovery delay when a collision occurs. Our algorithm toleratesf {\textless} n/2 replicas crashes, is genuine, i.e, it tries not to order commuting commands, and recovers in one step when a collision occurs.},
	address = {Madrid, Spain},
	author = {Sutra, Pierre and Shapiro, Marc},
	booktitle = {2011 {IEEE} 30th {International} {Symposium} on {Reliable} {Distributed} {Systems}},
	doi = {10.1109/SRDS.2011.38},
	file = {Sutra and Shapiro - 2011 - Fast Genuine Generalized Consensus.pdf:/Users/pandey/Zotero/storage/G64JVXUH/Sutra and Shapiro - 2011 - Fast Genuine Generalized Consensus.pdf:application/pdf},
	isbn = {978-1-4577-1349-1},
	language = {en},
	month = oct,
	pages = {255--264},
	publisher = {IEEE},
	title = {Fast {Genuine} {Generalized} {Consensus}},
	url = {http://ieeexplore.ieee.org/document/6076784/},
	urldate = {2023-05-17},
	year = {2011},
	bdsk-url-1 = {http://ieeexplore.ieee.org/document/6076784/},
	bdsk-url-2 = {https://doi.org/10.1109/SRDS.2011.38}}

@inproceedings{enes_efficient_2021,
	abstract = {Modern web applications replicate their data across the globe and require strong consistency guarantees for their most critical data. These guarantees are usually provided via statemachine replication (SMR). Recent advances in SMR have focused on leaderless protocols, which improve the availability and performance of traditional Paxos-based solutions. We propose Tempo -- a leaderless SMR protocol that, in comparison to prior solutions, achieves superior throughput and offers predictable performance even in contended workloads. To achieve these benefits, Tempo timestamps each application command and executes it only after the timestamp becomes stable, i.e., all commands with a lower timestamp are known. Both the timestamping and stability detection mechanisms are fully decentralized, thus obviating the need for a leader replica. Our protocol furthermore generalizes to partial replication settings, enabling scalability in highly parallel workloads. We evaluate the protocol in both real and simulated geo-distributed environments and demonstrate that it outperforms state-of-the-art alternatives.},
	address = {Online Event United Kingdom},
	author = {Enes, Vitor and Baquero, Carlos and Gotsman, Alexey and Sutra, Pierre},
	booktitle = {Proceedings of the {Sixteenth} {European} {Conference} on {Computer} {Systems}},
	doi = {10.1145/3447786.3456236},
	file = {Enes et al. - 2021 - Efficient replication via timestamp stability.pdf:/Users/pandey/Zotero/storage/8ZUAXUFF/Enes et al. - 2021 - Efficient replication via timestamp stability.pdf:application/pdf},
	isbn = {978-1-4503-8334-9},
	language = {en},
	month = apr,
	pages = {178--193},
	publisher = {ACM},
	title = {Efficient replication via timestamp stability},
	url = {https://dl.acm.org/doi/10.1145/3447786.3456236},
	urldate = {2023-05-22},
	year = {2021},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3447786.3456236},
	bdsk-url-2 = {https://doi.org/10.1145/3447786.3456236}}

@misc{noauthor_complexity_nodate,
	file = {On the complexity of checking transactional consis.pdf:/Users/pandey/Zotero/storage/VHYM9FAM/On the complexity of checking transactional consis.pdf:application/pdf;On the complexity of checking transactional consistency | Proceedings of the ACM on Programming Languages:/Users/pandey/Zotero/storage/QCKQPXT8/3360591.html:text/html},
	title = {On the complexity of checking transactional consistency {\textbar} {Proceedings} of the {ACM} on {Programming} {Languages}},
	url = {https://dl.acm.org/doi/abs/10.1145/3360591},
	urldate = {2023-05-26},
	bdsk-url-1 = {https://dl.acm.org/doi/abs/10.1145/3360591}}

@article{biswas_monkeydb_2021,
	abstract = {Modern applications, such as social networking systems and e-commerce platforms are centered around using large-scale storage systems for storing and retrieving data. In the presence of concurrent accesses, these storage systems trade off isolation for performance. The weaker the isolation level, the more behaviors a storage system is allowed to exhibit and it is up to the developer to ensure that their application can tolerate those behaviors. However, these weak behaviors only occur rarely in practice and outside the control of the application, making it difficult for developers to test the robustness of their code against weak isolation levels. This paper presents MonkeyDB, a mock storage system for testing storage-backed applications. MonkeyDB supports a key-value interface as well as SQL queries under multiple isolation levels. It uses a logical specification of the isolation level to compute, on a read operation, the set of all possible return values. MonkeyDB then returns a value randomly from this set. We show that MonkeyDB provides good coverage of weak behaviors, which is complete in the limit. We test a variety of applications for assertions that fail only under weak isolation. MonkeyDB is able to break each of those assertions in a small number of attempts.},
	author = {Biswas, Ranadeep and Kakwani, Diptanshu and Vedurada, Jyothi and Enea, Constantin and Lal, Akash},
	doi = {10.1145/3485546},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/4N2KE8LP/Biswas et al. - 2021 - MonkeyDB effectively testing correctness under we.pdf:application/pdf},
	journal = {Proceedings of the ACM on Programming Languages},
	keywords = {Applications of Storage Systems, Testing, Transactional Databases, Weak Isolation Levels},
	month = oct,
	number = {OOPSLA},
	pages = {132:1--132:27},
	shorttitle = {{MonkeyDB}},
	title = {{MonkeyDB}: effectively testing correctness under weak isolation levels},
	url = {https://dl.acm.org/doi/10.1145/3485546},
	urldate = {2023-05-26},
	volume = {5},
	year = {2021},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3485546},
	bdsk-url-2 = {https://doi.org/10.1145/3485546}}

@article{sergey_programming_2018,
	abstract = {Distributed systems play a crucial role in modern infrastructure, but are notoriously difficult to implement correctly. This difficulty arises from two main challenges: (a) correctly implementing core system components (e.g., two-phase commit), so all their internal invariants hold, and (b) correctly composing standalone system components into functioning trustworthy applications (e.g., persistent storage built on top of a two-phase commit instance). Recent work has developed several approaches for addressing (a) by means of mechanically verifying implementations of core distributed components, but no methodology exists to address (b) by composing such verified components into larger verified applications. As a result, expensive verification efforts for key system components are not easily reusable, which hinders further verification efforts.
            In this paper, we present Disel, the first framework for implementation and compositional verification of distributed systems and their clients, all within the mechanized, foundational context of the Coq proof assistant. In Disel, users implement distributed systems using a domain specific language shallowly embedded in Coq and providing both high-level programming constructs as well as low-level communication primitives. Components of composite systems are specified in Disel as protocols, which capture system-specific logic and disentangle system definitions from implementation details. By virtue of Disel's dependent type system, well-typed implementations always satisfy their protocols' invariants and never go wrong, allowing users to verify system implementations interactively using Disel's Hoare-style program logic, which extends state-of-the-art techniques for concurrency verification to the distributed setting. By virtue of the substitution principle and frame rule provided by Disel's logic, system components can be composed leading to modular, reusable verified distributed systems.
            We describe Disel, illustrate its use with a series of examples, outline its logic and metatheory, and report on our experience using it as a framework for implementing, specifying, and verifying distributed systems.},
	author = {Sergey, Ilya and Wilcox, James R. and Tatlock, Zachary},
	doi = {10.1145/3158116},
	file = {Sergey et al. - 2018 - Programming and proving with distributed protocols.pdf:/Users/pandey/Zotero/storage/3EXRMD7N/Sergey et al. - 2018 - Programming and proving with distributed protocols.pdf:application/pdf},
	issn = {2475-1421},
	journal = {Proceedings of the ACM on Programming Languages},
	language = {en},
	month = jan,
	number = {POPL},
	pages = {1--30},
	title = {Programming and proving with distributed protocols},
	url = {https://dl.acm.org/doi/10.1145/3158116},
	urldate = {2023-05-26},
	volume = {2},
	year = {2018},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3158116},
	bdsk-url-2 = {https://doi.org/10.1145/3158116}}

@inproceedings{fernandes_probabilistic_2023,
	abstract = {Conflict-free Replicated Data Types (CRDTs) are useful to allow a distributed system to operate on data even when partitions occur, and thus preserve operational availability. Most CRDTs need to track whether data evolved concurrently at different nodes and needs to be reconciled; this requires storing causality metadata that is proportional to the number of nodes. In this paper, we try to overcome this limitation by introducing a stochastic mechanism that is no longer linear on the number of nodes, but whose accuracy is now tied to how much divergence occurs between synchronizations. This provides a new tool that can be useful in deployments with many anonymous nodes and frequent synchronizations. However, there is an underlying trade-off with classic deterministic solutions, since the approach is now probabilistic and the accuracy depends on the configurable metadata space size.},
	address = {New York, NY, USA},
	author = {Fernandes, Pedro Henrique and Baquero, Carlos},
	booktitle = {Proceedings of the 10th {Workshop} on {Principles} and {Practice} of {Consistency} for {Distributed} {Data}},
	doi = {10.1145/3578358.3591331},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/TF7LYKHK/Fernandes and Baquero - 2023 - Probabilistic Causal Contexts for Scalable CRDTs.pdf:application/pdf},
	isbn = {9798400700866},
	keywords = {bloom filters, conflict-free replicated data types (CRDTs), eventual consistency},
	month = may,
	pages = {1--8},
	publisher = {Association for Computing Machinery},
	series = {{PaPoC} '23},
	title = {Probabilistic {Causal} {Contexts} for {Scalable} {CRDTs}},
	url = {https://dl.acm.org/doi/10.1145/3578358.3591331},
	urldate = {2023-05-26},
	year = {2023},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3578358.3591331},
	bdsk-url-2 = {https://doi.org/10.1145/3578358.3591331}}

@article{gondelman_distributed_2021,
	abstract = {We present the first specification and verification of an implementation of a causally-consistent distributed database that supports modular verification of full functional correctness properties of clients and servers. We specify and reason about the causally-consistent distributed database in Aneris, a higher-order distributed separation logic for an ML-like programming language with network primitives for programming distributed systems. We demonstrate that our specifications are useful, by proving the correctness of small, but tricky, synthetic examples involving causal dependency and by verifying a session manager library implemented on top of the distributed database. We use Aneris's facilities for modular specification and verification to obtain a highly modular development, where each component is verified in isolation, relying only on the specifications (not the implementations) of other components. We have used the Coq formalization of the Aneris logic to formalize all the results presented in the paper in the Coq proof assistant.},
	author = {Gondelman, L{\'e}on and Gregersen, Simon Oddershede and Nieto, Abel and Timany, Amin and Birkedal, Lars},
	doi = {10.1145/3434323},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/NRS2STUP/Gondelman et al. - 2021 - Distributed causal memory modular specification a.pdf:application/pdf},
	journal = {Proceedings of the ACM on Programming Languages},
	keywords = {causal consistency, Distributed systems, concurrency, formal verification, higher-order logic, separation logic},
	month = jan,
	number = {POPL},
	pages = {42:1--42:29},
	shorttitle = {Distributed causal memory},
	title = {Distributed causal memory: modular specification and verification in higher-order distributed separation logic},
	url = {https://dl.acm.org/doi/10.1145/3434323},
	urldate = {2023-05-26},
	volume = {5},
	year = {2021},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3434323},
	bdsk-url-2 = {https://doi.org/10.1145/3434323}}

@article{sprenger_igloo_2020,
	abstract = {Lighthouse projects like CompCert, seL4, IronFleet, and DeepSpec have demonstrated that full system verification is feasible by establishing a refinement between an abstract system specification and an executable implementation. Existing approaches however impose severe restrictions on the abstract system specifications due to their limited expressiveness or versatility, or on the executable code due to their use of suboptimal code extraction or inexpressive program logics. We propose a novel methodology that combines the compositional refinement of event-based models of distributed systems with the verification of full-fledged program code using expressive separation logics, which support features of realistic programming languages like heap data structures and concurrency. Our main technical contribution is a formal framework that soundly relates event-based system models to program specifications in separation logics. This enables protocol development tools to soundly interoperate with program verifiers to establish a refinement between the model and the code. We formalized our framework, Igloo, in Isabelle/HOL. We report on three case studies, a leader election protocol, a replication protocol, and a security protocol, for which we refine formal requirements into program specifications that we implement in Java and Python and prove correct using the VeriFast and Nagini tools.},
	author = {Sprenger, Christoph and Klenze, Tobias and Eilers, Marco and Wolf, Felix A. and M{\"u}ller, Peter and Clochard, Martin and Basin, David},
	doi = {10.1145/3428220},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/TTDY6J4V/Sprenger et al. - 2020 - Igloo soundly linking compositional refinement an.pdf:application/pdf},
	journal = {Proceedings of the ACM on Programming Languages},
	keywords = {distributed systems, higher-order logic, separation logic, compositional refinement, end-to-end verification, fault-tolerance, leader election, security protocols, tool interoperability},
	month = nov,
	number = {OOPSLA},
	pages = {152:1--152:31},
	shorttitle = {Igloo},
	title = {Igloo: soundly linking compositional refinement and separation logic for distributed system verification},
	url = {https://dl.acm.org/doi/10.1145/3428220},
	urldate = {2023-05-26},
	volume = {4},
	year = {2020},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3428220},
	bdsk-url-2 = {https://doi.org/10.1145/3428220}}

@article{wilcox_verdi_nodate,
	abstract = {Distributed systems are difficult to implement correctly because they must handle both concurrency and failures: machines may crash at arbitrary points and networks may reorder, drop, or duplicate packets. Further, their behavior is often too complex to permit exhaustive testing. Bugs in these systems have led to the loss of critical data and unacceptable service outages.},
	author = {Wilcox, James R and Woos, Doug and Panchekha, Pavel and Tatlock, Zachary and Wang, Xi and Ernst, Michael D and Anderson, Thomas},
	file = {Wilcox et al. - Verdi A Framework for Implementing and Formally V.pdf:/Users/pandey/Zotero/storage/ZIQW2K7A/Wilcox et al. - Verdi A Framework for Implementing and Formally V.pdf:application/pdf},
	language = {en},
	title = {Verdi: {A} {Framework} for {Implementing} and {Formally} {Verifying} {Distributed} {Systems}}}

@inproceedings{wang_model_2023,
	abstract = {Distributed systems have become the backbone of cloud computing. Incorrect system designs and implementations can greatly impair the reliability of distributed systems. Although a distributed system design modelled in the formal specification can be verified by formal model checking, it is still challenging to figure out whether its corresponding implementation conforms to the verified specification. An incorrect system implementation can violate its verified specification, and causes intricate bugs. In this paper, we propose a novel distributed system testing technique, Model checking guided testing (Mocket), to fill the gap between the specification and its implementation in a distributed system. Specially, we use the state space generated by formal model checking to guide the testing for the system implementation, and unearth bugs in the target distributed system. To evaluate the feasibility and effectiveness of Mocket, we apply Mocket on three popular distributed systems, and find 3 previously unknown bugs in them.},
	address = {Rome Italy},
	author = {Wang, Dong and Dou, Wensheng and Gao, Yu and Wu, Chenao and Wei, Jun and Huang, Tao},
	booktitle = {Proceedings of the {Eighteenth} {European} {Conference} on {Computer} {Systems}},
	doi = {10.1145/3552326.3587442},
	file = {Wang et al. - 2023 - Model Checking Guided Testing for Distributed Syst.pdf:/Users/pandey/Zotero/storage/V2F9DKV5/Wang et al. - 2023 - Model Checking Guided Testing for Distributed Syst.pdf:application/pdf},
	isbn = {978-1-4503-9487-1},
	language = {en},
	month = may,
	pages = {127--143},
	publisher = {ACM},
	title = {Model {Checking} {Guided} {Testing} for {Distributed} {Systems}},
	url = {https://dl.acm.org/doi/10.1145/3552326.3587442},
	urldate = {2023-05-27},
	year = {2023},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3552326.3587442},
	bdsk-url-2 = {https://doi.org/10.1145/3552326.3587442}}

@inproceedings{bornholt_using_2021-1,
	abstract = {This paper reports our experience applying lightweight formal methods to validate the correctness of ShardStore, a new key-value storage node implementation for the Amazon S3 cloud object storage service. By "lightweight formal methods" we mean a pragmatic approach to verifying the correctness of a production storage node that is under ongoing feature development by a full-time engineering team. We do not aim to achieve full formal verification, but instead emphasize automation, usability, and the ability to continually ensure correctness as both software and its specification evolve over time. Our approach decomposes correctness into independent properties, each checked by the most appropriate tool, and develops executable reference models as specifications to be checked against the implementation. Our work has prevented 16 issues from reaching production, including subtle crash consistency and concurrency problems, and has been extended by non-formal-methods experts to check new features and properties as ShardStore has evolved.},
	address = {New York, NY, USA},
	author = {Bornholt, James and Joshi, Rajeev and Astrauskas, Vytautas and Cully, Brendan and Kragl, Bernhard and Markle, Seth and Sauri, Kyle and Schleit, Drew and Slatton, Grant and Tasiran, Serdar and Van Geffen, Jacob and Warfield, Andrew},
	booktitle = {Proceedings of the {ACM} {SIGOPS} 28th {Symposium} on {Operating} {Systems} {Principles}},
	doi = {10.1145/3477132.3483540},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/FAZ7WTNR/Bornholt et al. - 2021 - Using Lightweight Formal Methods to Validate a Key.pdf:application/pdf},
	isbn = {978-1-4503-8709-5},
	keywords = {cloud storage, lightweight formal methods},
	month = oct,
	pages = {836--850},
	publisher = {Association for Computing Machinery},
	series = {{SOSP} '21},
	title = {Using {Lightweight} {Formal} {Methods} to {Validate} a {Key}-{Value} {Storage} {Node} in {Amazon} {S3}},
	url = {https://dl.acm.org/doi/10.1145/3477132.3483540},
	urldate = {2023-06-02},
	year = {2021},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3477132.3483540},
	bdsk-url-2 = {https://doi.org/10.1145/3477132.3483540}}

@article{mcmillan_methodology_2000,
	abstract = {A methodology for system-level hardware veri{\"y}cation based on compositional model checking is described. This methodology relies on a simple set of proof techniques, and a domain speci{\"y}c strategy for applying them. The goal of this strategy is to reduce the veri{\"y}cation of a large system to {\"y}nite state subgoals that are tractable in both size and number. These subgoals are then discharged by model checking. The proof strategy uses proof techniques for design re{\"y}nement, temporal case splitting, data-type reduction and the exploitation of symmetry. Uninterpreted functions can be used to abstract operations on data. A proof system supporting this approach generates veri{\"y}cation subgoals to be discharged by the SMV symbolic model checker. Application of the methodology is illustrated using an implementation of Tomasulo's algorithm, a packet bu ering device and a cache coherence protocol as examples. c 2000 Published by Elsevier Science B.V. All rights reserved.},
	author = {McMillan, K L},
	file = {McMillan - 2000 - A methodology for hardware veri{\"y}cation using compo.pdf:/Users/pandey/Zotero/storage/K99NH3F7/McMillan - 2000 - A methodology for hardware veri{\"y}cation using compo.pdf:application/pdf},
	journal = {Science of Computer Programming},
	language = {en},
	title = {A methodology for hardware veri{\"y}cation using compositional model checking},
	year = {2000}}

@article{desai_compositional_2018,
	abstract = {A real-world distributed system is rarely implemented as a standalone monolithic system. Instead, it is composed of multiple independent interacting components that together ensure the desired system-level specification. One can scale systematic testing to large, industrial-scale implementations by decomposing the system-level testing problem into a collection of simpler component-level testing problems.
            This paper proposes techniques for compositional programming and testing of distributed systems with two central contributions: (1) We propose a module system based on the theory of compositional trace refinement for dynamic systems consisting of asynchronously-communicating state machines, where state machines can be dynamically created, and communication topology of the existing state machines can change at runtime; (2) We present ModP, a programming system that implements our module system to enable compositional reasoning (assume-guarantee) of distributed systems.
            We demonstrate the efficacy of our framework by building two practical fault-tolerant distributed systems, a transaction-commit service and a replicated hash-table. ModP helps implement these systems modularly and validate them via compositional testing. We empirically demonstrate that the abstraction-based compositional reasoning approach helps amplify the coverage during testing and scale it to real-world distributed systems. The distributed services built using ModP achieve performance comparable to open-source equivalents.},
	author = {Desai, Ankush and Phanishayee, Amar and Qadeer, Shaz and Seshia, Sanjit A.},
	doi = {10.1145/3276529},
	file = {Desai et al. - 2018 - Compositional programming and testing of dynamic d.pdf:/Users/pandey/Zotero/storage/FJT7J8YK/Desai et al. - 2018 - Compositional programming and testing of dynamic d.pdf:application/pdf},
	issn = {2475-1421},
	journal = {Proceedings of the ACM on Programming Languages},
	language = {en},
	month = oct,
	number = {OOPSLA},
	pages = {1--30},
	title = {Compositional programming and testing of dynamic distributed systems},
	url = {https://dl.acm.org/doi/10.1145/3276529},
	urldate = {2023-06-04},
	volume = {2},
	year = {2018},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3276529},
	bdsk-url-2 = {https://doi.org/10.1145/3276529}}

@article{hance_sharding_2023,
	abstract = {We present IronSync, an automated verification framework for concurrent code with shared memory. IronSync scales to complex systems by splitting system-wide proofs into isolated concerns such that each can be substantially automated. As a starting point, IronSync's ownership type system allows a developer to straightforwardly prove both data safety and the logical correctness of thread-local operations. IronSync then introduces the concept of a Localized Transition System, which connects the correctness of local actions to the correctness of the entire system. We demonstrate IronSync by verifying two state-of-the-art concurrent systems comprising thousands of lines: a library for black-box replication on NUMA architectures, and a highly concurrent page cache.},
	author = {Hance, Travis and Zhou, Yi and Lattuada, Andrea and Achermann, Reto and Conway, Alex and Stutsman, Ryan and Zellweger, Gerd and Hawblitzel, Chris and Howell, Jon and Parno, Bryan},
	file = {Hance et al. - 2023 - Sharding the State Machine Automated Modular Reas.pdf:/Users/pandey/Zotero/storage/GFLGU4ID/Hance et al. - 2023 - Sharding the State Machine Automated Modular Reas.pdf:application/pdf},
	language = {en},
	title = {Sharding the {State} {Machine}: {Automated} {Modular} {Reasoning} for {Complex} {Concurrent} {Systems}},
	year = {2023}}

@inproceedings{crooks_seeing_2017,
	abstract = {This paper introduces the first state-based formalization of isolation guarantees. Our approach is premised on a simple observation: applications view storage systems as black-boxes that transition through a series of states, a subset of which are observed by applications. Defining isolation guarantees in terms of these states frees definitions from implementation-specific assumptions. It makes immediately clear what anomalies, if any, applications can expect to observe, thus bridging the gap that exists today between how isolation guarantees are defined and how they are perceived. The clarity that results from definitions based on client-observable states brings forth several benefits. First, it allows us to easily compare the guarantees of distinct, but semantically close, isolation guarantees. We find that several well-known guarantees, previously thought to be distinct, are in fact equivalent, and that many previously incomparable flavors of snapshot isolation can be organized in a clean hierarchy. Second, freeing definitions from implementation-specific artefacts can suggest more efficient implementations of the same isolation guarantee. We show how a client-centric implementation of parallel snapshot isolation can be more resilient to slowdown cascades, a common phenomenon in large-scale datacenters.},
	address = {New York, NY, USA},
	author = {Crooks, Natacha and Pu, Youer and Alvisi, Lorenzo and Clement, Allen},
	booktitle = {Proceedings of the {ACM} {Symposium} on {Principles} of {Distributed} {Computing}},
	doi = {10.1145/3087801.3087802},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/EZE8VG9I/Crooks et al. - 2017 - Seeing is Believing A Client-Centric Specificatio.pdf:application/pdf},
	isbn = {978-1-4503-4992-5},
	keywords = {transactions, database, distributed systems, serializability, concurrency control, eventual consistency, cloud storage, acid, consistency, distributed storage, isolation, weak consistency},
	month = jul,
	pages = {73--82},
	publisher = {Association for Computing Machinery},
	series = {{PODC} '17},
	shorttitle = {Seeing is {Believing}},
	title = {Seeing is {Believing}: {A} {Client}-{Centric} {Specification} of {Database} {Isolation}},
	url = {https://dl.acm.org/doi/10.1145/3087801.3087802},
	urldate = {2023-06-04},
	year = {2017},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3087801.3087802},
	bdsk-url-2 = {https://doi.org/10.1145/3087801.3087802}}

@article{irmert_new_2008,
	abstract = {In this paper we present our approach towards a modularized database management system (DBMS) whose components can be adapted at runtime and show the modularization of a DBMS beneath the record-oriented interface as a first step. Cross-cutting concerns like transactions pose thereby a challenge that we answer with aspect-oriented programming (AOP). Finally we show the implementation techniques that enable the exchange of database modules dynamically. Particularly with regard to stateful components we define a service adaptation process that preserves and transmits the component's state.},
	author = {Irmert, Florian and Daum, Michael and Meyer-Wegener, Klaus},
	doi = {10.1145/1385486.1385498},
	file = {Irmert et al. - 2008 - A new approach to modular database systems.pdf:/Users/pandey/Zotero/storage/BLHN4C3V/Irmert et al. - 2008 - A new approach to modular database systems.pdf:application/pdf},
	journal = {SETMDM '08},
	month = mar,
	note = {MAG ID: 2024743951 S2ID: b823e6df7612686f8b51093525e7782a5b25bfff},
	pages = {40--44},
	title = {A new approach to modular database systems},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1145/1385486.1385498}}

@article{carey_architecture_1986,
	abstract = {With non-traditional application areas such as engineering design, image/voice data management, scientific/statistical applications, and artificial intelligence systems all clamoring for ways to store and efficiently process larger and larger volumes of data, it is clear that traditional database technology has been pushed to its limits. It also seems clear that no single database system will be capable of simultaneously meeting the functionality and performance requirements of such a diverse set of applications. In this paper we describe the preliminary design of EXODUS, an extensible database system that will facilitate the fast development of high-performance, application-specific database systems. EXODUS provides certain kernel facilities, including a versatile storage manager and a type manager. In addition, it provides an architectural framework for building application-specific database systems, tools to partially automate the generation of such systems, and libraries of software components (e.g., access methods) that are likely to be useful for many application domains.},
	author = {Carey, Michael J. and DeWitt, David J. and Frank, Daniel and Muralikrishna, M. and Graefe, Goetz and Richardson, Joel E. and Shekita, Eugene J.},
	doi = {10.1007/978-3-642-84374-7_15},
	file = {Submitted Version:/Users/pandey/Zotero/storage/MJCDAV9E/Carey et al. - 1986 - The architecture of the EXODUS extensible DBMS.pdf:application/pdf},
	month = sep,
	note = {MAG ID: 1694233656},
	pages = {52--65},
	title = {The architecture of the {EXODUS} extensible {DBMS}},
	year = {1986},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-642-84374-7_15}}

@article{geppert_kids_1997,
	abstract = {This paper describes the KIDS approach to database management system (DBMS) construction. KIDS aims at the development of a DBMS-construction approach by defining specification-based approaches strongly relying on software reuse. We propose an architecture model that supports reuse of architectures, but is also able to integrate various DBMS components into a coherent entirety. Second, KIDS defines a construction process model that strives for reuse in all phases. We have identified three important aspects that are initially addressed by the construction approach: the data model aspect, the transaction management aspect, and the integrity management aspect. For each of these aspects, a customized approach how to construct the corresponding subsystem has been developed, again relying on software reuse wherever possible. While these aspects have been considered because of their relevance for any DBMS, the construction method is open for further aspects.},
	author = {Geppert, Andreas and Scherrer, Stefan and Dittrich, Klaus R. and Dittrich, Klaus R.},
	month = jan,
	note = {MAG ID: 1599241092},
	title = {{KIDS}: {Construction} of {Database} {Management} {Systems} based on {Reuse}},
	year = {1997}}

@article{simion_slingshot_2015,
	abstract = {Traditional relational database engines have been losing ground to specialized data processing engines in virtually every market segment, from data warehousing, OLTP, and stream processing, to scientific applications. Although relational database engines are evolving to leverage new technologies and more efficient processing paradigms, the generality of a large monolithic engine often makes this a significant effort. Our aim is to delimit and decouple database engine components to design a more lightweight and flexible data processing engine that can support any application domain efficiently and without the effort of a complete redesign. We introduce Slingshot, a new data processing engine, where modularity and implementation flexibility are the top priority. Its core database engine is minimal and mainly handles inter-operation of the database components. Each component, abstracted by an interface, can be externally implemented and plugged into the framework as a module that handles the component's functionality. As a result, this allows designers the liberty to choose suitable features for their target applications, to drop excess functionality, and to optimize code independent of the rest of the engine. We compare Slingshot to a traditional RDBMS and to custom solutions on queries that are representative of three application types (spatial, OLAP, and OLTP). We show that Slingshot outperforms the RDBMS in most cases, while performing comparably in others. Furthermore, Slingshot performs better or comparable to custom solutions on most tests. Finally, Slingshot's flexibility allows us to efficiently leverage computer architectures such as GPUs for speeding up complex computational tasks.},
	author = {Simion, Bogdan and Ilha, Daniel N. and Ray, Suprio and Barron, Leslie and Brown, Angela Demke and Johnson, Ryan},
	doi = {10.1109/bigdata.2015.7363783},
	file = {Simion et al. - 2015 - Slingshot A modular framework for designing data .pdf:/Users/pandey/Zotero/storage/D7Q7U3G2/Simion et al. - 2015 - Slingshot A modular framework for designing data .pdf:application/pdf},
	month = oct,
	note = {MAG ID: 2199534348},
	pages = {421--430},
	title = {Slingshot: {A} modular framework for designing data processing systems},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/bigdata.2015.7363783}}

@misc{lu_ncc_2023,
	abstract = {Strictly serializable datastores greatly simplify the development of correct applications by providing strong consistency guarantees. However, existing techniques pay unnecessary costs for naturally consistent transactions, which arrive at servers in an order that is already strictly serializable. We find these transactions are prevalent in datacenter workloads. We exploit this natural arrival order by executing transaction requests with minimal costs while optimistically assuming they are naturally consistent, and then leverage a timestamp-based technique to efficiently verify if the execution is indeed consistent. In the process of designing such a timestamp-based technique, we identify a fundamental pitfall in relying on timestamps to provide strict serializability, and name it the timestamp-inversion pitfall. We find timestamp-inversion has affected several existing works. We present Natural Concurrency Control (NCC), a new concurrency control technique that guarantees strict serializability and ensures minimal costs -- i.e., one-round latency, lock-free, and non-blocking execution -- in the best (and common) case by leveraging natural consistency. NCC is enabled by three key components: non-blocking execution, decoupled response control, and timestamp-based consistency check. NCC avoids timestamp-inversion with a new technique: response timing control, and proposes two optimization techniques, asynchrony-aware timestamps and smart retry, to reduce false aborts. Moreover, NCC designs a specialized protocol for read-only transactions, which is the first to achieve the optimal best-case performance while ensuring strict serializability, without relying on synchronized clocks. Our evaluation shows that NCC outperforms state-of-the-art solutions by an order of magnitude on many workloads.},
	author = {Lu, Haonan and Mu, Shuai and Sen, Siddhartha and Lloyd, Wyatt},
	doi = {10.48550/arXiv.2305.14270},
	file = {arXiv Fulltext PDF:/Users/pandey/Zotero/storage/7JDXATP7/Lu et al. - 2023 - NCC Natural Concurrency Control for Strictly Seri.pdf:application/pdf;arXiv.org Snapshot:/Users/pandey/Zotero/storage/JRHX8KMF/2305.html:text/html},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, C.2.4},
	month = may,
	note = {arXiv:2305.14270 [cs]},
	publisher = {arXiv},
	shorttitle = {{NCC}},
	title = {{NCC}: {Natural} {Concurrency} {Control} for {Strictly} {Serializable} {Datastores} by {Avoiding} the {Timestamp}-{Inversion} {Pitfall}},
	url = {http://arxiv.org/abs/2305.14270},
	urldate = {2023-06-13},
	year = {2023},
	bdsk-url-1 = {http://arxiv.org/abs/2305.14270},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2305.14270}}

@inproceedings{idreos_data_2018,
	abstract = {Data structures are critical in any data-driven scenario, but they are notoriously hard to design due to a massive design space and the dependence of performance on workload and hardware which evolve continuously. We present a design engine, the Data Calculator, which enables interactive and semi-automated design of data structures. It brings two innovations. First, it offers a set of fine-grained design primitives that capture the first principles of data layout design: how data structure nodes lay data out, and how they are positioned relative to each other. This allows for a structured description of the universe of possible data structure designs that can be synthesized as combinations of those primitives. The second innovation is computation of performance using learned cost models. These models are trained on diverse hardware and data profiles and capture the cost properties of fundamental data access primitives (e.g., random access). With these models, we synthesize the performance cost of complex operations on arbitrary data structure designs without having to: 1) implement the data structure, 2) run the workload, or even 3) access the target hardware. We demonstrate that the Data Calculator can assist data structure designers and researchers by accurately answering rich what-if design questions on the order of a few seconds or minutes, i.e., computing how the performance (response time) of a given data structure design is impacted by variations in the: 1) design, 2) hardware, 3) data, and 4) query workloads. This makes it effortless to test numerous designs and ideas before embarking on lengthy implementation, deployment, and hardware acquisition steps. We also demonstrate that the Data Calculator can synthesize entirely new designs, auto-complete partial designs, and detect suboptimal design choices.},
	address = {New York, NY, USA},
	author = {Idreos, Stratos and Zoumpatianos, Kostas and Hentschel, Brian and Kester, Michael S. and Guo, Demi},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Management} of {Data}},
	doi = {10.1145/3183713.3199671},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/6UUKF7PS/Idreos et al. - 2018 - The Data Calculator Data Structure Design and Cos.pdf:application/pdf},
	isbn = {978-1-4503-4703-7},
	keywords = {data structure synthesis, learned cost models},
	month = may,
	pages = {535--550},
	publisher = {Association for Computing Machinery},
	series = {{SIGMOD} '18},
	shorttitle = {The {Data} {Calculator}},
	title = {The {Data} {Calculator}: {Data} {Structure} {Design} and {Cost} {Synthesis} from {First} {Principles} and {Learned} {Cost} {Models}},
	url = {https://dl.acm.org/doi/10.1145/3183713.3199671},
	urldate = {2023-06-13},
	year = {2018},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3183713.3199671},
	bdsk-url-2 = {https://doi.org/10.1145/3183713.3199671}}

@article{chen_g-tran_2022,
	abstract = {Graph transaction processing poses unique challenges such as random data access due to the irregularity of graph structures, low throughput and high abort rate due to the relatively large read/write sets in graph transactions. To address these challenges, we present G-Tran, a remote direct memory access (RDMA)-enabled distributed in-memory graph database with serializable and snapshot isolation support. First, we propose a graph-native data store to achieve good data locality and fast data access for transactional updates and queries. Second, G-Tran adopts a fully decentralized architecture that leverages RDMA to process distributed transactions with the massively parallel processing (MPP) model, which can achieve high performance by utilizing all computing resources. In addition, we propose a new multi-version optimistic concurrency control (MV-OCC) protocol with two optimizations to address the issue of large read/write sets in graph transactions. Extensive experiments show that G-Tran achieves competitive performance compared with other popular graph databases on benchmark workloads.},
	author = {Chen, Hongzhi and Li, Changji and Zheng, Chenguang and Huang, Chenghuan and Fang, Juncheng and Cheng, James and Zhang, Jian},
	doi = {10.14778/3551793.3551813},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/8CYZ4ZMM/Chen et al. - 2022 - G-tran a high performance distributed graph datab.pdf:application/pdf},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	month = jul,
	number = {11},
	pages = {2545--2558},
	shorttitle = {G-tran},
	title = {G-tran: a high performance distributed graph database with a decentralized architecture},
	url = {https://dl.acm.org/doi/10.14778/3551793.3551813},
	urldate = {2023-06-26},
	volume = {15},
	year = {2022},
	bdsk-url-1 = {https://dl.acm.org/doi/10.14778/3551793.3551813},
	bdsk-url-2 = {https://doi.org/10.14778/3551793.3551813}}

@article{ezhilchelvan_design_2019,
	author = {Ezhilchelvan, P. and Mitrani, I. and Waudby, J. and Webber, J.},
	doi = {10.1007/978-3-030-44411-2_4},
	file = {Ezhilchelvan et al. - 2019 - Design and Evaluation of an Edge Concurrency Contr.pdf:/Users/pandey/Zotero/storage/H8JP73FE/Ezhilchelvan et al. - 2019 - Design and Evaluation of an Edge Concurrency Contr.pdf:application/pdf;Snapshot:/Users/pandey/Zotero/storage/D7MSE2NU/262429.html:text/html},
	journal = {16th European Performance Engineering Workshop (EPEW 2019)},
	language = {en},
	note = {Publisher: Newcastle University},
	title = {Design and {Evaluation} of an {Edge} {Concurrency} {Control} {Protocol} for {Distributed} {Graph} {Databases}},
	url = {https://eprints.ncl.ac.uk},
	urldate = {2023-06-26},
	year = {2019},
	bdsk-url-1 = {https://eprints.ncl.ac.uk},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-44411-2_4}}

@article{ezhilchelvan_degradation_2018,
	author = {Ezhilchelvan, Paul and Mitrani, Isi and Webber, Jim},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/EMFKD7NM/Ezhilchelvan et al. - 2018 - On the degradation of distributed graph databases .pdf:application/pdf},
	journal = {School of Computing Technical report Series},
	language = {en},
	note = {Publisher: Newcastle University},
	title = {On the degradation of distributed graph databases with eventual consistency},
	url = {https://eprints.ncl.ac.uk},
	urldate = {2023-06-26},
	year = {2018},
	bdsk-url-1 = {https://eprints.ncl.ac.uk}}

@article{huang_leopard_2016,
	abstract = {This paper introduces a dynamic graph partitioning algorithm, designed for large, constantly changing graphs. We propose a partitioning framework that adjusts on the fly as the graph structure changes. We also introduce a replication algorithm that is tightly integrated with the partitioning algorithm, which further reduces the number of edges cut by the partitioning algorithm. Even though the proposed approach is handicapped by only taking into consideration local parts of the graph when reassigning vertices, extensive evaluation shows that the proposed approach maintains a quality partitioning over time, which is comparable at any point in time to performing a full partitioning from scratch using a state-the-art static graph partitioning algorithm such as METIS. Furthermore, when vertex replication is turned on, edge-cut can improve by an order of magnitude.},
	author = {Huang, Jiewen and Abadi, Daniel J.},
	doi = {10.14778/2904483.2904486},
	file = {Huang and Abadi - 2016 - Leopard lightweight edge-oriented partitioning an.pdf:/Users/pandey/Zotero/storage/PPVVN5XM/Huang and Abadi - 2016 - Leopard lightweight edge-oriented partitioning an.pdf:application/pdf},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	language = {en},
	month = mar,
	number = {7},
	pages = {540--551},
	shorttitle = {Leopard},
	title = {Leopard: lightweight edge-oriented partitioning and replication for dynamic graphs},
	url = {https://dl.acm.org/doi/10.14778/2904483.2904486},
	urldate = {2023-06-26},
	volume = {9},
	year = {2016},
	bdsk-url-1 = {https://dl.acm.org/doi/10.14778/2904483.2904486},
	bdsk-url-2 = {https://doi.org/10.14778/2904483.2904486}}

@inproceedings{perrin_causal_2016,
	abstract = {In distributed systems where strong consistency is costly when not impossible, causal consistency provides a valuable abstraction to represent program executions as partial orders. In addition to the sequential program order of each computing entity, causal order also contains the semantic links between the events that affect the shared objects --messages emission and reception in a communication channel, reads and writes on a shared register. Usual approaches based on semantic links are very difficult to adapt to other data types such as queues or counters because they require a specific analysis of causal dependencies for each data type. This paper presents a new approach to define causal consistency for any abstract data type based on sequential specifications. It explores, formalizes and studies the differences between three variations of causal consistency and highlights them in the light of PRAM, eventual consistency and sequential consistency: weak causal consistency, that captures the notion of causality preservation when focusing on convergence; causal convergence that mixes weak causal consistency and convergence; and causal consistency, that coincides with causal memory when applied to shared memory.},
	address = {Barcelona Spain},
	author = {Perrin, Matthieu and Mostefaoui, Achour and Jard, Claude},
	booktitle = {Proceedings of the 21st {ACM} {SIGPLAN} {Symposium} on {Principles} and {Practice} of {Parallel} {Programming}},
	doi = {10.1145/2851141.2851170},
	file = {Perrin et al. - 2016 - Causal consistency beyond memory.pdf:/Users/pandey/Zotero/storage/4LALLF5J/Perrin et al. - 2016 - Causal consistency beyond memory.pdf:application/pdf},
	isbn = {978-1-4503-4092-2},
	language = {en},
	month = feb,
	pages = {1--12},
	publisher = {ACM},
	shorttitle = {Causal consistency},
	title = {Causal consistency: beyond memory},
	url = {https://dl.acm.org/doi/10.1145/2851141.2851170},
	urldate = {2023-06-26},
	year = {2016},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/2851141.2851170},
	bdsk-url-2 = {https://doi.org/10.1145/2851141.2851170}}

@article{waudby_high_nodate,
	author = {Waudby, Jack},
	file = {Waudby - High Performance Concurrency Control and Commit Pr.pdf:/Users/pandey/Zotero/storage/CS57EJGQ/Waudby - High Performance Concurrency Control and Commit Pr.pdf:application/pdf},
	language = {en},
	title = {High {Performance} {Concurrency} {Control} and {Commit} {Protocols} in {OLTP} {Databases}}}

@inproceedings{waudby_towards_2021-1,
	abstract = {Verifying ACID compliance is an essential part of database benchmarking, because the integrity of performance results can be undermined as the performance benefits of operating with weaker safety guarantees (at the potential cost of correctness) are well known. Traditionally, benchmarks have specified a number of tests to validate ACID compliance. However, these tests have been formulated in the context of relational database systems and SQL, whereas our scope of benchmarking are systems for graph data, many of which are non-relational. This paper presents a set of data model-agnostic ACID compliance tests for the LDBC (Linked Data Benchmark Council) Social Network Benchmark suite's Interactive (SNB-I) workload, a transaction processing benchmark for graph databases. We test all ACID properties with a particular emphasis on isolation, covering 10 transaction anomalies in total. We present results from implementing the test suite on 5 database systems.},
	address = {Cham},
	author = {Waudby, Jack and Steer, Benjamin A. and Karimov, Karim and Marton, J{\'o}zsef and Boncz, Peter and Sz{\'a}rnyas, G{\'a}bor},
	booktitle = {Performance {Evaluation} and {Benchmarking}},
	doi = {10.1007/978-3-030-84924-5_1},
	editor = {Nambiar, Raghunath and Poess, Meikel},
	isbn = {978-3-030-84924-5},
	language = {en},
	pages = {1--17},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Towards {Testing} {ACID} {Compliance} in the {LDBC} {Social} {Network} {Benchmark}},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-84924-5_1}}

@inproceedings{durner_no_2019,
	abstract = {Concurrency control is one of the most performance critical steps in modern many-core database systems. Achieving higher throughput on multi-socket servers is difficult and many concurrency control algorithms reduce the amount of accepted schedules in favor of transaction throughput or relax the isolation level which introduces unwanted anomalies. Both approaches lead to unexpected transaction behavior that is difficult to understand by the database users. We introduce a novel multi-version concurrency protocol that achieves high performance while reducing the number of aborted schedules to a minimum and providing the best isolation level. Our approach leverages the idea of a graph-based scheduler that uses the concept of conflict graphs. As conflict serializable histories can be represented by acyclic conflict graphs, our scheduler maintains the conflict graph and allows all transactions that keep the graph acyclic. All conflict serializable schedules can be accepted by such a graph-based algorithm due to the conflict graph theorem. Hence, only transaction schedules that truly violate the serializability constraints need to abort. Our developed approach is able to accept the useful intersection of commit order preserving conflict serializable (COCSR) and recoverable (RC) schedules which are the two most desirable classes in terms of correctness and user experience. We show experimentally that our graph-based scheduler has very competitive throughput in pure transactional workloads while providing fewer aborts and improved user experience. Our multi-version extension helps to efficiently perform long-running read transactions on the same up-to-date database. Moreover, our graph-based scheduler can outperform the competitors on mixed workloads.},
	author = {Durner, Dominik and Neumann, Thomas},
	booktitle = {2019 {IEEE} 35th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	doi = {10.1109/ICDE.2019.00071},
	file = {IEEE Xplore Abstract Record:/Users/pandey/Zotero/storage/8IP7MP37/8731610.html:text/html;Submitted Version:/Users/pandey/Zotero/storage/YRCI9EQS/Durner and Neumann - 2019 - No False Negatives Accepting All Useful Schedules.pdf:application/pdf},
	keywords = {Protocols, Servers, Concurrency control, concurrency control, Database systems, History, Modern Hardware and In Memory Database Systems, Schedules, Throughput, transaction processing},
	month = apr,
	note = {ISSN: 2375-026X},
	pages = {734--745},
	shorttitle = {No {False} {Negatives}},
	title = {No {False} {Negatives}: {Accepting} {All} {Useful} {Schedules} in a {Fast} {Serializable} {Many}-{Core} {System}},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/ICDE.2019.00071}}

@incollection{nambiar_pick_2023,
	abstract = {Concurrency control is an integral component in achieving high performance in many-core databases. Implementing serializable transaction processing efficiently is challenging. One approach, serialization graph testing (SGT) faithfully implements the conflict graph theorem by aborting only those transactions that would actually violate serializability (introduce a cycle), thus maintaining the required acyclic invariant. Alternative approaches, such as two-phase locking, disallow certain valid schedules to increase throughput, whereas SGT has the theoretically optimal property of accepting all and only conflict serializable schedules. Historically, SGT was deemed unviable in practice due to the high computational costs of maintaining an acyclic graph. Research has however overturned this historical view by utilising the increased computational power available due to modern hardware. Furthermore, a survey of 24 databases suggests that not all transactions demand conflict serializability but different transactions can perfectly settle for different, weaker isolation levels which typically require relatively lower overheads. Thus, in such a mixed environment, providing only the isolation level required of each transaction should, in theory, increase throughput and reduce aborts. The aim of this paper is to extend SGT for mixed environments subject to Adya's mixing-correct theorem and demonstrate the resulting performance improvement. We augment the YCSB benchmark to generate transactions with different isolation requirements. For certain workloads, mixed serialization graph testing can achieve up to a 28\% increase in throughput and a 19\% decrease in aborts over SGT.},
	address = {Cham},
	author = {Waudby, Jack and Ezhilchelvan, Paul and Webber, Jim},
	booktitle = {Performance {Evaluation} and {Benchmarking}},
	doi = {10.1007/978-3-031-29576-8_1},
	editor = {Nambiar, Raghunath and Poess, Meikel},
	file = {Waudby et al. - 2023 - Pick & Mix Isolation Levels Mixed Serialization G.pdf:/Users/pandey/Zotero/storage/ZB25V849/Waudby et al. - 2023 - Pick & Mix Isolation Levels Mixed Serialization G.pdf:application/pdf},
	isbn = {978-3-031-29575-1 978-3-031-29576-8},
	language = {en},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {1--16},
	publisher = {Springer Nature Switzerland},
	shorttitle = {Pick \& {Mix} {Isolation} {Levels}},
	title = {Pick \& {Mix} {Isolation} {Levels}: {Mixed} {Serialization} {Graph} {Testing}},
	url = {https://link.springer.com/10.1007/978-3-031-29576-8_1},
	urldate = {2023-06-27},
	volume = {13860},
	year = {2023},
	bdsk-url-1 = {https://link.springer.com/10.1007/978-3-031-29576-8_1},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-031-29576-8_1}}

@misc{noauthor_gernots_nodate,
	file = {Gernot's List of Systems Benchmarking Crimes:/Users/pandey/Zotero/storage/F4V3V9NF/benchmarking-crimes.html:text/html},
	title = {Gernot's {List} of {Systems} {Benchmarking} {Crimes}},
	url = {https://gernot-heiser.org/benchmarking-crimes.html},
	urldate = {2023-07-05},
	bdsk-url-1 = {https://gernot-heiser.org/benchmarking-crimes.html}}

@inproceedings{lepers_kvell_2019,
	abstract = {Modern block-addressable NVMe SSDs provide much higher bandwidth and similar performance for random and sequential access. Persistent key-value stores (KVs) designed for earlier storage devices, using either Log-Structured Merge (LSM) or B trees, do not take full advantage of these new devices. Logic to avoid random accesses, expensive operations for keeping data sorted on disk, and synchronization bottlenecks make these KVs CPU-bound on NVMe SSDs. We present a new persistent KV design. Unlike earlier designs, no attempt is made at sequential access, and data is not sorted when stored on disk. A shared-nothing philosophy is adopted to avoid synchronization overhead. Together with batching of device accesses, these design decisions make for read and write performance close to device bandwidth. Finally, maintaining an inexpensive partial sort in memory produces adequate scan performance. We implement this design in KVell, the first persistent KV able to utilize modern NVMe SSDs at maximum bandwidth. We compare KVell against available state-of-the-art LSM and B tree KVs, both with synthetic benchmarks and production workloads. KVell achieves throughput at least 2x that of its closest competitor on read-dominated workloads, and 5x on write-dominated workloads. For workloads that contain mostly scans, KVell performs comparably or better than its competitors. KVell provides maximum latencies an order of magnitude lower than the best of its competitors, even on scan-based workloads.},
	address = {New York, NY, USA},
	author = {Lepers, Baptiste and Balmau, Oana and Gupta, Karan and Zwaenepoel, Willy},
	booktitle = {Proceedings of the 27th {ACM} {Symposium} on {Operating} {Systems} {Principles}},
	doi = {10.1145/3341301.3359628},
	isbn = {978-1-4503-6873-5},
	keywords = {B+ tree, key-value store, log-structured merge tree (LSM), NVMe, performance, persistence, SSD},
	month = oct,
	pages = {447--461},
	publisher = {Association for Computing Machinery},
	series = {{SOSP} '19},
	shorttitle = {{KVell}},
	title = {{KVell}: the design and implementation of a fast persistent key-value store},
	url = {https://doi.org/10.1145/3341301.3359628},
	urldate = {2023-07-05},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3341301.3359628}}

@incollection{hilger_graph_2022,
	abstract = {This chapter identifies graph databases as the next step in knowledge management technology alongside knowledge graphs, as they drive many modern AI capabilities. It describes the graph database space as a new, up-and-coming technology space with many product vendors, each of which has specific features and functions that are discussed in detail, such as standards compliance, scalability and availability, integration, and administration and security. Finally, the chapter addresses business problems that organizations may face as they build systems to mine and manage information.},
	address = {Cham},
	author = {Hilger, Joseph and Wahl, Zachary},
	booktitle = {Making {Knowledge} {Management} {Clickable} : {Knowledge} {Management} {Systems} {Strategy}, {Design}, and {Implementation}},
	doi = {10.1007/978-3-030-92385-3_13},
	editor = {Hilger, Joseph and Wahl, Zachary},
	isbn = {978-3-030-92385-3},
	language = {en},
	pages = {199--208},
	publisher = {Springer International Publishing},
	title = {Graph {Databases}},
	url = {https://doi.org/10.1007/978-3-030-92385-3_13},
	urldate = {2023-07-05},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-92385-3_13}}

@inproceedings{rajendrakumar_bidirectional_2021,
	abstract = {Erlang is a strict, dynamically typed functional programming language popular for its use in distributed and fault-tolerant applications. The absence of static type checking allows ill-typed programs to cause type errors at run time. The benefits of catching these type errors at compile time are the primary motivation for introducing a static type system for Erlang. The greatest challenge is to find a balance between keeping the type checking sound while retaining the flexibility and philosophy of the Erlang. However, since Erlang allows higher-rank polymorphism, it is unavoidable to require type annotations for some functions to ensure decidability. In this paper, we propose a static type system for Erlang based on bidirectional type checking. In bidirectional type checking, terms are either used to infer a type, or they are checked against a given type. With the bidirectional type checking, we are trying to keep type checking sound without limiting the language's philosophy. In addition, type annotations are only required when inference fails, which only occurs at predictable places, such as usage of higher-ranked polymorphism.},
	address = {New York, NY, USA},
	author = {Rajendrakumar, Nithin Vadukkumchery and Bieniusa, Annette},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN} {International} {Workshop} on {Erlang}},
	doi = {10.1145/3471871.3472966},
	isbn = {978-1-4503-8612-8},
	keywords = {Bidirectional type checking, Erlang, type inference},
	month = aug,
	pages = {54--63},
	publisher = {Association for Computing Machinery},
	series = {Erlang 2021},
	title = {Bidirectional typing for {Erlang}},
	url = {https://doi.org/10.1145/3471871.3472966},
	urldate = {2023-07-06},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3471871.3472966}}

@article{zhao_consistency_2020,
	abstract = {Distributed systems address the increasing demand for fast access to resources and fault tolerance for data. However, due to scalability ...},
	author = {Zhao, Xin and Haller, Philipp},
	doi = {10.22152/programming-journal.org/2021/5/6},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/VXUJVK9W/Zhao and Haller - 2020 - Consistency types for replicated data in a higher-.pdf:application/pdf},
	issn = {2473-7321},
	journal = {The Art, Science, and Engineering of Programming},
	language = {en},
	month = nov,
	number = {2},
	pages = {6:1--6:30},
	title = {Consistency types for replicated data in a higher-order distributed programming language},
	url = {https://programming-journal.org/2021/5/6/},
	urldate = {2023-07-06},
	volume = {5},
	year = {2020},
	bdsk-url-1 = {https://programming-journal.org/2021/5/6/},
	bdsk-url-2 = {https://doi.org/10.22152/programming-journal.org/2021/5/6}}

@article{zhao_replicated_2020,
	author = {Zhao, Xin and Haller, Philipp},
	doi = {10.1016/j.jlamp.2020.100561},
	file = {Zhao and Haller - 2020 - Replicated data types that unify eventual consiste.pdf:/Users/pandey/Zotero/storage/VZDGNMHR/Zhao and Haller - 2020 - Replicated data types that unify eventual consiste.pdf:application/pdf},
	issn = {23522208},
	journal = {Journal of Logical and Algebraic Methods in Programming},
	language = {en},
	month = aug,
	pages = {100561},
	title = {Replicated data types that unify eventual consistency and observable atomic consistency},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352220820300468},
	urldate = {2023-07-06},
	volume = {114},
	year = {2020},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S2352220820300468},
	bdsk-url-2 = {https://doi.org/10.1016/j.jlamp.2020.100561}}

@inproceedings{paton_formal_1995,
	abstract = {This paper reviews research on the formal specification of active behaviour, indicating both what has been done in this area, and how. The scope of different approaches is compared within a common framework, which reveals that although many aspects of active behaviour have been described formally, no single proposal covers all phenomena associated with active database systems.},
	address = {Berlin, Heidelberg},
	author = {Paton, Norman W. and Campin, Jack and Fernandes, Alvaro A. A. and Williams, M. Howard},
	booktitle = {Rules in {Database} {Systems}},
	doi = {10.1007/3-540-60365-4_117},
	editor = {Sellis, Timos},
	isbn = {978-3-540-45137-2},
	keywords = {Event Algebra, Execution Model, Operational Semantic, Relational Algebra, Rule Base},
	language = {en},
	pages = {19--35},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	shorttitle = {Formal specification of active database functionality},
	title = {Formal specification of active database functionality: {A} survey},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1007/3-540-60365-4_117}}

@article{abiteboul_transaction-based_1989,
	abstract = {An operational approach to database specification is proposed and investigated. Valid database states are described as the states resulting from the application of admissible transactions, specified by a transactional schema. The approach is similar in spirit to the modeling of behavior by methods and encapsulation in object-oriented systems. The transactions considered are line programs consisting of insertions, deletions, and modifications, using simple selection conditions. The results concern basic properties of transactional schemas, as well as the connection with traditional constraint schemas. In particular, the expressive power of transactional schemas is characterized. Although it is shown that transaction-based specification and constraint-based specification are incomparable, constraints of practical interest that have corresponding transactional schemas are identified. The preservation of constraints by transactions is also studied.},
	author = {Abiteboul, Serge and Vianu, Victor},
	doi = {10.1145/76359.76363},
	file = {Abiteboul and Vianu - 1989 - A transaction-based approach to relational databas.pdf:/Users/pandey/Zotero/storage/8AE4DV8B/Abiteboul and Vianu - 1989 - A transaction-based approach to relational databas.pdf:application/pdf},
	issn = {0004-5411, 1557-735X},
	journal = {Journal of the ACM},
	language = {en},
	month = oct,
	number = {4},
	pages = {758--789},
	title = {A transaction-based approach to relational database specification},
	url = {https://dl.acm.org/doi/10.1145/76359.76363},
	urldate = {2023-07-10},
	volume = {36},
	year = {1989},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/76359.76363},
	bdsk-url-2 = {https://doi.org/10.1145/76359.76363}}

@inproceedings{bidoit_design_1990,
	abstract = {Among the design issues for symbolic computation systems, a non trivial one is the design of the database where informations (specifications, proofs, test sets, program modules, etc.) that are necessary to use the various tools available are stored. In this paper we address the design of a database for an algebraic specification environment, and provide an algebraic specification for this database management. One of the issues in this design is to distinguish between ``static'' aspects, i.e. the attribute updating that is necessary to preserve the database consistency when a single action takes place, and the ``dynamic'' aspects, that take into account the management of concurrent accesses to the database. The specification language modularity was crucial in order to correctly specify such notions as ``coherent list of attributes'', ``coherent list of modules'', etc. Since various cases of attribute dependencies were studied (transitive dependence of a given module attributes, and dependence w.r.t. other module attributes), the specification may be easily modified to take into account modifications in the environment architecture (when new tools are added, some new attributes may be necessary). In the same way, this specification could be adapted to specify other symbolic computation systems database management.},
	address = {Berlin, Heidelberg},
	author = {Bidoit, M. and Capy, F. and Choppy, C.},
	booktitle = {Design and {Implementation} of {Symbolic} {Computation} {Systems}},
	doi = {10.1007/3-540-52531-9_141},
	editor = {Miola, Alfonso},
	isbn = {978-3-540-47014-4},
	keywords = {Abstract Data Type, Complex Specification, Database Management, Module Attribute, Specification Language},
	language = {en},
	pages = {205--214},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The design and specification of the {ASSPEGIQUE} database},
	year = {1990},
	bdsk-url-1 = {https://doi.org/10.1007/3-540-52531-9_141}}

@misc{patwary_sdp_2021,
	abstract = {Time-evolving large graph has received attention due to their participation in real-world applications such as social networks and PageRank calculation. It is necessary to partition a large-scale dynamic graph in a streaming manner to overcome the memory bottleneck while partitioning the computational load. Reducing network communication and balancing the load between the partitions are the criteria to achieve effective run-time performance in graph partitioning. Moreover, an optimal resource allocation is needed to utilise the resources while catering the graph streams into the partitions. A number of existing partitioning algorithms (ADP, LogGP and LEOPARD) have been proposed to address the above problem. However, these partitioning methods are incapable of scaling the resources and handling the stream of data in real-time. In this study, we propose a dynamic graph partitioning method called Scalable Dynamic Graph Partitioner (SDP) using the streaming partitioning technique. The SDP contributes a novel vertex assigning method, communication-aware balancing method, and a scaling technique to produce an efficient dynamic graph partitioner. Experiment results show that the proposed method achieves up to 90\% reduction of communication cost and 60\%-70\% balancing the load dynamically, compared with previous algorithms. Moreover, the proposed algorithm significantly reduces the execution time during partitioning.},
	author = {Patwary, Md Anwarul Kaium and Garg, Saurabh and Battula, Sudheer Kumar and Kang, Byeong},
	doi = {10.48550/arXiv.2110.15669},
	file = {arXiv Fulltext PDF:/Users/pandey/Zotero/storage/D5YK3U5V/Patwary et al. - 2021 - SDP Scalable Real-time Dynamic Graph Partitioner.pdf:application/pdf;arXiv.org Snapshot:/Users/pandey/Zotero/storage/53339CE4/2110.html:text/html},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	month = oct,
	note = {arXiv:2110.15669 [cs]},
	publisher = {arXiv},
	shorttitle = {{SDP}},
	title = {{SDP}: {Scalable} {Real}-time {Dynamic} {Graph} {Partitioner}},
	url = {http://arxiv.org/abs/2110.15669},
	urldate = {2023-07-17},
	year = {2021},
	bdsk-url-1 = {http://arxiv.org/abs/2110.15669},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2110.15669}}

@article{huang_leopard_2016-1,
	abstract = {This paper introduces a dynamic graph partitioning algorithm, designed for large, constantly changing graphs. We propose a partitioning framework that adjusts on the fly as the graph structure changes. We also introduce a replication algorithm that is tightly integrated with the partitioning algorithm, which further reduces the number of edges cut by the partitioning algorithm. Even though the proposed approach is handicapped by only taking into consideration local parts of the graph when reassigning vertices, extensive evaluation shows that the proposed approach maintains a quality partitioning over time, which is comparable at any point in time to performing a full partitioning from scratch using a state-the-art static graph partitioning algorithm such as METIS. Furthermore, when vertex replication is turned on, edge-cut can improve by an order of magnitude.},
	author = {Huang, Jiewen and Abadi, Daniel J.},
	doi = {10.14778/2904483.2904486},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/ULBA5TW8/Huang and Abadi - 2016 - Leopard lightweight edge-oriented partitioning an.pdf:application/pdf},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	month = mar,
	number = {7},
	pages = {540--551},
	shorttitle = {Leopard},
	title = {Leopard: lightweight edge-oriented partitioning and replication for dynamic graphs},
	url = {https://dl.acm.org/doi/10.14778/2904483.2904486},
	urldate = {2023-07-17},
	volume = {9},
	year = {2016},
	bdsk-url-1 = {https://dl.acm.org/doi/10.14778/2904483.2904486},
	bdsk-url-2 = {https://doi.org/10.14778/2904483.2904486}}

@inproceedings{ben_ammar_graph_2016,
	abstract = {Today, graph databases (GDB) represent a requirement for many applications that manage graph-like data, such as social networks. They are able to manage highly interconnected data such as analyzing whole-graph and answering user queries, which are more interested in the relationships between data rather than on the nodes of the graph. Partitioning data over several systems is one of the most techniques used to optimize queries. It allows distributing data over several servers when it is infeasible to query and store them on a single site. The aim is to improve query response time and to minimize the data storage cost. Although, it has been extensively used in traditional databases, data partitioning has many specificities when it is applied in GDB, which are characterized by a dynamic structure and highly interconnected data. In this paper, we study the recent approaches of GDB partitioning. We summarize and then discuss what partitioning options these approaches have used, what partitioning criteria they have optimized and what are the main steps for partitioning GDB.},
	author = {Ben Ammar, Ali},
	booktitle = {2016 7th {International} {Conference} on {Information}, {Intelligence}, {Systems} \& {Applications} ({IISA})},
	doi = {10.1109/IISA.2016.7785355},
	file = {IEEE Xplore Abstract Record:/Users/pandey/Zotero/storage/42QAIDHB/7785355.html:text/html},
	keywords = {Servers, Time factors, Query processing, Database languages, Graph Database partitioning, Graph theory, partitioning optimization, partitioning processing, Query optimization, Social network services, Streaming graph partitioning, Workload balancing},
	month = jul,
	pages = {1--9},
	shorttitle = {Graph database partitioning},
	title = {Graph database partitioning: {A} study},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/IISA.2016.7785355}}

@inproceedings{mccrabb_dredge_2019,
	abstract = {Graph-based algorithms have gained significant interest in several application domains. Solutions addressing the computational efficiency of such algorithms have mostly relied on many-core architectures. Cleverly laying out input graphs in storage, by placing adjacent vertices in a same storage unit (memory bank or cache unit), enables fast access during graph traversal. Dynamic graphs, however, must be continuously repartitioned to leverage this benefit. Yet software repartitioning solutions rely on costly, cross-vault communication to query and optimize the graph layout between algorithm iterations. In this work, we propose DREDGE, a novel hardware solution to provide heuristic repartitioning optimizations in the background without extra communication. Our evaluation indicates that we achieve a 1.9x speedup, on average, over several graph algorithms and datasets, executing on a 24x24-core architecture, when compared against a baseline solution that does not repartition the dynamic graph. We estimated that DREDGE incurs only 1.5\% area and 2.1\% power overheads over an ARM A5 processor core.},
	address = {New York, NY, USA},
	author = {McCrabb, Andrew and Winsor, Eric and Bertacco, Valeria},
	booktitle = {Proceedings of the 56th {Annual} {Design} {Automation} {Conference} 2019},
	doi = {10.1145/3316781.3317804},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/9K6XQNN4/McCrabb et al. - 2019 - DREDGE Dynamic Repartitioning during Dynamic Grap.pdf:application/pdf},
	isbn = {978-1-4503-6725-7},
	month = jun,
	pages = {1--6},
	publisher = {Association for Computing Machinery},
	series = {{DAC} '19},
	shorttitle = {{DREDGE}},
	title = {{DREDGE}: {Dynamic} {Repartitioning} during {Dynamic} {Graph} {Execution}},
	url = {https://dl.acm.org/doi/10.1145/3316781.3317804},
	urldate = {2023-07-17},
	year = {2019},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3316781.3317804},
	bdsk-url-2 = {https://doi.org/10.1145/3316781.3317804}}

@inproceedings{davoudian_helios_2019,
	abstract = {In this paper, we present a novel adaptive and workload-driven partitioning framework, named Helios, aiming to achieve low-latency and high-throughput online queries in distributed graph stores. As each workload typically contains popular or similar queries, our partitioning method uses the existing workload to capture active vertices and edges which are frequently visited and traversed respectively. This information is used to heuristically improve the quality of partitions either by avoiding the concentration of active vertices in a few partitions proportional to their visit frequencies or by reducing the probability of the cut of active edges proportional to their traversal frequencies. In order to assess the impact of Helios on a graph store, and to show how easily the approach can be plugged on top of the system, we exploit it in a distributed, graph-based RDF store. The query engine of the store exploits Helios to reduce or eliminate data communication for future queries and balance the load among nodes. We evaluate the store by using realistic query workloads over an RDF dataset. Our results demonstrate the ability of Helios to handle varying query workloads with minimum overhead while maintaining the quality of partitions over time, along with being scalable by increasing either the data size or the number of computing nodes.},
	address = {New York, NY, USA},
	author = {Davoudian, Ali},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Management} of {Data}},
	doi = {10.1145/3299869.3300103},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/W35KS2F4/Davoudian - 2019 - Helios An Adaptive and Query Workload-driven Part.pdf:application/pdf},
	isbn = {978-1-4503-5643-5},
	keywords = {adaptive, graph stores, workload--driven partitioning},
	month = jun,
	pages = {1820--1822},
	publisher = {Association for Computing Machinery},
	series = {{SIGMOD} '19},
	shorttitle = {Helios},
	title = {Helios: {An} {Adaptive} and {Query} {Workload}-driven {Partitioning} {Framework} for {Distributed} {Graph} {Stores}},
	url = {https://dl.acm.org/doi/10.1145/3299869.3300103},
	urldate = {2023-07-17},
	year = {2019},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3299869.3300103},
	bdsk-url-2 = {https://doi.org/10.1145/3299869.3300103}}

@misc{hanai_distributed_2019,
	abstract = {We propose Distributed Neighbor Expansion (Distributed NE), a parallel and distributed graph partitioning method that can scale to trillion-edge graphs while providing high partitioning quality. Distributed NE is based on a new heuristic, called parallel expansion, where each partition is constructed in parallel by greedily expanding its edge set from a single vertex in such a way that the increase of the vertex cuts becomes local minimal. We theoretically prove that the proposed method has the upper bound in the partitioning quality. The empirical evaluation with various graphs shows that the proposed method produces higher-quality partitions than the state-of-the-art distributed graph partitioning algorithms. The performance evaluation shows that the space efficiency of the proposed method is an order-of-magnitude better than the existing algorithms, keeping its time efficiency comparable. As a result, Distributed NE can partition a trillion-edge graph using only 256 machines within 70 minutes.},
	author = {Hanai, Masatoshi and Suzumura, Toyotaro and Tan, Wen Jun and Liu, Elvis and Theodoropoulos, Georgios and Cai, Wentong},
	doi = {10.48550/arXiv.1908.05855},
	file = {arXiv Fulltext PDF:/Users/pandey/Zotero/storage/JT67XEUW/Hanai et al. - 2019 - Distributed Edge Partitioning for Trillion-edge Gr.pdf:application/pdf;arXiv.org Snapshot:/Users/pandey/Zotero/storage/5X8PD6HV/1908.html:text/html},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Data Structures and Algorithms},
	month = sep,
	note = {arXiv:1908.05855 [cs]},
	publisher = {arXiv},
	title = {Distributed {Edge} {Partitioning} for {Trillion}-edge {Graphs}},
	url = {http://arxiv.org/abs/1908.05855},
	urldate = {2023-07-17},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1908.05855},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.1908.05855}}

@inproceedings{ferme_framework_2015,
	abstract = {The diverse landscape of Workflow Management Systems (WfMSs) makes it challenging for users to compare different solutions to identify the ones most suitable to their requirements. Thus a comparison framework that would define common grounds in many different aspects, such as price, reliability, security, robustness and performance is necessary. In this paper we focus on the performance aspect, and we present a framework for automatic and reliable calculation of performance metrics for BPMN 2.0 WfMSs. We validate the framework by applying it on two open-source WfMSs. The goal is to contribute to the improvement of existing WfMSs by pinpointing performance bottlenecks, and to empower end users to make informed decisions when selecting a WfMS.},
	address = {Cham},
	author = {Ferme, Vincenzo and Ivanchikj, Ana and Pautasso, Cesare},
	booktitle = {Business {Process} {Management}},
	doi = {10.1007/978-3-319-23063-4_18},
	editor = {Motahari-Nezhad, Hamid Reza and Recker, Jan and Weidlich, Matthias},
	isbn = {978-3-319-23063-4},
	keywords = {Benchmarking, BPMN 2.0, Workflow management systems},
	language = {en},
	pages = {251--259},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Framework} for {Benchmarking} {BPMN} 2.0 {Workflow} {Management} {Systems}},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-319-23063-4_18}}

@article{nicoara_distneo4j_nodate,
	author = {Nicoara, Daniel},
	file = {Nicoara - DistNeo4j Scaling Graph Databases through Dynamic.pdf:/Users/pandey/Zotero/storage/LGKVNV9W/Nicoara - DistNeo4j Scaling Graph Databases through Dynamic.pdf:application/pdf},
	language = {en},
	title = {{DistNeo4j}: {Scaling} {Graph} {Databases} through {Dynamic} {Distributed} {Partitioning}}}

@article{nicoara_hermes_nodate,
	abstract = {Social networks are large graphs that require multiple graph database servers to store and manage them. Each database server hosts a graph partition with the objectives of balancing server loads, reducing remote traversals (edge-cuts), and adapting the partitioning to changes in the structure of the graph in the face of changing workloads. To achieve these objectives, a dynamic repartitioning algorithm is required to modify an existing partitioning to maintain good quality partitions while not imposing a significant overhead to the system. In this paper, we introduce a lightweight repartitioner, which dynamically modifies a partitioning using a small amount of resources. In contrast to the existing repartitioning algorithms, our lightweight repartitioner is efficient, making it suitable for use in a real system. We integrated our lightweight repartitioner into Hermes, which we designed as an extension of the open source Neo4j graph database system, to support workloads over partitioned graph data distributed over multiple servers. Using real-world social network data, we show that Hermes leverages the lightweight repartitioner to maintain high quality partitions and provides a 2 to 3 times performance improvement over the de-facto standard random hash-based partitioning.},
	author = {Nicoara, Daniel and Kamali, Shahin and Daudjee, Khuzaima and Chen, Lei},
	file = {Nicoara et al. - Hermes Dynamic Partitioning for Distributed Socia.pdf:/Users/pandey/Zotero/storage/8VBJFJYL/Nicoara et al. - Hermes Dynamic Partitioning for Distributed Socia.pdf:application/pdf},
	language = {en},
	title = {Hermes: {Dynamic} {Partitioning} for {Distributed} {Social} {Network} {Graph} {Databases}}}

@inproceedings{bok_dynamic_2018,
	abstract = {In this paper, we propose a dynamic partitioning scheme of RDF graph to support load balancing in the dynamic environment. It generates clusters by grouping RDF subgraphs with high use frequencies while generating subclusters with RDF subgraphs with lower use frequencies. These clusters and subclusters perform load balancing based on the mean frequency of queries for the distributed server. In addition, the number of edge-cuts connected to clusters and subclusters is minimized to minimize the cost of communication between servers.},
	address = {Singapore},
	author = {Bok, Kyoungsoo and Kim, Cheonjung and Jeong, Jaeyun and Lim, Jongtae and Yoo, Jaesoo},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Emerging} {Databases}},
	doi = {10.1007/978-981-10-6520-0_5},
	editor = {Lee, Wookey and Choi, Wonik and Jung, Sungwon and Song, Min},
	isbn = {978-981-10-6520-0},
	keywords = {Cluster, Dynamic partition, Load balancing, RDF},
	language = {en},
	pages = {43--49},
	publisher = {Springer},
	series = {Lecture {Notes} in {Electrical} {Engineering}},
	title = {Dynamic {Partitioning} of {Large} {Scale} {RDF} {Graph} in {Dynamic} {Environments}},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1007/978-981-10-6520-0_5}}

@article{demirci_cascade-aware_2019,
	abstract = {Graph partitioning is an essential task for scalable data management and analysis. The current partitioning methods utilize the structure of the graph, and the query log if available. Some queries performed on the database may trigger further operations. For example, the query workload of a social network application may contain re-sharing operations in the form of cascades. It is beneficial to include the potential cascades in the graph partitioning objectives. In this paper, we introduce the problem of cascade-aware graph partitioning that aims to minimize the overall cost of communication among parts/servers during cascade processes. We develop a randomized solution that estimates the underlying cascades, and use it as an input for partitioning of large-scale graphs. Experiments on 17 real social networks demonstrate the effectiveness of the proposed solution in terms of the partitioning objectives.},
	author = {Demirci, Gunduz Vehbi and Ferhatosmanoglu, Hakan and Aykanat, Cevdet},
	doi = {10.1007/s00778-018-0531-8},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/YWWXDJ76/Demirci et al. - 2019 - Cascade-aware partitioning of large graph database.pdf:application/pdf},
	issn = {0949-877X},
	journal = {The VLDB Journal},
	keywords = {Scalability, Graph partitioning, Information cascade, Propagation models, Randomized algorithms, Social networks},
	language = {en},
	month = jun,
	number = {3},
	pages = {329--350},
	title = {Cascade-aware partitioning of large graph databases},
	url = {https://doi.org/10.1007/s00778-018-0531-8},
	urldate = {2023-08-21},
	volume = {28},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1007/s00778-018-0531-8}}

@inproceedings{davoudian_helios_2019-1,
	abstract = {In this paper, we present a novel adaptive and workload-driven partitioning framework, named Helios, aiming to achieve low-latency and high-throughput online queries in distributed graph stores. As each workload typically contains popular or similar queries, our partitioning method uses the existing workload to capture active vertices and edges which are frequently visited and traversed respectively. This information is used to heuristically improve the quality of partitions either by avoiding the concentration of active vertices in a few partitions proportional to their visit frequencies or by reducing the probability of the cut of active edges proportional to their traversal frequencies. In order to assess the impact of Helios on a graph store, and to show how easily the approach can be plugged on top of the system, we exploit it in a distributed, graph-based RDF store. The query engine of the store exploits Helios to reduce or eliminate data communication for future queries and balance the load among nodes. We evaluate the store by using realistic query workloads over an RDF dataset. Our results demonstrate the ability of Helios to handle varying query workloads with minimum overhead while maintaining the quality of partitions over time, along with being scalable by increasing either the data size or the number of computing nodes.},
	address = {New York, NY, USA},
	author = {Davoudian, Ali},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Management} of {Data}},
	doi = {10.1145/3299869.3300103},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/M62Q7BFJ/Davoudian - 2019 - Helios An Adaptive and Query Workload-driven Part.pdf:application/pdf},
	isbn = {978-1-4503-5643-5},
	keywords = {adaptive, graph stores, workload--driven partitioning},
	month = jun,
	pages = {1820--1822},
	publisher = {Association for Computing Machinery},
	series = {{SIGMOD} '19},
	shorttitle = {Helios},
	title = {Helios: {An} {Adaptive} and {Query} {Workload}-driven {Partitioning} {Framework} for {Distributed} {Graph} {Stores}},
	url = {https://dl.acm.org/doi/10.1145/3299869.3300103},
	urldate = {2023-08-21},
	year = {2019},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3299869.3300103},
	bdsk-url-2 = {https://doi.org/10.1145/3299869.3300103}}

@inproceedings{li_dynamic_2020,
	abstract = {With the increase of large graph data arising in applications like Web, social network, knowledge graph, and so on, there is a growing need for partitioning and repartitioning large graph data in graph data systems. However, the existing graph repartitioning methods are known for poor efficiency in the dynamic environment. In this paper, we devise an efficient lightweight method to identify and move the candidate vertices to achieve graph repartitioning in the dynamic environment. Different from previous approaches that just focus on the case of moving a single vertex as a basic unit, we show that the movement of some closely connected vertices as a group can further improve the quality of graph repartitioning result. We conduct experiments on a large set of real and synthetic graph data sets, and the results showed that the proposed method is more efficient comparing with existing method in several aspects.},
	address = {Cham},
	author = {Li, He and Yuan, Hang and Huang, Jianbin and Cui, Jiangtao and Yoo, Jaesoo},
	booktitle = {Database {Systems} for {Advanced} {Applications}},
	doi = {10.1007/978-3-030-59416-9_29},
	editor = {Nah, Yunmook and Cui, Bin and Lee, Sang-Won and Yu, Jeffrey Xu and Moon, Yang-Sae and Whang, Steven Euijong},
	isbn = {978-3-030-59416-9},
	keywords = {Graph algorithm, Graph repartitioning, Large graph data, Lightweight method},
	language = {en},
	pages = {482--497},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	shorttitle = {Dynamic {Graph} {Repartitioning}},
	title = {Dynamic {Graph} {Repartitioning}: {From} {Single} {Vertex} to {Vertex} {Group}},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-59416-9_29}}

@article{davoudian_workload-adaptive_2021,
	abstract = {Streaming graph partitioning methods have recently gained attention due to their ability to scale to very large graphs with limited resources. However, many such methods do not consider workload and graph characteristics. This may degrade the performance of queries by increasing inter-node communication and computational load imbalance. Moreover, existing workload-aware methods cannot consistently provide good performance as they do not consider dynamic workloads that keep emerging in graph applications. We address these issues by proposing a novel workload-adaptive streaming partitioner named WASP, that aims to achieve low-latency and high-throughput online graph queries. As each workload typically contains frequent query patterns, WASP exploits the existing workload to capture active vertices and edges which are frequently visited and traversed, respectively. This information is used to heuristically improve the quality of partitions either by avoiding the concentration of active vertices in a few partitions proportional to their visit frequencies or by reducing the probability of the cut of active edges proportional to their traversal frequencies. In order to assess the impact of WASP on a graph store and to show how easily the approach can be plugged on top of the system, we exploit it in a distributed graph-based RDF store. Our experiments over three synthetic and real-world graph datasets and the corresponding static and dynamic query workloads show that WASP achieves a better query performance against state-of-the-art graph partitioners, especially in dynamic query workloads.},
	author = {Davoudian, Ali and Chen, Liu and Tu, Hongwei and Liu, Mengchi},
	doi = {10.1007/s41019-021-00156-2},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/8TAS2ZK5/Davoudian et al. - 2021 - A Workload-Adaptive Streaming Partitioner for Dist.pdf:application/pdf},
	issn = {2364-1541},
	journal = {Data Science and Engineering},
	keywords = {Graph partitioning, Dynamic workloads, Streaming, Topology-aware, Workload-adaptive},
	language = {en},
	month = jun,
	number = {2},
	pages = {163--179},
	title = {A {Workload}-{Adaptive} {Streaming} {Partitioner} for {Distributed} {Graph} {Stores}},
	url = {https://doi.org/10.1007/s41019-021-00156-2},
	urldate = {2023-08-21},
	volume = {6},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/s41019-021-00156-2}}

@article{sakouhi_hammer_2021,
	abstract = {The graph partitioning challenge is well known and ongoing classical problem. Many heuristic methods tried to propose solutions focusing mainly on load processing and cost-efficiency. With the emergency of big data technology, the graph partitioning challenge became even more demanding, as an imminent need to handle big volume of data in real time. This reveals a new challenge as most of the existing studies does not consider the volume metric with their streaming graph algorithms causing imbalanced workloads and graph storage. With this article, we propose a specific lightweight algorithm which we called ``Hammer'' Algorithm. Our proposed Hammer algorithm is a streaming graph based on volume metric to ensure optimal load processing and communication cost efficiency. Our proof of concept was done on real world dataset and the Hammer algorithm showed considerable performance against some existing graph partitioning algorithms.},
	author = {Sakouhi, Chayma and Khaldi, Abir and Ghezala, Henda Ben},
	doi = {10.1016/j.jpdc.2021.07.008},
	file = {ScienceDirect Full Text PDF:/Users/pandey/Zotero/storage/W3IXYZL4/Sakouhi et al. - 2021 - Hammer lightweight graph partitioner based on grap.pdf:application/pdf;ScienceDirect Snapshot:/Users/pandey/Zotero/storage/3L326Z33/S0743731521001544.html:text/html},
	issn = {0743-7315},
	journal = {Journal of Parallel and Distributed Computing},
	keywords = {Balance size, Balance volume, Graph partitioning methods, Property graph},
	month = dec,
	pages = {16--28},
	title = {Hammer lightweight graph partitioner based on graph data volumes},
	url = {https://www.sciencedirect.com/science/article/pii/S0743731521001544},
	urldate = {2023-08-21},
	volume = {158},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0743731521001544},
	bdsk-url-2 = {https://doi.org/10.1016/j.jpdc.2021.07.008}}

@misc{sanders_distributed_2023,
	abstract = {We describe the engineering of the distributed-memory multilevel graph partitioner dKaMinPar. It scales to (at least) 8192 cores while achieving partitioning quality comparable to widely used sequential and shared-memory graph partitioners. In comparison, previous distributed graph partitioners scale only in more restricted scenarios and often induce a considerable quality penalty compared to non-distributed partitioners. When partitioning into a large number of blocks, they even produce infeasible solution that violate the balancing constraint. dKaMinPar achieves its robustness by a scalable distributed implementation of the deep-multilevel scheme for graph partitioning. Crucially, this includes new algorithms for balancing during refinement and coarsening.},
	author = {Sanders, Peter and Seemaier, Daniel},
	doi = {10.48550/arXiv.2303.01417},
	file = {arXiv Fulltext PDF:/Users/pandey/Zotero/storage/MNKUBKGA/Sanders and Seemaier - 2023 - Distributed Deep Multilevel Graph Partitioning.pdf:application/pdf;arXiv.org Snapshot:/Users/pandey/Zotero/storage/JUIWSLYS/2303.html:text/html},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Data Structures and Algorithms},
	month = mar,
	note = {arXiv:2303.01417 [cs]},
	publisher = {arXiv},
	title = {Distributed {Deep} {Multilevel} {Graph} {Partitioning}},
	url = {http://arxiv.org/abs/2303.01417},
	urldate = {2023-08-29},
	year = {2023},
	bdsk-url-1 = {http://arxiv.org/abs/2303.01417},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2303.01417}}

@article{fu_geabase_2019,
	abstract = {Graph analytics have been gaining traction rapidly in the past few years. It has a wide array of application areas in the industry, ranging from e-commerce, social networks and recommendation systems to fraud detection and virtually any problem that requires insights into data connections, not just data itself. In this paper, we present GeaBase, a new distributed graph database that provides the capability to store and analyse graph-structured data in real-time at massive scale. We describe the details of the system and the implementation, including novel update architecture, called update centre (UC), and a new language that is suitable for both graph traversal and analytics. We also compare the performance of GeaBase to a widely used open-source graph database Titan. Experiments show that GeaBase is up to 182x faster than Titan in our testing scenarios. We also achieve 22x higher throughput on social network workloads in the comparison.},
	author = {Fu, Zhisong and Wu, Zhengwei and Li, Houyi and Li, Yize and Wu, Min and Chen, Xiaojie and Ye, Xiaomeng and Yu, Benquan and Hu, Xi},
	copyright = {Copyright {\copyright} 2019 Inderscience Enterprises Ltd.},
	journal = {International Journal of High Performance Computing and Networking},
	language = {en},
	month = nov,
	note = {Publisher: Inderscience Publishers (IEL)},
	shorttitle = {{GeaBase}},
	title = {{GeaBase}: a high-performance distributed graph database for industry-scale applications},
	url = {https://www.inderscienceonline.com/doi/10.1504/IJHPCN.2019.103537},
	urldate = {2023-09-01},
	year = {2019},
	bdsk-url-1 = {https://www.inderscienceonline.com/doi/10.1504/IJHPCN.2019.103537}}

@inproceedings{kalmegh_graph_2012,
	abstract = {Graph Database Management Systems, also called graph databases, have recently gained popularity in the database research community due to a need to effectively manage large scale data with inherent graph-like properties. Graph databases are representation, storage and querying systems for naturally occurring graph structures. Graph databases are finding increasing applications in social networks, computational geometry, bioinformatics, drug discovery, semantic web applications and so on. The intent of this paper is to introduce the role of graph database management systems in the context of high-performance computing platforms and present the options for designing high-performance graph databases. We also present a set of potential research directions and list the challenges in combining the research in the two fields of graph databases and high-performance computing.},
	author = {Kalmegh, Prajakta and Navathe, Shamkant B.},
	booktitle = {2012 {SC} {Companion}: {High} {Performance} {Computing}, {Networking} {Storage} and {Analysis}},
	doi = {10.1109/SC.Companion.2012.160},
	file = {IEEE Xplore Abstract Record:/Users/pandey/Zotero/storage/E4QSYCPU/6495942.html:text/html},
	keywords = {Database systems, graph theory, high performance computing, system-level design},
	month = nov,
	pages = {1306--1309},
	title = {Graph {Database} {Design} {Challenges} {Using} {HPC} {Platforms}},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/SC.Companion.2012.160}}

@misc{besta_demystifying_2023,
	abstract = {Graph processing has become an important part of multiple areas of computer science, such as machine learning, computational sciences, medical applications, social network analysis, and many others. Numerous graphs such as web or social networks may contain up to trillions of edges. Often, these graphs are also dynamic (their structure changes over time) and have domain-specific rich data associated with vertices and edges. Graph database systems such as Neo4j enable storing, processing, and analyzing such large, evolving, and rich datasets. Due to the sheer size of such datasets, combined with the irregular nature of graph processing, these systems face unique design challenges. To facilitate the understanding of this emerging domain, we present the first survey and taxonomy of graph database systems. We focus on identifying and analyzing fundamental categories of these systems (e.g., triple stores, tuple stores, native graph database systems, or object-oriented systems), the associated graph models (e.g., RDF or Labeled Property Graph), data organization techniques (e.g., storing graph data in indexing structures or dividing data into records), and different aspects of data distribution and query execution (e.g., support for sharding and ACID). 51 graph database systems are presented and compared, including Neo4j, OrientDB, or Virtuoso. We outline graph database queries and relationships with associated domains (NoSQL stores, graph streaming, and dynamic graph algorithms). Finally, we describe research and engineering challenges to outline the future of graph databases.},
	author = {Besta, Maciej and Gerstenberger, Robert and Peter, Emanuel and Fischer, Marc and Podstawski, Micha{\l} and Barthels, Claude and Alonso, Gustavo and Hoefler, Torsten},
	file = {Besta et al. - 2023 - Demystifying Graph Databases Analysis and Taxonom.pdf:/Users/pandey/Zotero/storage/NELMEXS3/Besta et al. - 2023 - Demystifying Graph Databases Analysis and Taxonom.pdf:application/pdf},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing},
	language = {en},
	month = aug,
	note = {arXiv:1910.09017 [cs]},
	publisher = {arXiv},
	shorttitle = {Demystifying {Graph} {Databases}},
	title = {Demystifying {Graph} {Databases}: {Analysis} and {Taxonomy} of {Data} {Organization}, {System} {Designs}, and {Graph} {Queries}},
	url = {http://arxiv.org/abs/1910.09017},
	urldate = {2023-09-01},
	year = {2023},
	bdsk-url-1 = {http://arxiv.org/abs/1910.09017}}

@misc{besta_graph_2023,
	abstract = {Graph databases (GDBs) are crucial in academic and industry applications. The key challenges in developing GDBs are achieving high performance, scalability, programmability, and portability. To tackle these challenges, we harness established practices from the HPC landscape to build a system that outperforms all past GDBs presented in the literature by orders of magnitude, for both OLTP and OLAP workloads. For this, we first identify and crystallize performance-critical building blocks in the GDB design, and abstract them into a portable and programmable API specification, called the Graph Database Interface (GDI), inspired by the best practices of MPI. We then use GDI to design a GDB for distributedmemory RDMA architectures. Our implementation harnesses onesided RDMA communication and collective operations, and it offers architecture-independent theoretical performance guarantees. The resulting design achieves extreme scales of more than a hundred thousand cores. Our work will facilitate the development of nextgeneration extreme-scale graph databases.},
	author = {Besta, Maciej and Gerstenberger, Robert and Fischer, Marc and Podstawski, Micha{\l} and M{\"u}ller, J{\"u}rgen and Blach, Nils and Egeli, Berke and Mitenkov, George and Chlapek, Wojciech and Michalewicz, Marek and Hoefler, Torsten},
	file = {Besta et al. - 2023 - The Graph Database Interface Scaling Online Trans.pdf:/Users/pandey/Zotero/storage/E76E6UQY/Besta et al. - 2023 - The Graph Database Interface Scaling Online Trans.pdf:application/pdf},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing},
	language = {en},
	month = aug,
	note = {arXiv:2305.11162 [cs]},
	publisher = {arXiv},
	shorttitle = {The {Graph} {Database} {Interface}},
	title = {The {Graph} {Database} {Interface}: {Scaling} {Online} {Transactional} and {Analytical} {Graph} {Workloads} to {Hundreds} of {Thousands} of {Cores}},
	url = {http://arxiv.org/abs/2305.11162},
	urldate = {2023-09-01},
	year = {2023},
	bdsk-url-1 = {http://arxiv.org/abs/2305.11162}}

@article{tian_world_2023-1,
	abstract = {Rapidly growing social networks and other graph data have created a high demand for graph technologies in the market. A plethora of graph databases, systems, and solutions have emerged, as a result. On the other hand, graph has long been a well studied area in the database research community. Despite the numerous surveys on various graph research topics, there is a lack of survey on graph technologies from an industry perspective. The purpose of this paper is to provide the research community with an industrial perspective on the graph database landscape, so that graph researcher can better understand the industry trend and the challenges that the industry is facing, and work on solutions to help address these problems.},
	author = {Tian, Yuanyuan},
	doi = {10.1145/3582302.3582320},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/N5R3TY3Q/Tian - 2023 - The World of Graph Databases from An Industry Pers.pdf:application/pdf},
	issn = {0163-5808},
	journal = {ACM SIGMOD Record},
	month = jan,
	number = {4},
	pages = {60--67},
	title = {The {World} of {Graph} {Databases} from {An} {Industry} {Perspective}},
	url = {https://dl.acm.org/doi/10.1145/3582302.3582320},
	urldate = {2023-09-01},
	volume = {51},
	year = {2023},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3582302.3582320},
	bdsk-url-2 = {https://doi.org/10.1145/3582302.3582320}}

@inproceedings{dai_iogp_2017,
	abstract = {Graphs have become increasingly important in many applications and domains such as querying relationships in social networks or managing rich metadata generated in scientific computing. Many of these use cases require high-performance distributed graph databases for serving continuous updates from clients and, at the same time, answering complex queries regarding the current graph. These operations in graph databases, also referred to as online transaction processing (OLTP) operations, have specific design and implementation requirements for graph partitioning algorithms. In this research, we argue it is necessary to consider the connectivity and the vertex degree changes during graph partitioning. Based on this idea, we designed an Incremental Online Graph Partitioning (IOGP) algorithm that responds accordingly to the incremental changes of vertex degree. IOGP helps achieve better locality, generate balanced partitions, and increase the parallelism for accessing high-degree vertices of the graph. Over both real-world and synthetic graphs, IOGP demonstrates as much as 2x better query performance with a less than 10\% overhead when compared against state-of-the-art graph partitioning algorithms.},
	address = {New York, NY, USA},
	author = {Dai, Dong and Zhang, Wei and Chen, Yong},
	booktitle = {Proceedings of the 26th {International} {Symposium} on {High}-{Performance} {Parallel} and {Distributed} {Computing}},
	doi = {10.1145/3078597.3078606},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/BDK84757/Dai et al. - 2017 - IOGP An Incremental Online Graph Partitioning Alg.pdf:application/pdf},
	isbn = {978-1-4503-4699-3},
	keywords = {graph database, distributed storage, graph partitioning, oltp},
	month = jun,
	pages = {219--230},
	publisher = {Association for Computing Machinery},
	series = {{HPDC} '17},
	shorttitle = {{IOGP}},
	title = {{IOGP}: {An} {Incremental} {Online} {Graph} {Partitioning} {Algorithm} for {Distributed} {Graph} {Databases}},
	url = {https://dl.acm.org/doi/10.1145/3078597.3078606},
	urldate = {2023-09-04},
	year = {2017},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3078597.3078606},
	bdsk-url-2 = {https://doi.org/10.1145/3078597.3078606}}

@misc{wu_nebula_2022,
	abstract = {This paper introduces the recent work of Nebula Graph, an open-source, distributed, scalable, and native graph database. We present a system design trade-off and a comprehensive overview of Nebula Graph internals, including graph data models, partitioning strategies, secondary indexes, optimizer rules, storage-side transactions, graph query languages, observability, graph processing frameworks, and visualization tool-kits. In addition, three sets of large-scale graph b},
	author = {Wu, Min and Yi, Xinglu and Yu, Hui and Liu, Yu and Wang, Yujue},
	doi = {10.48550/arXiv.2206.07278},
	file = {arXiv Fulltext PDF:/Users/pandey/Zotero/storage/PHLECTVL/Wu et al. - 2022 - Nebula Graph An open source distributed graph dat.pdf:application/pdf;arXiv.org Snapshot:/Users/pandey/Zotero/storage/6ZUYK8NW/2206.html:text/html},
	keywords = {Computer Science - Databases},
	month = jun,
	note = {arXiv:2206.07278 [cs]},
	publisher = {arXiv},
	shorttitle = {Nebula {Graph}},
	title = {Nebula {Graph}: {An} open source distributed graph database},
	url = {http://arxiv.org/abs/2206.07278},
	urldate = {2023-09-04},
	year = {2022},
	bdsk-url-1 = {http://arxiv.org/abs/2206.07278},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2206.07278}}

@misc{jonas_cloud_2019,
	abstract = {Serverless cloud computing handles virtually all the system administration operations needed to make it easier for programmers to use the cloud. It provides an interface that greatly simplifies cloud programming, and represents an evolution that parallels the transition from assembly language to high-level programming languages. This paper gives a quick history of cloud computing, including an accounting of the predictions of the 2009 Berkeley View of Cloud Computing paper, explains the motivation for serverless computing, describes applications that stretch the current limits of serverless, and then lists obstacles and research opportunities required for serverless computing to fulfill its full potential. Just as the 2009 paper identified challenges for the cloud and predicted they would be addressed and that cloud use would accelerate, we predict these issues are solvable and that serverless computing will grow to dominate the future of cloud computing.},
	author = {Jonas, Eric and Schleier-Smith, Johann and Sreekanti, Vikram and Tsai, Chia-Che and Khandelwal, Anurag and Pu, Qifan and Shankar, Vaishaal and Carreira, Joao and Krauth, Karl and Yadwadkar, Neeraja and Gonzalez, Joseph E. and Popa, Raluca Ada and Stoica, Ion and Patterson, David A.},
	doi = {10.48550/arXiv.1902.03383},
	file = {arXiv Fulltext PDF:/Users/pandey/Zotero/storage/7RPA8SUS/Jonas et al. - 2019 - Cloud Programming Simplified A Berkeley View on S.pdf:application/pdf;arXiv.org Snapshot:/Users/pandey/Zotero/storage/J66F28YJ/1902.html:text/html},
	keywords = {Computer Science - Operating Systems},
	month = feb,
	note = {arXiv:1902.03383 [cs]},
	publisher = {arXiv},
	shorttitle = {Cloud {Programming} {Simplified}},
	title = {Cloud {Programming} {Simplified}: {A} {Berkeley} {View} on {Serverless} {Computing}},
	url = {http://arxiv.org/abs/1902.03383},
	urldate = {2023-09-04},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1902.03383},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.1902.03383}}

@article{hoang_cusp_nodate,
	abstract = {Graph analytics systems must analyze graphs with billions of vertices and edges which require several terabytes of storage. Distributed-memory clusters are often used for analyzing such large graphs since the main memory of a single machine is usually restricted to a few hundreds of gigabytes. This requires partitioning the graph among the machines in the cluster. Existing graph analytics systems use a built-in partitioner that incorporates a particular partitioning policy, but the best policy is dependent on the algorithm, input graph, and platform. Therefore, built-in partitioners are not sufficiently flexible. Stand-alone graph partitioners are available, but they too implement only a few policies.},
	author = {Hoang, Loc and Dathathri, Roshan and Gill, Gurbinder and Pingali, Keshav},
	file = {Hoang et al. - CuSP A Customizable Streaming Edge Partitioner fo.pdf:/Users/pandey/Zotero/storage/G5CI3N5P/Hoang et al. - CuSP A Customizable Streaming Edge Partitioner fo.pdf:application/pdf},
	language = {en},
	title = {{CuSP}: {A} {Customizable} {Streaming} {Edge} {Partitioner} for {Distributed} {Graph} {Analytics}}}

@article{gill_study_2018,
	abstract = {Distributed-memory clusters are used for in-memory processing of very large graphs with billions of nodes and edges. This requires partitioning the graph among the machines in the cluster. When a graph is partitioned, a node in the graph may be replicated on several machines, and communication is required to keep these replicas synchronized. Good partitioning policies attempt to reduce this synchronization overhead while keeping the computational load balanced across machines. A number of recent studies have looked at ways to control replication of nodes, but these studies are not conclusive because they were performed on small clusters with eight to sixteen machines, did not consider work-efficient data-driven algorithms, or did not optimize communication for the partitioning strategies they studied. This paper presents an experimental study of partitioning strategies for work-efficient graph analytics applications on large KNL and Skylake clusters with up to 256 machines using the Gluon communication runtime which implements partitioning-specific communication optimizations. Evaluation results show that although simple partitioning strategies like Edge-Cuts perform well on a small number of machines, an alternative partitioning strategy called Cartesian Vertex-Cut (CVC) performs better at scale even though paradoxically it has a higher replication factor and performs more communication than Edge-Cut partitioning does. Results from communication micro-benchmarks resolve this paradox by showing that communication overhead depends not only on communication volume but also on the communication pattern among the partitions. These experiments suggest that high-performance graph analytics systems should support multiple partitioning strategies, like Gluon does, as no single graph partitioning strategy is best for all cluster sizes. For such systems, a decision tree for selecting a good partitioning strategy based on characteristics of the computation and the cluster is presented.},
	author = {Gill, Gurbinder and Dathathri, Roshan and Hoang, Loc and Pingali, Keshav},
	doi = {10.14778/3297753.3297754},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/AVA98WBR/Gill et al. - 2018 - A study of partitioning policies for graph analyti.pdf:application/pdf},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	month = dec,
	number = {4},
	pages = {321--334},
	title = {A study of partitioning policies for graph analytics on large-scale distributed platforms},
	url = {https://dl.acm.org/doi/10.14778/3297753.3297754},
	urldate = {2023-09-04},
	volume = {12},
	year = {2018},
	bdsk-url-1 = {https://dl.acm.org/doi/10.14778/3297753.3297754},
	bdsk-url-2 = {https://doi.org/10.14778/3297753.3297754}}

@phdthesis{shapiro_designing_2007,
	abstract = {Commuting operations greatly simplify consistency in distributed systems. This paper focuses on designing for commutativity, a topic neglected previously. We show that the replicas of any data type for which concurrent operations commute converges to a correct value, under some simple and standard assumptions. We also show that such a data type supports transactions with very low cost. We identify a number of approaches and techniques to ensure commutativity. We re-use some existing ideas (non-destructive updates coupled with invariant identification), but propose a much more efficient implementation. Furthermore, we propose a new technique, background consensus. We illustrate these ideas with a shared edit buffer data type.},
	author = {Shapiro, Marc and Pregui{\c c}a, Nuno},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/SNDVSCFX/Shapiro and Pregui{\c c}a - 2007 - Designing a commutative replicated data type.pdf:application/pdf},
	language = {en},
	school = {INRIA},
	title = {Designing a commutative replicated data type},
	type = {report},
	url = {https://inria.hal.science/inria-00177693},
	urldate = {2023-09-11},
	year = {2007},
	bdsk-url-1 = {https://inria.hal.science/inria-00177693}}

@article{choi_dynamic_2021,
	abstract = {As dynamic graph data have been actively used, incremental graph partition schemes have been studied to efficiently store and manage large graphs. In this paper, we propose a vertex-cut based novel incremental graph partitioning scheme that supports load balancing in a distributed environment. The proposed scheme chooses the load of each node that considers its storage utilization and throughput as the partitioning criterion. The proposed scheme defines hot data that means a particular vertex frequently searched among graphs requested by queries. We manage and utilize hot data for graph partitioning. Finally, we perform vertex-cut based dynamic graph partitioning by using a vertex replication index, the load each node, and hot data to distribute the load evenly in a distributed environment. In order to verify the superiority of the proposed partitioning scheme, we compare it with the existing partitioning schemes through a variety of performance evaluations.},
	author = {Choi, Dojin and Han, Jinsu and Lim, Jongtae and Han, Jinsuk and Bok, Kyoungsoo and Yoo, Jaesoo},
	doi = {10.1109/ACCESS.2021.3075457},
	file = {IEEE Xplore Abstract Record:/Users/pandey/Zotero/storage/LI8AI9TS/9415741.html:text/html;IEEE Xplore Full Text PDF:/Users/pandey/Zotero/storage/3EMCZ89W/Choi et al. - 2021 - Dynamic Graph Partitioning Scheme for Supporting L.pdf:application/pdf},
	issn = {2169-3536},
	journal = {IEEE Access},
	note = {Conference Name: IEEE Access},
	pages = {65254--65265},
	title = {Dynamic {Graph} {Partitioning} {Scheme} for {Supporting} {Load} {Balancing} in {Distributed} {Graph} {Environments}},
	url = {https://ieeexplore.ieee.org/document/9415741},
	urldate = {2023-09-24},
	volume = {9},
	year = {2021},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/9415741},
	bdsk-url-2 = {https://doi.org/10.1109/ACCESS.2021.3075457}}

@inproceedings{fan_application_2020,
	abstract = {Graph partitioning is crucial to parallel computations on large graphs. The choice of partitioning strategies has strong impact on not only the performance of graph algorithms, but also the design of the algorithms. For an algorithm of our interest, what partitioning strategy fits it the best and improves its parallel execution? Is it possible to develop graph algorithms with partition transparency, such that the algorithms work under different partitions without changes? This paper aims to answer these questions. We propose an application-driven hybrid partitioning strategy that, given a graph algorithm A, learns a cost model for A as polynomial regression. We develop partitioners that given the learned cost model, refine an edge-cut or vertex-cut partition to a hybrid partition and reduce the parallel cost of A. Moreover, we identify a general condition under which graph-centric algorithms are partition transparent. We show that a number of graph algorithms can be made partition transparent. Using real-life and synthetic graphs, we experimentally verify that our partitioning strategy improves the performance of a variety of graph computations, up to 22.5 times.},
	address = {New York, NY, USA},
	author = {Fan, Wenfei and Jin, Ruochun and Liu, Muyang and Lu, Ping and Luo, Xiaojian and Xu, Ruiqi and Yin, Qiang and Yu, Wenyuan and Zhou, Jingren},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	doi = {10.1145/3318464.3389745},
	isbn = {978-1-4503-6735-6},
	keywords = {graph partition, machine learning, partition transparency},
	month = may,
	pages = {1765--1779},
	publisher = {Association for Computing Machinery},
	series = {{SIGMOD} '20},
	title = {Application {Driven} {Graph} {Partitioning}},
	url = {https://doi.org/10.1145/3318464.3389745},
	urldate = {2023-09-24},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3318464.3389745}}

@article{catalyurek_more_2023,
	abstract = {In recent years, significant advances have been made in the design and evaluation of balanced (hyper)graph partitioning algorithms. We survey trends of the past decade in practical algorithms for balanced (hyper)graph partitioning together with future research directions. Our work serves as an update to a previous survey on the topic [29]. In particular, the survey extends the previous survey by also covering hypergraph partitioning and has an additional focus on parallel algorithms.},
	author = {{\c C}ataly{\"u}rek, {\"U}mit and Devine, Karen and Faraj, Marcelo and Gottesb{\"u}ren, Lars and Heuer, Tobias and Meyerhenke, Henning and Sanders, Peter and Schlag, Sebastian and Schulz, Christian and Seemaier, Daniel and Wagner, Dorothea},
	doi = {10.1145/3571808},
	file = {Submitted Version:/Users/pandey/Zotero/storage/A26586XP/{\c C}ataly{\"u}rek et al. - 2023 - More Recent Advances in (Hyper)Graph Partitioning.pdf:application/pdf},
	issn = {0360-0300},
	journal = {ACM Computing Surveys},
	keywords = {Graph partitioning, hypergraph partitioning, load balancing},
	month = mar,
	number = {12},
	pages = {253:1--253:38},
	title = {More {Recent} {Advances} in ({Hyper}){Graph} {Partitioning}},
	url = {https://doi.org/10.1145/3571808},
	urldate = {2023-09-24},
	volume = {55},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3571808}}

@article{xu_loggp_2014,
	abstract = {With the increasing availability and scale of graph data from Web 2.0, graph partitioning becomes one of efficient preprocessing techniques to balance the computing workload. Since the cost of partitioning the entire graph is strictly prohibitive, there are some recent tentative works towards streaming graph partitioning which can run faster, be easily paralleled, and be incrementally updated. Unfortunately, the experiments show that the running time of each partitioning is still unbalanced due to the variation of workload access pattens during the supersteps. In addition, the one-pass streaming partitioning result is not always satisfactory for the algorithms' local view of the graph. In this paper, we present LogGP, a log-based graph partitioning system that records, analyzes and reuses the historical statistical information to refine the partitioning result. LogGP can be used as a middle-ware and deployed to many state-of-the-art paralleled graph processing systems easily. LogGP utilizes the historical partitioning results to generate a hyper-graph and uses a novel hyper-graph streaming partitioning approach to generate a better initial streaming graph partitioning result. During the execution, the system uses running logs to optimize graph partitioning which prevents performance degradation. Moreover, LogGP can dynamically repartition the massive graphs in accordance with the structural changes. Extensive experiments conducted on a moderate size of computing cluster with real-world graph datasets demonstrate the superiority of our approach against the state-of-the-art solutions.},
	author = {Xu, Ning and Chen, Lei and Cui, Bin},
	doi = {10.14778/2733085.2733097},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/3LH5YZ8Y/Xu et al. - 2014 - LogGP a log-based dynamic graph partitioning meth.pdf:application/pdf},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	month = oct,
	number = {14},
	pages = {1917--1928},
	shorttitle = {{LogGP}},
	title = {{LogGP}: a log-based dynamic graph partitioning method},
	url = {https://dl.acm.org/doi/10.14778/2733085.2733097},
	urldate = {2023-09-25},
	volume = {7},
	year = {2014},
	bdsk-url-1 = {https://dl.acm.org/doi/10.14778/2733085.2733097},
	bdsk-url-2 = {https://doi.org/10.14778/2733085.2733097}}

@inproceedings{kiefer_penalized_2016,
	abstract = {With ubiquitous parallel architectures, the importance of optimally distributed and thereby balanced work is unprecedented. To tackle this challenge, graph partitioning algorithms have been successfully applied in various application areas. However, there is a mismatch between solutions found by classic graph partitioning and the behavior of many real hardware systems. Graph partitioning assumes that individual vertex weights add up{\"\i} {\'z}to partition weights here, referred to as linear graph partitioning. This implies that performance scales linearly with the number of tasks. In reality, performance does usually not scale linearly with the amount of work due to contention on various resources. We address this mismatch with our novel penalized graph partitioning approach in this paper. Furthermore, we experimentally evaluate the applicability and scalability of our method.},
	address = {Berlin, Heidelberg},
	author = {Kiefer, Tim and Habich, Dirk and Lehner, Wolfgang},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Euro}-{Par} 2016: {Parallel} {Processing} - {Volume} 9833},
	doi = {10.1007/978-3-319-43659-3_11},
	file = {Accepted Version:/Users/pandey/Zotero/storage/JXBXLVU4/Kiefer et al. - 2016 - Penalized Graph Partitioning for Static and Dynami.pdf:application/pdf},
	isbn = {978-3-319-43658-6},
	month = aug,
	pages = {146--158},
	publisher = {Springer-Verlag},
	title = {Penalized {Graph} {Partitioning} for {Static} and {Dynamic} {Load} {Balancing}},
	url = {https://doi.org/10.1007/978-3-319-43659-3_11},
	urldate = {2023-09-25},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-319-43659-3_11}}

@article{priyadarshi_awapart_nodate,
	abstract = {Large-scale knowledge graphs are increasingly common in many domains. Their large sizes often exceed the limits of systems storing the graphs in a centralized data store, especially if placed in main memory. To overcome this, large knowledge graphs need to be partitioned into multiple subgraphs and placed in nodes in a distributed system. But querying these fragmented sub-graphs poses new challenges, such as increased communication costs, due to distributed joins involving cut edges. To combat these problems, a good partitioning should reduce the edge cuts while considering a given query workload. However, a partitioned graph needs to be continually re-partitioned to accommodate changes in the query workload and maintain a good average processing time. In this paper, an adaptive partitioning method for large-scale knowledge graphs is introduced, which adapts the partitioning in response to changes in the query workload. Our evaluation demonstrates that the performance of processing time for queries is improved after dynamically adapting the partitioning of knowledge graph triples.},
	author = {Priyadarshi, Amitabh and Kochut, Krzysztof J},
	file = {Priyadarshi and Kochut - AWAPart Adaptive Workload-Aware Partitioning Know.pdf:/Users/pandey/Zotero/storage/DJY67TXD/Priyadarshi and Kochut - AWAPart Adaptive Workload-Aware Partitioning Know.pdf:application/pdf},
	language = {en},
	title = {{AWAPart}: {Adaptive} {Workload}-{Aware} {Partitioning} {Knowledge} {Graphs}}}

@misc{vaquero_xdgp_2013,
	abstract = {Many real-world systems, such as social networks, rely on mining efficiently large graphs, with hundreds of millions of vertices and edges. This volume of information requires partitioning the graph across multiple nodes in a distributed system. This has a deep effect on performance, as traversing edges cut between partitions incurs a significant performance penalty due to the cost of communication. Thus, several systems in the literature have attempted to improve computational performance by enhancing graph partitioning, but they do not support another characteristic of real-world graphs: graphs are inherently dynamic, their topology evolves continuously, and subsequently the optimum partitioning also changes over time. In this work, we present the first system that dynamically repartitions massive graphs to adapt to structural changes. The system optimises graph partitioning to prevent performance degradation without using data replication. The system adopts an iterative vertex migration algorithm that relies on local information only, making complex coordination unnecessary. We show how the improvement in graph partitioning reduces execution time by over 50\%, while adapting the partitioning to a large number of changes to the graph in three real-world scenarios.},
	author = {Vaquero, Luis and Cuadrado, Felix and Logothetis, Dionysios and Martella, Claudio},
	doi = {10.48550/arXiv.1309.1049},
	file = {arXiv Fulltext PDF:/Users/pandey/Zotero/storage/Z4U3HVHI/Vaquero et al. - 2013 - xDGP A Dynamic Graph Processing System with Adapt.pdf:application/pdf;arXiv.org Snapshot:/Users/pandey/Zotero/storage/FX38RE6N/1309.html:text/html},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	month = sep,
	note = {arXiv:1309.1049 [cs]},
	publisher = {arXiv},
	shorttitle = {{xDGP}},
	title = {{xDGP}: {A} {Dynamic} {Graph} {Processing} {System} with {Adaptive} {Partitioning}},
	url = {http://arxiv.org/abs/1309.1049},
	urldate = {2023-09-25},
	year = {2013},
	bdsk-url-1 = {http://arxiv.org/abs/1309.1049},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.1309.1049}}

@article{cheng_which_2019,
	abstract = {Over decades, relational database management systems (RDBMSs) have been the first choice to manage data. Recently, due to the variety properties of big data, graph database management systems (GDBMSs) have emerged as an important complement to RDBMSs. As pointed out in the existing literature, both RDBMSs and GDBMSs are capable of managing graph data and relational data; however, the boundaries of them still remain unclear. For this reason, in this paper, we first extend a unified benchmark for RDBMSs and GDBMSs over the same datasets using the same query workload under the same metrics. We then conduct extensive experiments to evaluate them and make the following findings: (1) RDBMSs outperform GDMBSs by a substantial margin under the workloads which mainly consist of group by, sort, and aggregation operations, and their combinations; (2) GDMBSs show their superiority under the workloads that mainly consist of multi-table join, pattern match, path identification, and their combinations.},
	author = {Cheng, Yijian and Ding, Pengjie and Wang, Tongtong and Lu, Wei and Du, Xiaoyong},
	doi = {10.1007/s41019-019-00110-3},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/2W834KNJ/Cheng et al. - 2019 - Which Category Is Better Benchmarking Relational .pdf:application/pdf},
	issn = {2364-1541},
	journal = {Data Science and Engineering},
	keywords = {Benchmark, Graph database, Relational database},
	language = {en},
	month = dec,
	number = {4},
	pages = {309--322},
	shorttitle = {Which {Category} {Is} {Better}},
	title = {Which {Category} {Is} {Better}: {Benchmarking} {Relational} and {Graph} {Database} {Management} {Systems}},
	url = {https://doi.org/10.1007/s41019-019-00110-3},
	urldate = {2023-09-25},
	volume = {4},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1007/s41019-019-00110-3}}

@misc{noauthor_early_nodate,
	file = {An early look at the LDBC social network benchmark's business intelligence workload | Proceedings of the 1st ACM SIGMOD Joint International Workshop on Graph Data Management Experiences & Systems (GRADES) and Network Data Analytics (NDA):/Users/pandey/Zotero/storage/ZLGBH5PZ/3210259.html:text/html},
	title = {An early look at the {LDBC} social network benchmark's business intelligence workload {\textbar} {Proceedings} of the 1st {ACM} {SIGMOD} {Joint} {International} {Workshop} on {Graph} {Data} {Management} {Experiences} \& {Systems} ({GRADES}) and {Network} {Data} {Analytics} ({NDA})},
	url = {https://dl.acm.org/doi/abs/10.1145/3210259.3210268},
	urldate = {2023-09-25},
	bdsk-url-1 = {https://dl.acm.org/doi/abs/10.1145/3210259.3210268}}

@inproceedings{shapiro_conflict-free_2011,
	abstract = {Replicating data under Eventual Consistency (EC) allows any replica to accept updates without remote synchronisation. This ensures performance and scalability in large-scale distributed systems (e.g., clouds). However, published EC approaches are ad-hoc and error-prone. Under a formal Strong Eventual Consistency (SEC) model, we study sufficient conditions for convergence. A data type that satisfies these conditions is called a Conflict-free Replicated Data Type (CRDT). Replicas of any CRDT are guaranteed to converge in a self-stabilising manner, despite any number of failures. This paper formalises two popular approaches (state- and operation-based) and their relevant sufficient conditions. We study a number of useful CRDTs, such as sets with clean semantics, supporting both add and remove operations, and consider in depth the more complex Graph data type. CRDT types can be composed to develop large-scale distributed applications, and have interesting theoretical properties.},
	address = {Berlin, Heidelberg},
	author = {Shapiro, Marc and Pregui{\c c}a, Nuno and Baquero, Carlos and Zawirski, Marek},
	booktitle = {Stabilization, {Safety}, and {Security} of {Distributed} {Systems}},
	doi = {10.1007/978-3-642-24550-3_29},
	editor = {D{\'e}fago, Xavier and Petit, Franck and Villain, Vincent},
	file = {Submitted Version:/Users/pandey/Zotero/storage/PQYGZLEN/Shapiro et al. - 2011 - Conflict-Free Replicated Data Types.pdf:application/pdf},
	isbn = {978-3-642-24550-3},
	keywords = {Eventual Consistency, Large-Scale Distributed Systems, Replicated Shared Objects},
	language = {en},
	pages = {386--400},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Conflict-{Free} {Replicated} {Data} {Types}},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-642-24550-3_29}}

@inproceedings{kleppmann_local-first_2019-1,
	abstract = {Cloud apps like Google Docs and Trello are popular because they enable real-time collaboration with colleagues, and they make it easy for us to access our work from all of our devices. However, by centralizing data storage on servers, cloud apps also take away ownership and agency from users. If a service shuts down, the software stops functioning, and data created with that software is lost.},
	address = {Athens Greece},
	author = {Kleppmann, Martin and Wiggins, Adam and Van Hardenberg, Peter and McGranaghan, Mark},
	booktitle = {Proceedings of the 2019 {ACM} {SIGPLAN} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software}},
	doi = {10.1145/3359591.3359737},
	file = {Kleppmann et al. - 2019 - Local-first software you own your data, in spite .pdf:/Users/pandey/Zotero/storage/SA738V8K/Kleppmann et al. - 2019 - Local-first software you own your data, in spite .pdf:application/pdf},
	isbn = {978-1-4503-6995-4},
	language = {en},
	month = oct,
	pages = {154--178},
	publisher = {ACM},
	shorttitle = {Local-first software},
	title = {Local-first software: you own your data, in spite of the cloud},
	url = {https://dl.acm.org/doi/10.1145/3359591.3359737},
	urldate = {2023-09-30},
	year = {2019},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3359591.3359737},
	bdsk-url-2 = {https://doi.org/10.1145/3359591.3359737}}

@inproceedings{sun_design_2023,
	abstract = {The main challenge faced by today's graph database systems is sacrificing performance (computation) for scalability (storage). Such systems probably can store a large amount of data across many instances but can't offer adequate graph-computing power to deeply penetrate dynamic graph dataset in real time. A seemingly simple and intuitive graph query like K-hop traversal or finding all shortest paths may lead to deep traversal of large amount of graph data, which tends to cause a typical BSP (Bulky Synchronous Processing) system to exchange heavily amongst its distributed instances, therefore causing significant latencies. This paper proposes three schools of architectural designs for distributed and horizontally scalable graph database while achieving highly performant graph data processing capabilities. The first school, coined HTAP, augments distributed consensus algorithm RAFT paired with vector-based computing acceleration to achieve fast online data ingestion and real-time deep-data traversal in a TP and AP hybrid mode. The second school, named as GRID, leverages human-intelligence for data partitioning, and preserving the HTAP data processing capabilities across all partitioned clusters. The last school incorporates SHARD and advanced GQL optimization techniques to allow data partitioning to be done fully automated yet strive to achieve lower latency via minimum I/O cost data migration model when queries spread across multiple clusters.},
	address = {New York, NY, USA},
	author = {Sun, Ricky and Chen, Jamie},
	booktitle = {Proceedings of the {International} {Workshop} on {Big} {Data} in {Emergent} {Distributed} {Environments}},
	doi = {10.1145/3579142.3594293},
	isbn = {9798400700934},
	keywords = {deep traversal, distributed graph database, GQL, graph analytics, graph data modeling, graph query optimization, grid, HTAP, linear scalability, real-time data processing, shard, XAI},
	month = jun,
	pages = {1--6},
	publisher = {Association for Computing Machinery},
	series = {{BiDEDE} '23},
	title = {Design of {Highly} {Scalable} {Graph} {Database} {Systems} without {Exponential} {Performance} {Degradation}},
	url = {https://doi.org/10.1145/3579142.3594293},
	urldate = {2023-09-30},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3579142.3594293}}

@inproceedings{buragohain_a1_2020-1,
	abstract = {A1 is an in-memory distributed database used by the Bing search engine to support complex queries over structured data. The key enablers for A1 are availability of cheap DRAM and high speed RDMA (Remote Direct Memory Access) networking in commodity hardware. A1 uses FaRM [11,12] as its underlying storage layer and builds the graph abstraction and query engine on top. The combination of in-memory storage and RDMA access requires rethinking how data is allocated, organized and queried in a large distributed system. A single A1 cluster can store tens of billions of vertices and edges and support a throughput of 350+ million of vertex reads per second with end to end query latency in single digit milliseconds. In this paper we describe the A1 data model, RDMA optimized data structures and query execution.},
	address = {New York, NY, USA},
	author = {Buragohain, Chiranjeeb and Risvik, Knut Magne and Brett, Paul and Castro, Miguel and Cho, Wonhee and Cowhig, Joshua and Gloy, Nikolas and Kalyanaraman, Karthik and Khanna, Richendra and Pao, John and Renzelmann, Matthew and Shamis, Alex and Tan, Timothy and Zheng, Shuheng},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	doi = {10.1145/3318464.3386135},
	file = {Submitted Version:/Users/pandey/Zotero/storage/ULDPTYB4/Buragohain et al. - 2020 - A1 A Distributed In-Memory Graph Database.pdf:application/pdf},
	isbn = {978-1-4503-6735-6},
	keywords = {graph database, distributed database, graph query processing, in-memory database, RDMA},
	month = may,
	pages = {329--344},
	publisher = {Association for Computing Machinery},
	series = {{SIGMOD} '20},
	shorttitle = {A1},
	title = {A1: {A} {Distributed} {In}-{Memory} {Graph} {Database}},
	url = {https://doi.org/10.1145/3318464.3386135},
	urldate = {2023-09-30},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3318464.3386135}}

@article{li_bytegraph_2022,
	abstract = {Most products at ByteDance, e.g., TikTok, Douyin, and Toutiao, naturally generate massive amounts of graph data. To efficiently store, query and update massive graph data is challenging for the broad range of products at ByteDance with various performance requirements. We categorize graph workloads at ByteDance into three types: online analytical, transaction, and serving processing, where each workload has its own characteristics. Existing graph databases have different performance bottlenecks in handling these workloads and none can efficiently handle the scale of graphs at ByteDance. We developed ByteGraph to process these graph workloads with high throughput, low latency and high scalability. There are several key designs in ByteGraph that make it efficient for processing our workloads, including edge-trees to store adjacency lists for high parallelism and low memory usage, adaptive optimizations on thread pools and indexes, and geographic replications to achieve fault tolerance and availability. ByteGraph has been in production use for several years and its performance has shown to be robust for processing a wide range of graph workloads at ByteDance.},
	author = {Li, Changji and Chen, Hongzhi and Zhang, Shuai and Hu, Yingqian and Chen, Chao and Zhang, Zhenjie and Li, Meng and Li, Xiangchen and Han, Dongqing and Chen, Xiaohui and Wang, Xudong and Zhu, Huiming and Fu, Xuwei and Wu, Tingwei and Tan, Hongfei and Ding, Hengtian and Liu, Mengjin and Wang, Kangcheng and Ye, Ting and Li, Lei and Li, Xin and Wang, Yu and Zheng, Chenguang and Yang, Hao and Cheng, James},
	doi = {10.14778/3554821.3554824},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	month = aug,
	number = {12},
	pages = {3306--3318},
	shorttitle = {{ByteGraph}},
	title = {{ByteGraph}: a high-performance distributed graph database in {ByteDance}},
	url = {https://doi.org/10.14778/3554821.3554824},
	urldate = {2023-09-30},
	volume = {15},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.14778/3554821.3554824}}

@inproceedings{sun_design_2023-1,
	abstract = {The main challenge faced by today's graph database systems is sacrificing performance (computation) for scalability (storage). Such systems probably can store a large amount of data across many instances but can't offer adequate graph-computing power to deeply penetrate dynamic graph dataset in real time. A seemingly simple and intuitive graph query like K-hop traversal or finding all shortest paths may lead to deep traversal of large amount of graph data, which tends to cause a typical BSP (Bulky Synchronous Processing) system to exchange heavily amongst its distributed instances, therefore causing significant latencies. This paper proposes three schools of architectural designs for distributed and horizontally scalable graph database while achieving highly performant graph data processing capabilities. The first school, coined HTAP, augments distributed consensus algorithm RAFT paired with vector-based computing acceleration to achieve fast online data ingestion and real-time deep-data traversal in a TP and AP hybrid mode. The second school, named as GRID, leverages human-intelligence for data partitioning, and preserving the HTAP data processing capabilities across all partitioned clusters. The last school incorporates SHARD and advanced GQL optimization techniques to allow data partitioning to be done fully automated yet strive to achieve lower latency via minimum I/O cost data migration model when queries spread across multiple clusters.},
	address = {New York, NY, USA},
	author = {Sun, Ricky and Chen, Jamie},
	booktitle = {Proceedings of the {International} {Workshop} on {Big} {Data} in {Emergent} {Distributed} {Environments}},
	doi = {10.1145/3579142.3594293},
	isbn = {9798400700934},
	keywords = {deep traversal, distributed graph database, GQL, graph analytics, graph data modeling, graph query optimization, grid, HTAP, linear scalability, real-time data processing, shard, XAI},
	month = jun,
	pages = {1--6},
	publisher = {Association for Computing Machinery},
	series = {{BiDEDE} '23},
	title = {Design of {Highly} {Scalable} {Graph} {Database} {Systems} without {Exponential} {Performance} {Degradation}},
	url = {https://doi.org/10.1145/3579142.3594293},
	urldate = {2023-09-29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3579142.3594293}}

@inproceedings{li_graphz_2015,
	abstract = {The emerging applications in big data and social networks issue rapidly increasing demands on graph processing. Graph query operations that involve a large number of vertices and edges can be tremendously slow on traditional databases. The state-of-the-art graph processing systems and databases usually adopt master/slave architecture that potentially impairs their scalability. This work describes the design and implementation of a new graph processing system based on Bulk Synchronous Parallel model. Our system is built on top of ZHT, a scalable distributed key-value store, which benefits the graph processing in terms of scalability, performance and persistency. The experiment results imply excellent scalability.},
	address = {Chicago, IL, USA},
	author = {Li, Tonglin and Ma, Chaoqi and Li, Jiabao and Zhou, Xiaobing and Wang, Ke and Zhao, Dongfang and Sadooghi, Iman and Raicu, Ioan},
	booktitle = {2015 {IEEE} {International} {Conference} on {Cluster} {Computing}},
	doi = {10.1109/CLUSTER.2015.90},
	file = {Li et al. - 2015 - GRAPHZ A Key-Value Store Based Scalable Graph Pr.pdf:/Users/pandey/Zotero/storage/ABJPMLJK/Li et al. - 2015 - GRAPHZ A Key-Value Store Based Scalable Graph Pr.pdf:application/pdf},
	isbn = {978-1-4673-6598-7},
	language = {en},
	month = sep,
	pages = {516--517},
	publisher = {IEEE},
	shorttitle = {{GRAPH}/{Z}},
	title = {{GRAPH}/{Z}: {A} {Key}-{Value} {Store} {Based} {Scalable} {Graph} {Processing} {System}},
	url = {http://ieeexplore.ieee.org/document/7307637/},
	urldate = {2023-09-30},
	year = {2015},
	bdsk-url-1 = {http://ieeexplore.ieee.org/document/7307637/},
	bdsk-url-2 = {https://doi.org/10.1109/CLUSTER.2015.90}}

@inproceedings{zhao_key-value_2023,
	abstract = {An increasing number of applications are modeling data as property graphs. In various scenarios, the scale of data can differ significantly, ranging from thousands of nodes/relationships to tens of billions of nodes/relationships. While distributed native graph databases can cater to the management and query requirements of large-scale graph data sets, they tend to be relatively cumbersome for small-scale data sets. This motivates us to develop a lightweight, scalable graph database capable of handling data across different scales. In this paper, we propose a method for constructing a graph database based on key-value storage, outlining the process of mapping graph data to key-value storage and executing graph queries on the key-value storage. We implemented and open-sourced a graph database based on RocksDB, namely KVGDB, which can manage data in an embedded fashion and be easily scaled to distributed environments. Experimental results demonstrate that KVGDB can effectively meet the management and query requirements of graph data sets, even at the scale of billions of nodes/relationships.},
	address = {Cham},
	author = {Zhao, Zihao and Hu, Chuan and Shen, Zhihong and Mao, Along and Ren, Hao},
	booktitle = {Database and {Expert} {Systems} {Applications}},
	doi = {10.1007/978-3-031-39847-6_26},
	editor = {Strauss, Christine and Amagasa, Toshiyuki and Kotsis, Gabriele and Tjoa, A. Min and Khalil, Ismail},
	isbn = {978-3-031-39847-6},
	keywords = {Graph data, Graph Database, Key-Value Database},
	language = {en},
	pages = {338--344},
	publisher = {Springer Nature Switzerland},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Key}-{Value} {Based} {Approach} to {Scalable} {Graph} {Database}},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-031-39847-6_26}}

@article{lee_multi-granularity_1996,
	abstract = {A locking model adopting a multi-granularity approach is proposed for concurrency control in object-oriented database systems. The model is motivated by a desire to provide high concurrency and low locking overhead in accessing objects. Locking in schemas and locking in instances are developed separately and then are integrated. Schema changes and composite objects are also taken into account. A dual queue scheme for efficient scheduling of lock requests is developed. The model consists of a rich set of lock modes, a compatibility matrix, and a locking protocol. Characteristic query examples on single class, class lattice, and composite objects are used to illustrate the comparison between the ORION model and the proposed model. It is shown that our locking model has indeed made some improvements and is suitable for concurrency control in object-oriented databases.},
	author = {Lee, Suh-Yin and Liou, Ruey-Long},
	doi = {10.1109/69.485643},
	file = {IEEE Xplore Abstract Record:/Users/pandey/Zotero/storage/NF92ZHM3/485643.html:text/html},
	issn = {1558-2191},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	month = feb,
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	number = {1},
	pages = {144--156},
	title = {A multi-granularity locking model for concurrency control in object-oriented database systems},
	url = {https://ieeexplore.ieee.org/abstract/document/485643},
	urldate = {2023-09-29},
	volume = {8},
	year = {1996},
	bdsk-url-1 = {https://ieeexplore.ieee.org/abstract/document/485643},
	bdsk-url-2 = {https://doi.org/10.1109/69.485643}}

@inproceedings{jouili_empirical_2013-1,
	abstract = {In recent years, more and more companies provide services that can not be anymore achieved efficiently using relational databases. As such, these companies are forced to use alternative database models such as XML databases, object-oriented databases, document-oriented databases and, more recently graph databases. Graph databases only exist for a few years. Although there have been some comparison attempts, they are mostly focused on certain aspects only. In this paper, we present a distributed graph database comparison framework and the results we obtained by comparing four important players in the graph databases market: Neo4j, Orient DB, Titan and DEX.},
	address = {USA},
	author = {Jouili, Salim and Vansteenberghe, Valentin},
	booktitle = {Proceedings of the 2013 {International} {Conference} on {Social} {Computing}},
	doi = {10.1109/SocialCom.2013.106},
	isbn = {978-0-7695-5137-1},
	keywords = {Database Benchmark, graph, graph database, graph traversal},
	month = sep,
	pages = {708--715},
	publisher = {IEEE Computer Society},
	series = {{SOCIALCOM} '13},
	title = {An {Empirical} {Comparison} of {Graph} {Databases}},
	url = {https://doi.org/10.1109/SocialCom.2013.106},
	urldate = {2023-10-03},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/SocialCom.2013.106}}

@article{wolde_duckpgq_2023,
	abstract = {We demonstrate the most important new feature of SQL:2023, namely SQL/PGQ, which eases querying graphs using SQL by introducing new syntax for pattern matching and (shortest) path-finding. We show how support for SQL/PGQ can be integrated into an RDBMS, specifically in the DuckDB system, using an extension module called DuckPGQ. As such, we also demonstrate the use of the DuckDB extensibility mechanism, which allows us to add new functions, data types, operators, optimizer rules, storage systems, and even parsers to DuckDB. We also describe the new data structures and algorithms that the DuckPGQ module is based on, and how they are injected into SQL plans. While the demonstrated DuckPGQ extension module is lean and efficient, we sketch a roadmap to (i) improve its performance through new algorithms (factorized and WCOJ) and better parallelism and (ii) extend its functionality to scenarios beyond SQL, e.g., building and analyzing Graph Neural Networks.},
	author = {Wolde, Daniel ten and Sz{\'a}rnyas, G{\'a}bor and Boncz, Peter},
	doi = {10.14778/3611540.3611614},
	file = {Full Text PDF:/Users/pandey/Zotero/storage/N6UETWV5/Wolde et al. - 2023 - DuckPGQ Bringing SQLPGQ to DuckDB.pdf:application/pdf},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	month = sep,
	number = {12},
	pages = {4034--4037},
	shorttitle = {{DuckPGQ}},
	title = {{DuckPGQ}: {Bringing} {SQL}/{PGQ} to {DuckDB}},
	url = {https://dl.acm.org/doi/10.14778/3611540.3611614},
	urldate = {2023-10-09},
	volume = {16},
	year = {2023},
	bdsk-url-1 = {https://dl.acm.org/doi/10.14778/3611540.3611614},
	bdsk-url-2 = {https://doi.org/10.14778/3611540.3611614}}


@incollection{w3cRDF,
  title        = {{W3C} Standard {RDF} Query Language},
  booktitle    = {Encyclopedia of Social Network Analysis and Mining},
  pages        = {2337},
  year         = {2014},
  url          = {https://doi.org/10.1007/978-1-4614-6170-8\_100894},
  doi          = {10.1007/978-1-4614-6170-8\_100894},
  timestamp    = {Wed, 12 Jul 2017 09:11:45 +0200},
  biburl       = {https://dblp.org/rec/reference/snam/X14xce.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
