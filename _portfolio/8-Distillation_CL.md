---
title: "Distilled Images for Continual Learning"
excerpt: "Explored effect of using distilled synthetic data for continual learning.<br/><img src='/images/portfolio_images/distillation_for_CL.png'>"
collection: portfolio
---
* Explored the effect of using images with features distilled from orignal dataset to continually train ResNet18/50. Studied
the SOTA dataset distillation approach, D4M, that uses **Latent Diffusion Models** to generate distilled images.
* Demonstrated that synthetic datasets significantly **reduce training time by 59%** while achieving comparable
accuracy across continual learning methods like **Experience Replay, A-GEM, and Task Arithmetic**.

[Report](https://drive.google.com/file/d/1zwAnXNTvDCi2FoLmfqLeC4N5Ba-HaxDP/view)
[Poster](https://drive.google.com/file/d/1GvUSqwIZdBDF2h-1OM2bm225sdtnHibq/view)
[Code](https://github.com/rajas1310/Distilled-Images-for-Continual-Learning)