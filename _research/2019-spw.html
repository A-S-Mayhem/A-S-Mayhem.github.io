---
title: 'Single-perspective Warps in Natural Image Stitching'
excerpt: 'Warp and align input images in the single-perspective setting'
teaser: '2019-spw.jpg'
permalink: /research/2019-spw
collection: research
---


<!doctype html>
<html>
<head>
<style>
.content { width: 960px; padding:20px; margin:auto }
</style>
</head>

<body>
<div  class = 'content'>

<h1 style="text-align:center;">Single-perspective Warps in Natural Image Stitching</h1>

<p style="text-align:center;"><a href='https://tlliao.github.io'>Tianli Liao</a> and Nan Li</p>

<figure>
<center>
<img src="../single-perspective.jpg" alt="single-perspective" width="75%">
<figcaption>Single-perspective v.s. multiple-perspective.</figcaption>
</center>
</figure>



<h2>Abstract</h2>

<p>Results of image stitching can be perceptually divided into single-perspective and multiple-perspective. Compared to the multiple-perspective result, the single-perspective result excels in perspective consistency but suffers from projective distortion. In this paper, we propose two single-perspective warps for natural image stitching. The first one is a parametric warp, which is an incremental combination of the dual-feature-based as-projective-as-possible warp and the quasi-homography warp. The second one is a mesh-based warp, which is determined by optimizing a total energy function that simultaneously emphasizes different characteristics of the single-perspective warp, including alignment, distortion and saliency. A comprehensive evaluation demonstrates that the proposed warp outperforms some stateof-the-art warps in urban scenes, including APAP, AutoStitch, SPHP and GSP.</p>

<h2>Paper</h2>

<p>
<ul>
<li>Single-perspective Warps in Natural Image Stitching,<br />Tianli Liao and Nan Li,<br />IEEE Transactions on Image Processing (TIP), doi: 10.1109/TIP.2019.2934344, 2019.</li></ul></p>


<h2>Source Codes</h2>
<p>
<ul>
<li><a href='https://github.com/tlliao/Single-perspective-warps'>Single-perspective-warps</a><br />Warp and align two images in the single-perspective setting.</li>
<li><a href='https://github.com/tlliao/Single-perspective-warps-multiple'>Single-perspective-warps-multiple</a><br />Warp and align multiple images in the single-perspective setting.</li></ul>
*Kindly cite the above paper if using our code in your work.
</p>


<h2>Datasets</h2>

<p>We experimented with our algorithm on images from public datasets, include DH [5], CAVE [6], SPHP [3], AANAP [4], GSP [2] and APAP [1]. We also tested our method on our own <a href='https://tlliao.github.io/images/dataset-2.zip'>dataset</a>, you can download here.
</p>



<h2>References</h2>

<p><ol>
<li>J. Zaragoza, T.-J. Chin, Q.-H. Tran, M. S. Brown, and D. Suter, “Asprojective-as-possible image stitching with moving dlt,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 7, no. 36, pp. 1285–1298, 2014.</li>
<li>Y.-S. Chen and Y.-Y. Chuang, “Natural image stitching with the global similarity prior,” in Proc. 14th Eur. Conf. Comput. Vision, 2016, pp. 186–201.</li>
<li>C.-H. Chang, Y. Sato, and Y.-Y. Chuang, “Shape-preserving halfprojective warps for image stitching,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2014, pp. 3254–3261.</li> 
<li>C.-C. Lin, S. U. Pankanti, K. N. Ramamurthy, and A. Y. Aravkin, “Adaptive as-natural-as-possible image stitching,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp. 1155–1163.</li> 
<li>J. Gao, S. J. Kim, and M. S. Brown, “Constructing image panoramas using dual-homography warping,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2011, pp. 49–56.</li>
<li>Y. Nomura, L. Zhang, and S. K. Nayar, “Scene collages and flexible camera arrays,” in Proc. 18th Eurographics Conf. Rendering Techniques. Eurographics Association, 2007, pp. 127–138.</li>
</ol>
</p>

<a name="Citation"></a>
<h2>BibTex</h2>
<p><code>
@article{liao2019single,<br>
  title={Single-Perspective Warps in Natural Image Stitching},<br>
  author={Liao, Tianli and Li, Nan},<br>
  journal={IEEE Transactions on Image Processing},<br>
  volume={29},<br>
  number={},<br>
  pages={724--735},<br>
  year={2020},<br>
  doi={10.1109/TIP.2019.2934344},<br>
  publisher={IEEE}<br>
}
</p></code>

</div>

</body>
</html>
