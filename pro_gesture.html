
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>FlatNet</title>

    <meta name="description" content="FlatNet">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--FACEBOOK-->
    <meta property="og:image" content="img/twitter-card.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1024">
    <meta property="og:image:height" content="512">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://siddiquesalman.github.io/flatnet/" />
    <meta property="og:title" content="FlatNet" />
    <meta property="og:description"
        content="Project page for FlatNet: Towards Photorealistic Scene Reconstruction from Lensless Measurements." />

    <!--TWITTER-->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="https://siddiquesalman.github.io/flatnet/" />
    <meta name="twitter:title" content="FlatNet" />
    <meta name="twitter:description"
        content="Project page for FlatNet: Towards Photorealistic Scene Reconstruction from Lensless Measurements." />
    <meta name="twitter:image" content="https://siddiquesalman.github.io/flatnet/img/twitter-card.jpg" />

    <!-- FAVICONS -->
    <link rel="apple-touch-icon-precomposed" sizes="57x57"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon-precomposed" sizes="114x114"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon-precomposed" sizes="72x72"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon-precomposed" sizes="144x144"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon-precomposed" sizes="60x60"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon-precomposed" sizes="120x120"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon-precomposed" sizes="76x76"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon-precomposed" sizes="152x152"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-152x152.png" />
    <link rel="icon" type="image/png" href="https://siddiquesalman.github.io/flatnet/favicons/favicon-196x196.png"
        sizes="196x196" />
    <link rel="icon" type="image/png" href="https://siddiquesalman.github.io/flatnet/favicons/favicon-96x96.png"
        sizes="96x96" />
    <link rel="icon" type="image/png" href="https://siddiquesalman.github.io/flatnet/favicons/favicon-32x32.png"
        sizes="32x32" />
    <link rel="icon" type="image/png" href="https://siddiquesalman.github.io/flatnet/favicons/favicon-16x16.png"
        sizes="16x16" />
    <link rel="icon" type="image/png" href="https://siddiquesalman.github.io/flatnet/favicons/favicon-128.png"
        sizes="128x128" />
    <meta name="application-name" content="FlatNet" />
    <meta name="msapplication-TileColor" content="#FFFFFF" />
    <meta name="msapplication-TileImage"
        content="https://siddiquesalman.github.io/flatnet/favicons/mstile-144x144.png" />
    <meta name="msapplication-square70x70logo"
        content="https://siddiquesalman.github.io/flatnet/favicons/mstile-70x70.png" />
    <meta name="msapplication-square150x150logo"
        content="https://siddiquesalman.github.io/flatnet/favicons/mstile-150x150.png" />
    <meta name="msapplication-wide310x150logo"
        content="https://siddiquesalman.github.io/flatnet/favicons/mstile-310x150.png" />
    <meta name="msapplication-square310x310logo"
        content="https://siddiquesalman.github.io/flatnet/favicons/mstile-310x310.png" />


    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-5H2C4DFSMD"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-5H2C4DFSMD');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Hand Gestures Recognition in Videos   <br>
                Taken with Lensless Camera</br>
                <small>
                    Optics Express
                </small>
            </h2>

        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>                        
                        Yinger Zhang    
                    </li>
                    <li>
                        Zhouyi Wu
                    </li>
                    <li>
                        Peiying Lin
                    </li>
                    <li>
                        Jiangtao Huangfu
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
            <div class="col-md-6 col-md-offset-3 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://opg.optica.org/oe/fulltext.cfm?uri=oe-30-22-39520&id=509832">
                            
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>



                </ul>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="image/pro_hand_framework.png" class="img-responsive" alt="overview"><br>
                    <p class="text-justify">
                        A lensless camera is an imaging system that uses a mask in place of a lens, making it thinner, lighter, and less expensive than a lensed camera. However, additional complex computation and time are required for image reconstruction. This work proposes a deep learning model named Raw3dNet that recognizes hand gestures directly on raw videos captured by a lensless camera without the need for image restoration. In addition to conserving computational resources, the reconstruction-free method provides privacy protection. Raw3dNet is a novel end-to-end deep neural network model for the recognition of hand gestures in lensless imaging systems. It is created specifically for raw video captured by a lensless camera and has the ability to properly extract and combine temporal and spatial features. The network is composed of two stages: 1. spatial feature extractor (SFE), which enhances the spatial features of each frame prior to temporal convolution; 2. 3D-ResNet, which implements spatial and temporal convolution of video streams. The proposed model achieves 98.59% accuracy on the Cambridge Hand Gesture dataset in the lensless optical experiment, which is comparable to the lensed-camera result. Additionally, the feasibility of physical object recognition is assessed. Furtherly, we show that the recognition can be achieved with respectable accuracy using only a tiny portion of the original raw data, indicating the potential for reducing data traffic in cloud computing scenarios.
                        <br>
                    </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Dataset
                </h3>
                

                <p align="center">
                <image class="center" src="image/pro_hand_hardware.jpg"   width="400"  alt="overview" /><br>
                </br><image class="center" src="image/pro_hand_dataset.png"   width="600"  alt="overview" /><br>
                </p>
                   
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>

                <br> <b>Reconstruction results from ADMM and CRNN</b>

                <p align="center">
                </br><image class="center" src="image/pro_hand_model.png" width="700"  alt="admm&cnn" /><br>
                </p>
               
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Result
                </h3>

                <br> <b>Definition of datasets</b>

                <p align="center">
                </br><image class="center" src="image/pro_hand_reconstruct.png" width="700"  alt="admm&cnn" /><br>
                </p>

                <b> Comparison of performances for 3D-ResNet/ Raw3dNet for lensless video; 
                    comparison for lensless video/reconstruction video/lensed video </b>

                <p align="center">
                </br><image class="center" src="image/pro_hand_compare1.PNG" width="700"  alt="admm&cnn" /><br>
                </p>

                <b> Assessment for various down-sampling techniques and ratios</b>
                <p align="center">
                </br><image class="center" src="image/pro_hand_compare2.PNG" width="700"  alt="admm&cnn" /><br>
                </p>
                
            </div>
        </div>

        

        

       






    <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Key Contributions
                </h3>
                <br />
                <ul>
                    <li>We propose an efficient implementation for the learnable intermediate stage of a general
                        lensless model. In our <a href=https://siddiquesalman.github.io/flatcam_iccv.html>prior
                            work</a>, we shown this for the separable lensless model. Here
                        we non-trivially extend it to the general lensless case.</li>

                    <figure>
                        <image src="img/fig_7_sim.jpg" class="img-responsive" alt="overview">
                            <figcaption><b>Some simulated outputs.</b> Notice how closely the simulated measurements
                                resemble real ones.
                            </figcaption>
                    </figure>

                    <li>We verify the robustness of the proposed learnable intermediate mapping for the non-separable
                        lensless model on challenging scenarios where the lensless system does not follow a full
                        convolutional assumption.</li>

                    <li>We propose an initialization scheme for the non- separable lensless model that doesn’t require
                        explicit PSF calibration.</li>

                    <li>Similar to the display and direct captured measurements collected using the separable mask
                        <i><a href=https://intra.ece.ucr.edu/~sasif/papers/2015_AASVR_flatcam_iccv.pdf>FlatCam</a></i>
                        and described in our <a href=https://siddiquesalman.github.io/flatcam_iccv.html>previous
                            work</a>, we collect corresponding datasets for the
                        non-separable mask <i><a href=https://ieeexplore.ieee.org/document/9076617>PhlatCam</a></i>.
                    </li>

                    <br>

                    <figure>
                        <image src="img/dataset3.jpg" class="img-responsive" alt="overview">
                            <figcaption><b><br>Some simulated outputs.</b> Notice how closely the simulated measurements
                                resemble real ones.
                            </figcaption>
                    </figure>

                    <li>We also collect a dataset of unconstrained indoor lensless measurements paired with
                        corresponding unaligned webcam images which is finally used to finetune our proposed
                        <b>FlatNet</b> to
                        robustly deal with unconstrained real-world scenes.</li>

                    <li>Our method outperforms previous traditional and deep learning based lensless reconstruction
                        methods.</li>
                </ul>
            </div>
        </div> -->




</body>

</html>